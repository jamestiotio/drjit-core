//
// Generated by NVIDIA NVVM Compiler
//
// Compiler Build ID: CL-31294372
// Cuda compilation tools, release 11.7, V11.7.64
// Based on NVVM 7.0.1
//

.version 7.7
.target sm_70
.address_size 64

	// .globl	reduce_sum_i32
.extern .shared .align 16 .b8 shared_d[];
.extern .shared .align 16 .b8 shared[];

.visible .entry reduce_sum_i32(
	.param .u64 reduce_sum_i32_param_0,
	.param .u32 reduce_sum_i32_param_1,
	.param .u64 reduce_sum_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_sum_i32_param_0];
	ld.param.u32 	%r22, [reduce_sum_i32_param_1];
	ld.param.u64 	%rd2, [reduce_sum_i32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r24, %ctaid.x;
	shl.b32 	%r25, %r24, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r55, %r25, %r1;
	setp.ge.u32 	%p1, %r55, %r22;
	mov.u32 	%r56, 0;
	@%p1 bra 	$L__BB0_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r3, %r27, 11;

$L__BB0_2:
	mul.wide.u32 	%rd4, %r55, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	add.s32 	%r56, %r28, %r56;
	add.s32 	%r7, %r55, 1024;
	setp.ge.u32 	%p2, %r7, %r22;
	@%p2 bra 	$L__BB0_4;

	mul.wide.u32 	%rd6, %r7, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	add.s32 	%r56, %r29, %r56;

$L__BB0_4:
	add.s32 	%r55, %r55, %r3;
	setp.lt.u32 	%p3, %r55, %r22;
	@%p3 bra 	$L__BB0_2;

$L__BB0_5:
	shl.b32 	%r30, %r1, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r12, %r31, %r30;
	st.shared.u32 	[%r12], %r56;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB0_7;

	ld.shared.u32 	%r32, [%r12+2048];
	add.s32 	%r56, %r32, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB0_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB0_9;

	ld.shared.u32 	%r33, [%r12+1024];
	add.s32 	%r56, %r33, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB0_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB0_11;

	ld.shared.u32 	%r34, [%r12+512];
	add.s32 	%r56, %r34, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB0_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB0_13;

	ld.shared.u32 	%r35, [%r12+256];
	add.s32 	%r56, %r35, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB0_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB0_16;

	ld.shared.u32 	%r36, [%r12+128];
	add.s32 	%r37, %r36, %r56;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	add.s32 	%r43, %r42, %r37;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	add.s32 	%r46, %r45, %r43;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	add.s32 	%r49, %r48, %r46;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	add.s32 	%r51, %r50, %r49;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	add.s32 	%r21, %r53, %r51;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB0_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r24, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

$L__BB0_16:
	ret;

}
	// .globl	reduce_sum_u32
.visible .entry reduce_sum_u32(
	.param .u64 reduce_sum_u32_param_0,
	.param .u32 reduce_sum_u32_param_1,
	.param .u64 reduce_sum_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_sum_u32_param_0];
	ld.param.u32 	%r22, [reduce_sum_u32_param_1];
	ld.param.u64 	%rd2, [reduce_sum_u32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r24, %ctaid.x;
	shl.b32 	%r25, %r24, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r55, %r25, %r1;
	setp.ge.u32 	%p1, %r55, %r22;
	mov.u32 	%r56, 0;
	@%p1 bra 	$L__BB1_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r3, %r27, 11;

$L__BB1_2:
	mul.wide.u32 	%rd4, %r55, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	add.s32 	%r56, %r28, %r56;
	add.s32 	%r7, %r55, 1024;
	setp.ge.u32 	%p2, %r7, %r22;
	@%p2 bra 	$L__BB1_4;

	mul.wide.u32 	%rd6, %r7, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	add.s32 	%r56, %r29, %r56;

$L__BB1_4:
	add.s32 	%r55, %r55, %r3;
	setp.lt.u32 	%p3, %r55, %r22;
	@%p3 bra 	$L__BB1_2;

$L__BB1_5:
	shl.b32 	%r30, %r1, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r12, %r31, %r30;
	st.shared.u32 	[%r12], %r56;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB1_7;

	ld.shared.u32 	%r32, [%r12+2048];
	add.s32 	%r56, %r32, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB1_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB1_9;

	ld.shared.u32 	%r33, [%r12+1024];
	add.s32 	%r56, %r33, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB1_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB1_11;

	ld.shared.u32 	%r34, [%r12+512];
	add.s32 	%r56, %r34, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB1_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB1_13;

	ld.shared.u32 	%r35, [%r12+256];
	add.s32 	%r56, %r35, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB1_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB1_16;

	ld.shared.u32 	%r36, [%r12+128];
	add.s32 	%r37, %r36, %r56;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	add.s32 	%r43, %r42, %r37;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	add.s32 	%r46, %r45, %r43;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	add.s32 	%r49, %r48, %r46;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	add.s32 	%r51, %r50, %r49;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	add.s32 	%r21, %r53, %r51;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB1_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r24, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

$L__BB1_16:
	ret;

}
	// .globl	reduce_sum_i64
.visible .entry reduce_sum_i64(
	.param .u64 reduce_sum_i64_param_0,
	.param .u32 reduce_sum_i64_param_1,
	.param .u64 reduce_sum_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_sum_i64_param_0];
	ld.param.u32 	%r9, [reduce_sum_i64_param_1];
	ld.param.u64 	%rd16, [reduce_sum_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, 0;
	@%p1 bra 	$L__BB2_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB2_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	add.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB2_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	add.s64 	%rd44, %rd25, %rd44;

$L__BB2_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB2_2;

$L__BB2_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB2_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	add.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB2_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB2_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	add.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB2_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB2_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	add.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB2_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB2_13;

	ld.shared.u64 	%rd29, [%r8+512];
	add.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB2_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB2_16;

	ld.shared.u64 	%rd40, [%r8+256];
	add.s64 	%rd30, %rd40, %rd44;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	add.s64 	%rd32, %rd31, %rd30;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	add.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	add.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	add.s64 	%rd38, %rd37, %rd36;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	add.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB2_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB2_16:
	ret;

}
	// .globl	reduce_sum_u64
.visible .entry reduce_sum_u64(
	.param .u64 reduce_sum_u64_param_0,
	.param .u32 reduce_sum_u64_param_1,
	.param .u64 reduce_sum_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_sum_u64_param_0];
	ld.param.u32 	%r9, [reduce_sum_u64_param_1];
	ld.param.u64 	%rd16, [reduce_sum_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, 0;
	@%p1 bra 	$L__BB3_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB3_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	add.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB3_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	add.s64 	%rd44, %rd25, %rd44;

$L__BB3_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB3_2;

$L__BB3_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB3_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	add.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB3_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB3_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	add.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB3_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB3_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	add.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB3_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB3_13;

	ld.shared.u64 	%rd29, [%r8+512];
	add.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB3_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB3_16;

	ld.shared.u64 	%rd40, [%r8+256];
	add.s64 	%rd30, %rd40, %rd44;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	add.s64 	%rd32, %rd31, %rd30;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	add.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	add.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	add.s64 	%rd38, %rd37, %rd36;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	add.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB3_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB3_16:
	ret;

}
	// .globl	reduce_sum_f16
.visible .entry reduce_sum_f16(
	.param .u64 reduce_sum_f16_param_0,
	.param .u32 reduce_sum_f16_param_1,
	.param .u64 reduce_sum_f16_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<79>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_sum_f16_param_0];
	ld.param.u32 	%r8, [reduce_sum_f16_param_1];
	ld.param.u64 	%rd2, [reduce_sum_f16_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r70, %r11, %r1;
	mov.u32 	%r9, 0;
	// begin inline asm
	cvt.rn.f16.s32 %rs72, %r9;
	// end inline asm
	setp.ge.u32 	%p1, %r70, %r8;
	@%p1 bra 	$L__BB4_5;

	mov.u32 	%r16, %nctaid.x;
	shl.b32 	%r3, %r16, 11;

$L__BB4_2:
	mul.wide.u32 	%rd4, %r70, 2;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u16 	%rs23, [%rd5];
	// begin inline asm
	{add.f16 %rs72,%rs72,%rs23;
}
	// end inline asm
	add.s32 	%r5, %r70, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB4_4;

	mul.wide.u32 	%rd6, %r5, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u16 	%rs26, [%rd7];
	// begin inline asm
	{add.f16 %rs72,%rs72,%rs26;
}
	// end inline asm

$L__BB4_4:
	add.s32 	%r70, %r70, %r3;
	setp.lt.u32 	%p3, %r70, %r8;
	@%p3 bra 	$L__BB4_2;

$L__BB4_5:
	shl.b32 	%r17, %r1, 1;
	mov.u32 	%r18, shared;
	add.s32 	%r7, %r18, %r17;
	st.shared.u16 	[%r7], %rs72;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB4_7;

	ld.shared.u16 	%rs29, [%r7+1024];
	// begin inline asm
	{add.f16 %rs72,%rs72,%rs29;
}
	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB4_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB4_9;

	ld.shared.u16 	%rs32, [%r7+512];
	// begin inline asm
	{add.f16 %rs72,%rs72,%rs32;
}
	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB4_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB4_11;

	ld.shared.u16 	%rs35, [%r7+256];
	// begin inline asm
	{add.f16 %rs72,%rs72,%rs35;
}
	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB4_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB4_13;

	ld.shared.u16 	%rs38, [%r7+128];
	// begin inline asm
	{add.f16 %rs72,%rs72,%rs38;
}
	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB4_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB4_16;

	ld.shared.u16 	%rs41, [%r7+64];
	// begin inline asm
	{add.f16 %rs39,%rs72,%rs41;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r19, {%rs39,%rs39};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r20, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r59, %r20, 8;
	mov.u32 	%r31, 8;
	add.s32 	%r60, %r59, -8192;
	or.b32  	%r24, %r60, 31;
	mov.u32 	%r23, 16;
	mov.u32 	%r57, -1;
	// begin inline asm
	{shfl.sync.down.b32 %r21,%r19,%r23,%r24,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r21;
 mov.b16 %rs44, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs45,%rs39,%rs44;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r27, {%rs45,%rs45};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r28, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r61, %r28, 8;
	add.s32 	%r62, %r61, -8192;
	or.b32  	%r32, %r62, 31;
	// begin inline asm
	{shfl.sync.down.b32 %r29,%r27,%r31,%r32,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r29;
 mov.b16 %rs50, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs51,%rs45,%rs50;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r35, {%rs51,%rs51};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r36, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r63, %r36, 8;
	add.s32 	%r64, %r63, -8192;
	or.b32  	%r40, %r64, 31;
	mov.u32 	%r39, 4;
	// begin inline asm
	{shfl.sync.down.b32 %r37,%r35,%r39,%r40,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r37;
 mov.b16 %rs56, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs57,%rs51,%rs56;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r43, {%rs57,%rs57};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r44, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r65, %r44, 8;
	add.s32 	%r66, %r65, -8192;
	or.b32  	%r48, %r66, 31;
	mov.u32 	%r47, 2;
	// begin inline asm
	{shfl.sync.down.b32 %r45,%r43,%r47,%r48,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r45;
 mov.b16 %rs62, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs63,%rs57,%rs62;
}
	// end inline asm
	// begin inline asm
	{  mov.b32 %r51, {%rs63,%rs63};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r52, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r67, %r52, 8;
	add.s32 	%r68, %r67, -8192;
	or.b32  	%r56, %r68, 31;
	mov.u32 	%r55, 1;
	// begin inline asm
	{shfl.sync.down.b32 %r53,%r51,%r55,%r56,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r53;
 mov.b16 %rs68, low;}
	// end inline asm
	// begin inline asm
	{add.f16 %rs69,%rs63,%rs68;
}
	// end inline asm
	setp.ne.s32 	%p9, %r1, 0;
	@%p9 bra 	$L__BB4_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r10, 2;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u16 	[%rd10], %rs69;

$L__BB4_16:
	ret;

}
	// .globl	reduce_sum_f32
.visible .entry reduce_sum_f32(
	.param .u64 reduce_sum_f32_param_0,
	.param .u32 reduce_sum_f32_param_1,
	.param .u64 reduce_sum_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_sum_f32_param_0];
	ld.param.u32 	%r8, [reduce_sum_f32_param_1];
	ld.param.u64 	%rd2, [reduce_sum_f32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r32, %r10, %r1;
	setp.ge.u32 	%p1, %r32, %r8;
	mov.f32 	%f34, 0f00000000;
	@%p1 bra 	$L__BB5_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r3, %r11, 11;

$L__BB5_2:
	mul.wide.u32 	%rd4, %r32, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f17, [%rd5];
	add.f32 	%f34, %f34, %f17;
	add.s32 	%r5, %r32, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB5_4;

	mul.wide.u32 	%rd6, %r5, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f18, [%rd7];
	add.f32 	%f34, %f34, %f18;

$L__BB5_4:
	add.s32 	%r32, %r32, %r3;
	setp.lt.u32 	%p3, %r32, %r8;
	@%p3 bra 	$L__BB5_2;

$L__BB5_5:
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r7, %r13, %r12;
	st.shared.f32 	[%r7], %f34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB5_7;

	ld.shared.f32 	%f19, [%r7+2048];
	add.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r7], %f34;

$L__BB5_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB5_9;

	ld.shared.f32 	%f20, [%r7+1024];
	add.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r7], %f34;

$L__BB5_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB5_11;

	ld.shared.f32 	%f21, [%r7+512];
	add.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r7], %f34;

$L__BB5_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB5_13;

	ld.shared.f32 	%f22, [%r7+256];
	add.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r7], %f34;

$L__BB5_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB5_16;

	ld.shared.f32 	%f23, [%r7+128];
	add.f32 	%f24, %f34, %f23;
	mov.b32 	%r14, %f24;
	mov.u32 	%r15, 2;
	mov.u32 	%r16, 31;
	mov.u32 	%r17, 16;
	mov.u32 	%r18, -1;
	shfl.sync.down.b32 	%r19|%p9, %r14, %r17, %r16, %r18;
	mov.b32 	%f25, %r19;
	add.f32 	%f26, %f24, %f25;
	mov.b32 	%r20, %f26;
	mov.u32 	%r21, 8;
	shfl.sync.down.b32 	%r22|%p10, %r20, %r21, %r16, %r18;
	mov.b32 	%f27, %r22;
	add.f32 	%f28, %f26, %f27;
	mov.b32 	%r23, %f28;
	mov.u32 	%r24, 4;
	shfl.sync.down.b32 	%r25|%p11, %r23, %r24, %r16, %r18;
	mov.b32 	%f29, %r25;
	add.f32 	%f30, %f28, %f29;
	mov.b32 	%r26, %f30;
	shfl.sync.down.b32 	%r27|%p12, %r26, %r15, %r16, %r18;
	mov.b32 	%f31, %r27;
	add.f32 	%f32, %f30, %f31;
	mov.b32 	%r28, %f32;
	mov.u32 	%r29, 1;
	shfl.sync.down.b32 	%r30|%p13, %r28, %r29, %r16, %r18;
	mov.b32 	%f33, %r30;
	add.f32 	%f14, %f32, %f33;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB5_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r9, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f14;

$L__BB5_16:
	ret;

}
	// .globl	reduce_sum_f64
.visible .entry reduce_sum_f64(
	.param .u64 reduce_sum_f64_param_0,
	.param .u32 reduce_sum_f64_param_1,
	.param .u64 reduce_sum_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_sum_f64_param_0];
	ld.param.u32 	%r9, [reduce_sum_f64_param_1];
	ld.param.u64 	%rd2, [reduce_sum_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.f64 	%fd34, 0d0000000000000000;
	@%p1 bra 	$L__BB6_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB6_2:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	add.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB6_4;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	add.f64 	%fd34, %fd34, %fd18;

$L__BB6_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB6_2;

$L__BB6_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB6_7;

	ld.shared.f64 	%fd19, [%r8+4096];
	add.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

$L__BB6_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB6_9;

	ld.shared.f64 	%fd20, [%r8+2048];
	add.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

$L__BB6_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB6_11;

	ld.shared.f64 	%fd21, [%r8+1024];
	add.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

$L__BB6_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB6_13;

	ld.shared.f64 	%fd22, [%r8+512];
	add.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

$L__BB6_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB6_16;

	ld.shared.f64 	%fd33, [%r8+256];
	add.f64 	%fd23, %fd34, %fd33;
	// begin inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %fd24, {%r16,%r17};
	// end inline asm
	add.f64 	%fd25, %fd23, %fd24;
	// begin inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %fd26, {%r20,%r21};
	// end inline asm
	add.f64 	%fd27, %fd25, %fd26;
	// begin inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %fd28, {%r24,%r25};
	// end inline asm
	add.f64 	%fd29, %fd27, %fd28;
	// begin inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %fd30, {%r28,%r29};
	// end inline asm
	add.f64 	%fd31, %fd29, %fd30;
	// begin inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %fd32, {%r32,%r33};
	// end inline asm
	add.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB6_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

$L__BB6_16:
	ret;

}
	// .globl	reduce_mul_i32
.visible .entry reduce_mul_i32(
	.param .u64 reduce_mul_i32_param_0,
	.param .u32 reduce_mul_i32_param_1,
	.param .u64 reduce_mul_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_mul_i32_param_0];
	ld.param.u32 	%r22, [reduce_mul_i32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_i32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r24, %ctaid.x;
	shl.b32 	%r25, %r24, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r55, %r25, %r1;
	setp.ge.u32 	%p1, %r55, %r22;
	mov.u32 	%r56, 1;
	@%p1 bra 	$L__BB7_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r3, %r27, 11;

$L__BB7_2:
	mul.wide.u32 	%rd4, %r55, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	mul.lo.s32 	%r56, %r28, %r56;
	add.s32 	%r7, %r55, 1024;
	setp.ge.u32 	%p2, %r7, %r22;
	@%p2 bra 	$L__BB7_4;

	mul.wide.u32 	%rd6, %r7, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	mul.lo.s32 	%r56, %r29, %r56;

$L__BB7_4:
	add.s32 	%r55, %r55, %r3;
	setp.lt.u32 	%p3, %r55, %r22;
	@%p3 bra 	$L__BB7_2;

$L__BB7_5:
	shl.b32 	%r30, %r1, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r12, %r31, %r30;
	st.shared.u32 	[%r12], %r56;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB7_7;

	ld.shared.u32 	%r32, [%r12+2048];
	mul.lo.s32 	%r56, %r32, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB7_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB7_9;

	ld.shared.u32 	%r33, [%r12+1024];
	mul.lo.s32 	%r56, %r33, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB7_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB7_11;

	ld.shared.u32 	%r34, [%r12+512];
	mul.lo.s32 	%r56, %r34, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB7_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB7_13;

	ld.shared.u32 	%r35, [%r12+256];
	mul.lo.s32 	%r56, %r35, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB7_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB7_16;

	ld.shared.u32 	%r36, [%r12+128];
	mul.lo.s32 	%r37, %r36, %r56;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	mul.lo.s32 	%r43, %r42, %r37;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	mul.lo.s32 	%r46, %r45, %r43;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	mul.lo.s32 	%r49, %r48, %r46;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	mul.lo.s32 	%r51, %r50, %r49;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	mul.lo.s32 	%r21, %r53, %r51;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB7_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r24, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

$L__BB7_16:
	ret;

}
	// .globl	reduce_mul_u32
.visible .entry reduce_mul_u32(
	.param .u64 reduce_mul_u32_param_0,
	.param .u32 reduce_mul_u32_param_1,
	.param .u64 reduce_mul_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_mul_u32_param_0];
	ld.param.u32 	%r22, [reduce_mul_u32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_u32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r24, %ctaid.x;
	shl.b32 	%r25, %r24, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r55, %r25, %r1;
	setp.ge.u32 	%p1, %r55, %r22;
	mov.u32 	%r56, 1;
	@%p1 bra 	$L__BB8_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r3, %r27, 11;

$L__BB8_2:
	mul.wide.u32 	%rd4, %r55, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	mul.lo.s32 	%r56, %r28, %r56;
	add.s32 	%r7, %r55, 1024;
	setp.ge.u32 	%p2, %r7, %r22;
	@%p2 bra 	$L__BB8_4;

	mul.wide.u32 	%rd6, %r7, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	mul.lo.s32 	%r56, %r29, %r56;

$L__BB8_4:
	add.s32 	%r55, %r55, %r3;
	setp.lt.u32 	%p3, %r55, %r22;
	@%p3 bra 	$L__BB8_2;

$L__BB8_5:
	shl.b32 	%r30, %r1, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r12, %r31, %r30;
	st.shared.u32 	[%r12], %r56;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB8_7;

	ld.shared.u32 	%r32, [%r12+2048];
	mul.lo.s32 	%r56, %r32, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB8_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB8_9;

	ld.shared.u32 	%r33, [%r12+1024];
	mul.lo.s32 	%r56, %r33, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB8_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB8_11;

	ld.shared.u32 	%r34, [%r12+512];
	mul.lo.s32 	%r56, %r34, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB8_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB8_13;

	ld.shared.u32 	%r35, [%r12+256];
	mul.lo.s32 	%r56, %r35, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB8_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB8_16;

	ld.shared.u32 	%r36, [%r12+128];
	mul.lo.s32 	%r37, %r36, %r56;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	mul.lo.s32 	%r43, %r42, %r37;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	mul.lo.s32 	%r46, %r45, %r43;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	mul.lo.s32 	%r49, %r48, %r46;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	mul.lo.s32 	%r51, %r50, %r49;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	mul.lo.s32 	%r21, %r53, %r51;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB8_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r24, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

$L__BB8_16:
	ret;

}
	// .globl	reduce_mul_i64
.visible .entry reduce_mul_i64(
	.param .u64 reduce_mul_i64_param_0,
	.param .u32 reduce_mul_i64_param_1,
	.param .u64 reduce_mul_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_mul_i64_param_0];
	ld.param.u32 	%r9, [reduce_mul_i64_param_1];
	ld.param.u64 	%rd16, [reduce_mul_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, 1;
	@%p1 bra 	$L__BB9_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB9_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	mul.lo.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB9_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	mul.lo.s64 	%rd44, %rd25, %rd44;

$L__BB9_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB9_2;

$L__BB9_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB9_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	mul.lo.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB9_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB9_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	mul.lo.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB9_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB9_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	mul.lo.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB9_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB9_13;

	ld.shared.u64 	%rd29, [%r8+512];
	mul.lo.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB9_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB9_16;

	ld.shared.u64 	%rd40, [%r8+256];
	mul.lo.s64 	%rd30, %rd40, %rd44;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	mul.lo.s64 	%rd32, %rd31, %rd30;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	mul.lo.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	mul.lo.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	mul.lo.s64 	%rd38, %rd37, %rd36;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	mul.lo.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB9_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB9_16:
	ret;

}
	// .globl	reduce_mul_u64
.visible .entry reduce_mul_u64(
	.param .u64 reduce_mul_u64_param_0,
	.param .u32 reduce_mul_u64_param_1,
	.param .u64 reduce_mul_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_mul_u64_param_0];
	ld.param.u32 	%r9, [reduce_mul_u64_param_1];
	ld.param.u64 	%rd16, [reduce_mul_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, 1;
	@%p1 bra 	$L__BB10_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB10_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	mul.lo.s64 	%rd44, %rd22, %rd44;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB10_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	mul.lo.s64 	%rd44, %rd25, %rd44;

$L__BB10_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB10_2;

$L__BB10_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB10_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	mul.lo.s64 	%rd44, %rd26, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB10_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB10_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	mul.lo.s64 	%rd44, %rd27, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB10_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB10_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	mul.lo.s64 	%rd44, %rd28, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB10_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB10_13;

	ld.shared.u64 	%rd29, [%r8+512];
	mul.lo.s64 	%rd44, %rd29, %rd44;
	st.shared.u64 	[%r8], %rd44;

$L__BB10_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB10_16;

	ld.shared.u64 	%rd40, [%r8+256];
	mul.lo.s64 	%rd30, %rd40, %rd44;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	mul.lo.s64 	%rd32, %rd31, %rd30;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	mul.lo.s64 	%rd34, %rd33, %rd32;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	mul.lo.s64 	%rd36, %rd35, %rd34;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	mul.lo.s64 	%rd38, %rd37, %rd36;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	mul.lo.s64 	%rd15, %rd39, %rd38;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB10_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB10_16:
	ret;

}
	// .globl	reduce_mul_f16
.visible .entry reduce_mul_f16(
	.param .u64 reduce_mul_f16_param_0,
	.param .u32 reduce_mul_f16_param_1,
	.param .u64 reduce_mul_f16_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<79>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<71>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_mul_f16_param_0];
	ld.param.u32 	%r8, [reduce_mul_f16_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f16_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r70, %r11, %r1;
	mov.u32 	%r9, 1;
	// begin inline asm
	cvt.rn.f16.s32 %rs72, %r9;
	// end inline asm
	setp.ge.u32 	%p1, %r70, %r8;
	@%p1 bra 	$L__BB11_5;

	mov.u32 	%r16, %nctaid.x;
	shl.b32 	%r3, %r16, 11;

$L__BB11_2:
	mul.wide.u32 	%rd4, %r70, 2;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u16 	%rs22, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs22;}

	// end inline asm
	mul.f32 	%f3, %f1, %f2;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f3;}

	// end inline asm
	add.s32 	%r5, %r70, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB11_4;

	mul.wide.u32 	%rd6, %r5, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u16 	%rs25, [%rd7];
	// begin inline asm
	{  cvt.f32.f16 %f4, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs25;}

	// end inline asm
	mul.f32 	%f6, %f4, %f5;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f6;}

	// end inline asm

$L__BB11_4:
	add.s32 	%r70, %r70, %r3;
	setp.lt.u32 	%p3, %r70, %r8;
	@%p3 bra 	$L__BB11_2;

$L__BB11_5:
	shl.b32 	%r17, %r1, 1;
	mov.u32 	%r18, shared;
	add.s32 	%r7, %r18, %r17;
	st.shared.u16 	[%r7], %rs72;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB11_7;

	ld.shared.u16 	%rs28, [%r7+1024];
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs28;}

	// end inline asm
	mul.f32 	%f9, %f7, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f9;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB11_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB11_9;

	ld.shared.u16 	%rs31, [%r7+512];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs31;}

	// end inline asm
	mul.f32 	%f12, %f10, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f12;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB11_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB11_11;

	ld.shared.u16 	%rs34, [%r7+256];
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs34;}

	// end inline asm
	mul.f32 	%f15, %f13, %f14;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f15;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB11_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB11_13;

	ld.shared.u16 	%rs37, [%r7+128];
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs37;}

	// end inline asm
	mul.f32 	%f18, %f16, %f17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f18;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB11_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB11_16;

	ld.shared.u16 	%rs40, [%r7+64];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs40;}

	// end inline asm
	mul.f32 	%f21, %f19, %f20;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs41, %f21;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r19, {%rs41,%rs41};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r20, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r59, %r20, 8;
	mov.u32 	%r31, 8;
	add.s32 	%r60, %r59, -8192;
	or.b32  	%r24, %r60, 31;
	mov.u32 	%r23, 16;
	mov.u32 	%r57, -1;
	// begin inline asm
	{shfl.sync.down.b32 %r21,%r19,%r23,%r24,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r21;
 mov.b16 %rs44, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs41;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs44;}

	// end inline asm
	mul.f32 	%f24, %f22, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs47, %f24;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r27, {%rs47,%rs47};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r28, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r61, %r28, 8;
	add.s32 	%r62, %r61, -8192;
	or.b32  	%r32, %r62, 31;
	// begin inline asm
	{shfl.sync.down.b32 %r29,%r27,%r31,%r32,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r29;
 mov.b16 %rs50, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f25, %rs47;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs50;}

	// end inline asm
	mul.f32 	%f27, %f25, %f26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f27;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r35, {%rs53,%rs53};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r36, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r63, %r36, 8;
	add.s32 	%r64, %r63, -8192;
	or.b32  	%r40, %r64, 31;
	mov.u32 	%r39, 4;
	// begin inline asm
	{shfl.sync.down.b32 %r37,%r35,%r39,%r40,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r37;
 mov.b16 %rs56, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs53;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs56;}

	// end inline asm
	mul.f32 	%f30, %f28, %f29;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs59, %f30;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r43, {%rs59,%rs59};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r44, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r65, %r44, 8;
	add.s32 	%r66, %r65, -8192;
	or.b32  	%r48, %r66, 31;
	mov.u32 	%r47, 2;
	// begin inline asm
	{shfl.sync.down.b32 %r45,%r43,%r47,%r48,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r45;
 mov.b16 %rs62, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs59;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs62;}

	// end inline asm
	mul.f32 	%f33, %f31, %f32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f33;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r51, {%rs65,%rs65};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r52, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r67, %r52, 8;
	add.s32 	%r68, %r67, -8192;
	or.b32  	%r56, %r68, 31;
	// begin inline asm
	{shfl.sync.down.b32 %r53,%r51,%r9,%r56,%r57;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r53;
 mov.b16 %rs68, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f34, %rs65;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f35, %rs68;}

	// end inline asm
	mul.f32 	%f36, %f34, %f35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f36;}

	// end inline asm
	setp.ne.s32 	%p9, %r1, 0;
	@%p9 bra 	$L__BB11_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r10, 2;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u16 	[%rd10], %rs71;

$L__BB11_16:
	ret;

}
	// .globl	reduce_mul_f32
.visible .entry reduce_mul_f32(
	.param .u64 reduce_mul_f32_param_0,
	.param .u32 reduce_mul_f32_param_1,
	.param .u64 reduce_mul_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_mul_f32_param_0];
	ld.param.u32 	%r8, [reduce_mul_f32_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r32, %r10, %r1;
	setp.ge.u32 	%p1, %r32, %r8;
	mov.f32 	%f34, 0f3F800000;
	@%p1 bra 	$L__BB12_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r3, %r11, 11;

$L__BB12_2:
	mul.wide.u32 	%rd4, %r32, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f17, [%rd5];
	mul.f32 	%f34, %f34, %f17;
	add.s32 	%r5, %r32, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB12_4;

	mul.wide.u32 	%rd6, %r5, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f18, [%rd7];
	mul.f32 	%f34, %f34, %f18;

$L__BB12_4:
	add.s32 	%r32, %r32, %r3;
	setp.lt.u32 	%p3, %r32, %r8;
	@%p3 bra 	$L__BB12_2;

$L__BB12_5:
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r7, %r13, %r12;
	st.shared.f32 	[%r7], %f34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB12_7;

	ld.shared.f32 	%f19, [%r7+2048];
	mul.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r7], %f34;

$L__BB12_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB12_9;

	ld.shared.f32 	%f20, [%r7+1024];
	mul.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r7], %f34;

$L__BB12_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB12_11;

	ld.shared.f32 	%f21, [%r7+512];
	mul.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r7], %f34;

$L__BB12_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB12_13;

	ld.shared.f32 	%f22, [%r7+256];
	mul.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r7], %f34;

$L__BB12_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB12_16;

	ld.shared.f32 	%f23, [%r7+128];
	mul.f32 	%f24, %f34, %f23;
	mov.b32 	%r14, %f24;
	mov.u32 	%r15, 2;
	mov.u32 	%r16, 31;
	mov.u32 	%r17, 16;
	mov.u32 	%r18, -1;
	shfl.sync.down.b32 	%r19|%p9, %r14, %r17, %r16, %r18;
	mov.b32 	%f25, %r19;
	mul.f32 	%f26, %f24, %f25;
	mov.b32 	%r20, %f26;
	mov.u32 	%r21, 8;
	shfl.sync.down.b32 	%r22|%p10, %r20, %r21, %r16, %r18;
	mov.b32 	%f27, %r22;
	mul.f32 	%f28, %f26, %f27;
	mov.b32 	%r23, %f28;
	mov.u32 	%r24, 4;
	shfl.sync.down.b32 	%r25|%p11, %r23, %r24, %r16, %r18;
	mov.b32 	%f29, %r25;
	mul.f32 	%f30, %f28, %f29;
	mov.b32 	%r26, %f30;
	shfl.sync.down.b32 	%r27|%p12, %r26, %r15, %r16, %r18;
	mov.b32 	%f31, %r27;
	mul.f32 	%f32, %f30, %f31;
	mov.b32 	%r28, %f32;
	mov.u32 	%r29, 1;
	shfl.sync.down.b32 	%r30|%p13, %r28, %r29, %r16, %r18;
	mov.b32 	%f33, %r30;
	mul.f32 	%f14, %f32, %f33;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB12_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r9, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f14;

$L__BB12_16:
	ret;

}
	// .globl	reduce_mul_f64
.visible .entry reduce_mul_f64(
	.param .u64 reduce_mul_f64_param_0,
	.param .u32 reduce_mul_f64_param_1,
	.param .u64 reduce_mul_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_mul_f64_param_0];
	ld.param.u32 	%r9, [reduce_mul_f64_param_1];
	ld.param.u64 	%rd2, [reduce_mul_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.f64 	%fd34, 0d3FF0000000000000;
	@%p1 bra 	$L__BB13_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB13_2:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	mul.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB13_4;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	mul.f64 	%fd34, %fd34, %fd18;

$L__BB13_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB13_2;

$L__BB13_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB13_7;

	ld.shared.f64 	%fd19, [%r8+4096];
	mul.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

$L__BB13_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB13_9;

	ld.shared.f64 	%fd20, [%r8+2048];
	mul.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

$L__BB13_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB13_11;

	ld.shared.f64 	%fd21, [%r8+1024];
	mul.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

$L__BB13_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB13_13;

	ld.shared.f64 	%fd22, [%r8+512];
	mul.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

$L__BB13_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB13_16;

	ld.shared.f64 	%fd33, [%r8+256];
	mul.f64 	%fd23, %fd34, %fd33;
	// begin inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %fd24, {%r16,%r17};
	// end inline asm
	mul.f64 	%fd25, %fd23, %fd24;
	// begin inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %fd26, {%r20,%r21};
	// end inline asm
	mul.f64 	%fd27, %fd25, %fd26;
	// begin inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %fd28, {%r24,%r25};
	// end inline asm
	mul.f64 	%fd29, %fd27, %fd28;
	// begin inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %fd30, {%r28,%r29};
	// end inline asm
	mul.f64 	%fd31, %fd29, %fd30;
	// begin inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %fd32, {%r32,%r33};
	// end inline asm
	mul.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB13_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

$L__BB13_16:
	ret;

}
	// .globl	reduce_min_i32
.visible .entry reduce_min_i32(
	.param .u64 reduce_min_i32_param_0,
	.param .u32 reduce_min_i32_param_1,
	.param .u64 reduce_min_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_min_i32_param_0];
	ld.param.u32 	%r23, [reduce_min_i32_param_1];
	ld.param.u64 	%rd2, [reduce_min_i32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r25, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r54, %r25, %r2;
	setp.ge.u32 	%p1, %r54, %r23;
	mov.u32 	%r55, 2147483647;
	@%p1 bra 	$L__BB14_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r4, %r27, 11;

$L__BB14_2:
	mul.wide.u32 	%rd4, %r54, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	min.s32 	%r55, %r55, %r28;
	add.s32 	%r8, %r54, 1024;
	setp.ge.u32 	%p2, %r8, %r23;
	@%p2 bra 	$L__BB14_4;

	mul.wide.u32 	%rd6, %r8, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	min.s32 	%r55, %r55, %r29;

$L__BB14_4:
	add.s32 	%r54, %r54, %r4;
	setp.lt.u32 	%p3, %r54, %r23;
	@%p3 bra 	$L__BB14_2;

$L__BB14_5:
	shl.b32 	%r30, %r2, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r13, %r31, %r30;
	st.shared.u32 	[%r13], %r55;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB14_7;

	ld.shared.u32 	%r32, [%r13+2048];
	min.s32 	%r55, %r55, %r32;
	st.shared.u32 	[%r13], %r55;

$L__BB14_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB14_9;

	ld.shared.u32 	%r33, [%r13+1024];
	min.s32 	%r55, %r55, %r33;
	st.shared.u32 	[%r13], %r55;

$L__BB14_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB14_11;

	ld.shared.u32 	%r34, [%r13+512];
	min.s32 	%r55, %r55, %r34;
	st.shared.u32 	[%r13], %r55;

$L__BB14_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB14_13;

	ld.shared.u32 	%r35, [%r13+256];
	min.s32 	%r55, %r55, %r35;
	st.shared.u32 	[%r13], %r55;

$L__BB14_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB14_16;

	ld.shared.u32 	%r36, [%r13+128];
	min.s32 	%r37, %r55, %r36;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	min.s32 	%r43, %r37, %r42;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	min.s32 	%r46, %r43, %r45;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	min.s32 	%r49, %r46, %r48;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	min.s32 	%r51, %r49, %r50;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	min.s32 	%r22, %r51, %r53;
	setp.ne.s32 	%p14, %r2, 0;
	@%p14 bra 	$L__BB14_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r22;

$L__BB14_16:
	ret;

}
	// .globl	reduce_min_u32
.visible .entry reduce_min_u32(
	.param .u64 reduce_min_u32_param_0,
	.param .u32 reduce_min_u32_param_1,
	.param .u64 reduce_min_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_min_u32_param_0];
	ld.param.u32 	%r23, [reduce_min_u32_param_1];
	ld.param.u64 	%rd2, [reduce_min_u32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r25, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r54, %r25, %r2;
	setp.ge.u32 	%p1, %r54, %r23;
	mov.u32 	%r55, -1;
	@%p1 bra 	$L__BB15_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r4, %r27, 11;

$L__BB15_2:
	mul.wide.u32 	%rd4, %r54, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	min.u32 	%r55, %r55, %r28;
	add.s32 	%r8, %r54, 1024;
	setp.ge.u32 	%p2, %r8, %r23;
	@%p2 bra 	$L__BB15_4;

	mul.wide.u32 	%rd6, %r8, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	min.u32 	%r55, %r55, %r29;

$L__BB15_4:
	add.s32 	%r54, %r54, %r4;
	setp.lt.u32 	%p3, %r54, %r23;
	@%p3 bra 	$L__BB15_2;

$L__BB15_5:
	shl.b32 	%r30, %r2, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r13, %r31, %r30;
	st.shared.u32 	[%r13], %r55;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB15_7;

	ld.shared.u32 	%r32, [%r13+2048];
	min.u32 	%r55, %r55, %r32;
	st.shared.u32 	[%r13], %r55;

$L__BB15_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB15_9;

	ld.shared.u32 	%r33, [%r13+1024];
	min.u32 	%r55, %r55, %r33;
	st.shared.u32 	[%r13], %r55;

$L__BB15_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB15_11;

	ld.shared.u32 	%r34, [%r13+512];
	min.u32 	%r55, %r55, %r34;
	st.shared.u32 	[%r13], %r55;

$L__BB15_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB15_13;

	ld.shared.u32 	%r35, [%r13+256];
	min.u32 	%r55, %r55, %r35;
	st.shared.u32 	[%r13], %r55;

$L__BB15_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB15_16;

	ld.shared.u32 	%r36, [%r13+128];
	min.u32 	%r37, %r55, %r36;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	min.u32 	%r43, %r37, %r42;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	min.u32 	%r46, %r43, %r45;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	min.u32 	%r49, %r46, %r48;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	min.u32 	%r51, %r49, %r50;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	min.u32 	%r22, %r51, %r53;
	setp.ne.s32 	%p14, %r2, 0;
	@%p14 bra 	$L__BB15_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r22;

$L__BB15_16:
	ret;

}
	// .globl	reduce_min_i64
.visible .entry reduce_min_i64(
	.param .u64 reduce_min_i64_param_0,
	.param .u32 reduce_min_i64_param_1,
	.param .u64 reduce_min_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_min_i64_param_0];
	ld.param.u32 	%r9, [reduce_min_i64_param_1];
	ld.param.u64 	%rd16, [reduce_min_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, 9223372036854775807;
	@%p1 bra 	$L__BB16_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB16_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	min.s64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB16_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	min.s64 	%rd44, %rd44, %rd25;

$L__BB16_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB16_2;

$L__BB16_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB16_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	min.s64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

$L__BB16_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB16_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	min.s64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

$L__BB16_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB16_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	min.s64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

$L__BB16_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB16_13;

	ld.shared.u64 	%rd29, [%r8+512];
	min.s64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

$L__BB16_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB16_16;

	ld.shared.u64 	%rd40, [%r8+256];
	min.s64 	%rd30, %rd44, %rd40;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	min.s64 	%rd32, %rd30, %rd31;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	min.s64 	%rd34, %rd32, %rd33;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	min.s64 	%rd36, %rd34, %rd35;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	min.s64 	%rd38, %rd36, %rd37;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	min.s64 	%rd15, %rd38, %rd39;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB16_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB16_16:
	ret;

}
	// .globl	reduce_min_u64
.visible .entry reduce_min_u64(
	.param .u64 reduce_min_u64_param_0,
	.param .u32 reduce_min_u64_param_1,
	.param .u64 reduce_min_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_min_u64_param_0];
	ld.param.u32 	%r9, [reduce_min_u64_param_1];
	ld.param.u64 	%rd16, [reduce_min_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, -1;
	@%p1 bra 	$L__BB17_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB17_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	min.u64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB17_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	min.u64 	%rd44, %rd44, %rd25;

$L__BB17_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB17_2;

$L__BB17_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB17_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	min.u64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

$L__BB17_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB17_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	min.u64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

$L__BB17_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB17_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	min.u64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

$L__BB17_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB17_13;

	ld.shared.u64 	%rd29, [%r8+512];
	min.u64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

$L__BB17_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB17_16;

	ld.shared.u64 	%rd40, [%r8+256];
	min.u64 	%rd30, %rd44, %rd40;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	min.u64 	%rd32, %rd30, %rd31;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	min.u64 	%rd34, %rd32, %rd33;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	min.u64 	%rd36, %rd34, %rd35;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	min.u64 	%rd38, %rd36, %rd37;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	min.u64 	%rd15, %rd38, %rd39;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB17_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB17_16:
	ret;

}
	// .globl	reduce_min_f16
.visible .entry reduce_min_f16(
	.param .u64 reduce_min_f16_param_0,
	.param .u32 reduce_min_f16_param_1,
	.param .u64 reduce_min_f16_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<79>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_min_f16_param_0];
	ld.param.u32 	%r8, [reduce_min_f16_param_1];
	ld.param.u64 	%rd2, [reduce_min_f16_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r65, %r10, %r1;
	setp.ge.u32 	%p1, %r65, %r8;
	mov.u16 	%rs72, 31743;
	@%p1 bra 	$L__BB18_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r3, %r11, 11;

$L__BB18_2:
	mul.wide.u32 	%rd4, %r65, 2;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u16 	%rs22, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs22;}

	// end inline asm
	min.f32 	%f3, %f1, %f2;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f3;}

	// end inline asm
	add.s32 	%r5, %r65, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB18_4;

	mul.wide.u32 	%rd6, %r5, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u16 	%rs25, [%rd7];
	// begin inline asm
	{  cvt.f32.f16 %f4, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs25;}

	// end inline asm
	min.f32 	%f6, %f4, %f5;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f6;}

	// end inline asm

$L__BB18_4:
	add.s32 	%r65, %r65, %r3;
	setp.lt.u32 	%p3, %r65, %r8;
	@%p3 bra 	$L__BB18_2;

$L__BB18_5:
	shl.b32 	%r12, %r1, 1;
	mov.u32 	%r13, shared;
	add.s32 	%r7, %r13, %r12;
	st.shared.u16 	[%r7], %rs72;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB18_7;

	ld.shared.u16 	%rs28, [%r7+1024];
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs28;}

	// end inline asm
	min.f32 	%f9, %f7, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f9;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB18_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB18_9;

	ld.shared.u16 	%rs31, [%r7+512];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs31;}

	// end inline asm
	min.f32 	%f12, %f10, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f12;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB18_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB18_11;

	ld.shared.u16 	%rs34, [%r7+256];
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs34;}

	// end inline asm
	min.f32 	%f15, %f13, %f14;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f15;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB18_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB18_13;

	ld.shared.u16 	%rs37, [%r7+128];
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs37;}

	// end inline asm
	min.f32 	%f18, %f16, %f17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f18;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB18_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB18_16;

	ld.shared.u16 	%rs40, [%r7+64];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs40;}

	// end inline asm
	min.f32 	%f21, %f19, %f20;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs41, %f21;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r14, {%rs41,%rs41};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r15, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r54, %r15, 8;
	mov.u32 	%r26, 8;
	add.s32 	%r55, %r54, -8192;
	or.b32  	%r19, %r55, 31;
	mov.u32 	%r18, 16;
	mov.u32 	%r52, -1;
	// begin inline asm
	{shfl.sync.down.b32 %r16,%r14,%r18,%r19,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r16;
 mov.b16 %rs44, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs41;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs44;}

	// end inline asm
	min.f32 	%f24, %f22, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs47, %f24;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r22, {%rs47,%rs47};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r23, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r56, %r23, 8;
	add.s32 	%r57, %r56, -8192;
	or.b32  	%r27, %r57, 31;
	// begin inline asm
	{shfl.sync.down.b32 %r24,%r22,%r26,%r27,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r24;
 mov.b16 %rs50, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f25, %rs47;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs50;}

	// end inline asm
	min.f32 	%f27, %f25, %f26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f27;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r30, {%rs53,%rs53};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r31, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r58, %r31, 8;
	add.s32 	%r59, %r58, -8192;
	or.b32  	%r35, %r59, 31;
	mov.u32 	%r34, 4;
	// begin inline asm
	{shfl.sync.down.b32 %r32,%r30,%r34,%r35,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r32;
 mov.b16 %rs56, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs53;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs56;}

	// end inline asm
	min.f32 	%f30, %f28, %f29;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs59, %f30;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r38, {%rs59,%rs59};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r39, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r60, %r39, 8;
	add.s32 	%r61, %r60, -8192;
	or.b32  	%r43, %r61, 31;
	mov.u32 	%r42, 2;
	// begin inline asm
	{shfl.sync.down.b32 %r40,%r38,%r42,%r43,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r40;
 mov.b16 %rs62, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs59;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs62;}

	// end inline asm
	min.f32 	%f33, %f31, %f32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f33;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r46, {%rs65,%rs65};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r47, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r62, %r47, 8;
	add.s32 	%r63, %r62, -8192;
	or.b32  	%r51, %r63, 31;
	mov.u32 	%r50, 1;
	// begin inline asm
	{shfl.sync.down.b32 %r48,%r46,%r50,%r51,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r48;
 mov.b16 %rs68, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f34, %rs65;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f35, %rs68;}

	// end inline asm
	min.f32 	%f36, %f34, %f35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f36;}

	// end inline asm
	setp.ne.s32 	%p9, %r1, 0;
	@%p9 bra 	$L__BB18_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r9, 2;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u16 	[%rd10], %rs71;

$L__BB18_16:
	ret;

}
	// .globl	reduce_min_f32
.visible .entry reduce_min_f32(
	.param .u64 reduce_min_f32_param_0,
	.param .u32 reduce_min_f32_param_1,
	.param .u64 reduce_min_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_min_f32_param_0];
	ld.param.u32 	%r8, [reduce_min_f32_param_1];
	ld.param.u64 	%rd2, [reduce_min_f32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r32, %r10, %r1;
	setp.ge.u32 	%p1, %r32, %r8;
	mov.f32 	%f34, 0f7F800000;
	@%p1 bra 	$L__BB19_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r3, %r11, 11;

$L__BB19_2:
	mul.wide.u32 	%rd4, %r32, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f17, [%rd5];
	min.f32 	%f34, %f34, %f17;
	add.s32 	%r5, %r32, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB19_4;

	mul.wide.u32 	%rd6, %r5, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f18, [%rd7];
	min.f32 	%f34, %f34, %f18;

$L__BB19_4:
	add.s32 	%r32, %r32, %r3;
	setp.lt.u32 	%p3, %r32, %r8;
	@%p3 bra 	$L__BB19_2;

$L__BB19_5:
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r7, %r13, %r12;
	st.shared.f32 	[%r7], %f34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB19_7;

	ld.shared.f32 	%f19, [%r7+2048];
	min.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r7], %f34;

$L__BB19_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB19_9;

	ld.shared.f32 	%f20, [%r7+1024];
	min.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r7], %f34;

$L__BB19_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB19_11;

	ld.shared.f32 	%f21, [%r7+512];
	min.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r7], %f34;

$L__BB19_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB19_13;

	ld.shared.f32 	%f22, [%r7+256];
	min.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r7], %f34;

$L__BB19_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB19_16;

	ld.shared.f32 	%f23, [%r7+128];
	min.f32 	%f24, %f34, %f23;
	mov.b32 	%r14, %f24;
	mov.u32 	%r15, 2;
	mov.u32 	%r16, 31;
	mov.u32 	%r17, 16;
	mov.u32 	%r18, -1;
	shfl.sync.down.b32 	%r19|%p9, %r14, %r17, %r16, %r18;
	mov.b32 	%f25, %r19;
	min.f32 	%f26, %f24, %f25;
	mov.b32 	%r20, %f26;
	mov.u32 	%r21, 8;
	shfl.sync.down.b32 	%r22|%p10, %r20, %r21, %r16, %r18;
	mov.b32 	%f27, %r22;
	min.f32 	%f28, %f26, %f27;
	mov.b32 	%r23, %f28;
	mov.u32 	%r24, 4;
	shfl.sync.down.b32 	%r25|%p11, %r23, %r24, %r16, %r18;
	mov.b32 	%f29, %r25;
	min.f32 	%f30, %f28, %f29;
	mov.b32 	%r26, %f30;
	shfl.sync.down.b32 	%r27|%p12, %r26, %r15, %r16, %r18;
	mov.b32 	%f31, %r27;
	min.f32 	%f32, %f30, %f31;
	mov.b32 	%r28, %f32;
	mov.u32 	%r29, 1;
	shfl.sync.down.b32 	%r30|%p13, %r28, %r29, %r16, %r18;
	mov.b32 	%f33, %r30;
	min.f32 	%f14, %f32, %f33;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB19_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r9, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f14;

$L__BB19_16:
	ret;

}
	// .globl	reduce_min_f64
.visible .entry reduce_min_f64(
	.param .u64 reduce_min_f64_param_0,
	.param .u32 reduce_min_f64_param_1,
	.param .u64 reduce_min_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_min_f64_param_0];
	ld.param.u32 	%r9, [reduce_min_f64_param_1];
	ld.param.u64 	%rd2, [reduce_min_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.f64 	%fd34, 0d7FF0000000000000;
	@%p1 bra 	$L__BB20_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB20_2:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	min.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB20_4;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	min.f64 	%fd34, %fd34, %fd18;

$L__BB20_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB20_2;

$L__BB20_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB20_7;

	ld.shared.f64 	%fd19, [%r8+4096];
	min.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

$L__BB20_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB20_9;

	ld.shared.f64 	%fd20, [%r8+2048];
	min.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

$L__BB20_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB20_11;

	ld.shared.f64 	%fd21, [%r8+1024];
	min.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

$L__BB20_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB20_13;

	ld.shared.f64 	%fd22, [%r8+512];
	min.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

$L__BB20_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB20_16;

	ld.shared.f64 	%fd33, [%r8+256];
	min.f64 	%fd23, %fd34, %fd33;
	// begin inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %fd24, {%r16,%r17};
	// end inline asm
	min.f64 	%fd25, %fd23, %fd24;
	// begin inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %fd26, {%r20,%r21};
	// end inline asm
	min.f64 	%fd27, %fd25, %fd26;
	// begin inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %fd28, {%r24,%r25};
	// end inline asm
	min.f64 	%fd29, %fd27, %fd28;
	// begin inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %fd30, {%r28,%r29};
	// end inline asm
	min.f64 	%fd31, %fd29, %fd30;
	// begin inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %fd32, {%r32,%r33};
	// end inline asm
	min.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB20_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

$L__BB20_16:
	ret;

}
	// .globl	reduce_max_i32
.visible .entry reduce_max_i32(
	.param .u64 reduce_max_i32_param_0,
	.param .u32 reduce_max_i32_param_1,
	.param .u64 reduce_max_i32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_max_i32_param_0];
	ld.param.u32 	%r23, [reduce_max_i32_param_1];
	ld.param.u64 	%rd2, [reduce_max_i32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r25, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r54, %r25, %r2;
	setp.ge.u32 	%p1, %r54, %r23;
	mov.u32 	%r55, -2147483648;
	@%p1 bra 	$L__BB21_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r4, %r27, 11;

$L__BB21_2:
	mul.wide.u32 	%rd4, %r54, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	max.s32 	%r55, %r55, %r28;
	add.s32 	%r8, %r54, 1024;
	setp.ge.u32 	%p2, %r8, %r23;
	@%p2 bra 	$L__BB21_4;

	mul.wide.u32 	%rd6, %r8, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	max.s32 	%r55, %r55, %r29;

$L__BB21_4:
	add.s32 	%r54, %r54, %r4;
	setp.lt.u32 	%p3, %r54, %r23;
	@%p3 bra 	$L__BB21_2;

$L__BB21_5:
	shl.b32 	%r30, %r2, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r13, %r31, %r30;
	st.shared.u32 	[%r13], %r55;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB21_7;

	ld.shared.u32 	%r32, [%r13+2048];
	max.s32 	%r55, %r55, %r32;
	st.shared.u32 	[%r13], %r55;

$L__BB21_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB21_9;

	ld.shared.u32 	%r33, [%r13+1024];
	max.s32 	%r55, %r55, %r33;
	st.shared.u32 	[%r13], %r55;

$L__BB21_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB21_11;

	ld.shared.u32 	%r34, [%r13+512];
	max.s32 	%r55, %r55, %r34;
	st.shared.u32 	[%r13], %r55;

$L__BB21_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB21_13;

	ld.shared.u32 	%r35, [%r13+256];
	max.s32 	%r55, %r55, %r35;
	st.shared.u32 	[%r13], %r55;

$L__BB21_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB21_16;

	ld.shared.u32 	%r36, [%r13+128];
	max.s32 	%r37, %r55, %r36;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	max.s32 	%r43, %r37, %r42;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	max.s32 	%r46, %r43, %r45;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	max.s32 	%r49, %r46, %r48;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	max.s32 	%r51, %r49, %r50;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	max.s32 	%r22, %r51, %r53;
	setp.ne.s32 	%p14, %r2, 0;
	@%p14 bra 	$L__BB21_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r22;

$L__BB21_16:
	ret;

}
	// .globl	reduce_max_u32
.visible .entry reduce_max_u32(
	.param .u64 reduce_max_u32_param_0,
	.param .u32 reduce_max_u32_param_1,
	.param .u64 reduce_max_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<62>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_max_u32_param_0];
	ld.param.u32 	%r23, [reduce_max_u32_param_1];
	ld.param.u64 	%rd2, [reduce_max_u32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r25, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r54, %r25, %r2;
	setp.ge.u32 	%p1, %r54, %r23;
	mov.u32 	%r55, 0;
	@%p1 bra 	$L__BB22_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r4, %r27, 11;

$L__BB22_2:
	mul.wide.u32 	%rd4, %r54, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	max.u32 	%r55, %r55, %r28;
	add.s32 	%r8, %r54, 1024;
	setp.ge.u32 	%p2, %r8, %r23;
	@%p2 bra 	$L__BB22_4;

	mul.wide.u32 	%rd6, %r8, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	max.u32 	%r55, %r55, %r29;

$L__BB22_4:
	add.s32 	%r54, %r54, %r4;
	setp.lt.u32 	%p3, %r54, %r23;
	@%p3 bra 	$L__BB22_2;

$L__BB22_5:
	shl.b32 	%r30, %r2, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r13, %r31, %r30;
	st.shared.u32 	[%r13], %r55;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB22_7;

	ld.shared.u32 	%r32, [%r13+2048];
	max.u32 	%r55, %r55, %r32;
	st.shared.u32 	[%r13], %r55;

$L__BB22_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB22_9;

	ld.shared.u32 	%r33, [%r13+1024];
	max.u32 	%r55, %r55, %r33;
	st.shared.u32 	[%r13], %r55;

$L__BB22_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB22_11;

	ld.shared.u32 	%r34, [%r13+512];
	max.u32 	%r55, %r55, %r34;
	st.shared.u32 	[%r13], %r55;

$L__BB22_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB22_13;

	ld.shared.u32 	%r35, [%r13+256];
	max.u32 	%r55, %r55, %r35;
	st.shared.u32 	[%r13], %r55;

$L__BB22_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB22_16;

	ld.shared.u32 	%r36, [%r13+128];
	max.u32 	%r37, %r55, %r36;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	max.u32 	%r43, %r37, %r42;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	max.u32 	%r46, %r43, %r45;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	max.u32 	%r49, %r46, %r48;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	max.u32 	%r51, %r49, %r50;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	max.u32 	%r22, %r51, %r53;
	setp.ne.s32 	%p14, %r2, 0;
	@%p14 bra 	$L__BB22_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r22;

$L__BB22_16:
	ret;

}
	// .globl	reduce_max_i64
.visible .entry reduce_max_i64(
	.param .u64 reduce_max_i64_param_0,
	.param .u32 reduce_max_i64_param_1,
	.param .u64 reduce_max_i64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_max_i64_param_0];
	ld.param.u32 	%r9, [reduce_max_i64_param_1];
	ld.param.u64 	%rd16, [reduce_max_i64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, -9223372036854775808;
	@%p1 bra 	$L__BB23_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB23_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	max.s64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB23_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	max.s64 	%rd44, %rd44, %rd25;

$L__BB23_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB23_2;

$L__BB23_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB23_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	max.s64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

$L__BB23_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB23_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	max.s64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

$L__BB23_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB23_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	max.s64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

$L__BB23_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB23_13;

	ld.shared.u64 	%rd29, [%r8+512];
	max.s64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

$L__BB23_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB23_16;

	ld.shared.u64 	%rd40, [%r8+256];
	max.s64 	%rd30, %rd44, %rd40;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	max.s64 	%rd32, %rd30, %rd31;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	max.s64 	%rd34, %rd32, %rd33;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	max.s64 	%rd36, %rd34, %rd35;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	max.s64 	%rd38, %rd36, %rd37;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	max.s64 	%rd15, %rd38, %rd39;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB23_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB23_16:
	ret;

}
	// .globl	reduce_max_u64
.visible .entry reduce_max_u64(
	.param .u64 reduce_max_u64_param_0,
	.param .u32 reduce_max_u64_param_1,
	.param .u64 reduce_max_u64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .b64 	%rd<51>;


	ld.param.u64 	%rd18, [reduce_max_u64_param_0];
	ld.param.u32 	%r9, [reduce_max_u64_param_1];
	ld.param.u64 	%rd16, [reduce_max_u64_param_2];
	cvta.to.global.u64 	%rd1, %rd18;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.u64 	%rd44, 0;
	@%p1 bra 	$L__BB24_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB24_2:
	mul.wide.u32 	%rd20, %r41, 8;
	add.s64 	%rd21, %rd1, %rd20;
	ld.global.u64 	%rd22, [%rd21];
	max.u64 	%rd44, %rd44, %rd22;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB24_4;

	mul.wide.u32 	%rd23, %r6, 8;
	add.s64 	%rd24, %rd1, %rd23;
	ld.global.u64 	%rd25, [%rd24];
	max.u64 	%rd44, %rd44, %rd25;

$L__BB24_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB24_2;

$L__BB24_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared;
	add.s32 	%r8, %r13, %r12;
	st.shared.u64 	[%r8], %rd44;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB24_7;

	ld.shared.u64 	%rd26, [%r8+4096];
	max.u64 	%rd44, %rd44, %rd26;
	st.shared.u64 	[%r8], %rd44;

$L__BB24_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB24_9;

	ld.shared.u64 	%rd27, [%r8+2048];
	max.u64 	%rd44, %rd44, %rd27;
	st.shared.u64 	[%r8], %rd44;

$L__BB24_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB24_11;

	ld.shared.u64 	%rd28, [%r8+1024];
	max.u64 	%rd44, %rd44, %rd28;
	st.shared.u64 	[%r8], %rd44;

$L__BB24_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB24_13;

	ld.shared.u64 	%rd29, [%r8+512];
	max.u64 	%rd44, %rd44, %rd29;
	st.shared.u64 	[%r8], %rd44;

$L__BB24_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB24_16;

	ld.shared.u64 	%rd40, [%r8+256];
	max.u64 	%rd30, %rd44, %rd40;
	// begin inline asm
	mov.b64 {%r14,%r15}, %rd30;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %rd31, {%r16,%r17};
	// end inline asm
	max.u64 	%rd32, %rd30, %rd31;
	// begin inline asm
	mov.b64 {%r18,%r19}, %rd32;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %rd33, {%r20,%r21};
	// end inline asm
	max.u64 	%rd34, %rd32, %rd33;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd34;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %rd35, {%r24,%r25};
	// end inline asm
	max.u64 	%rd36, %rd34, %rd35;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd36;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %rd37, {%r28,%r29};
	// end inline asm
	max.u64 	%rd38, %rd36, %rd37;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd38;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %rd39, {%r32,%r33};
	// end inline asm
	max.u64 	%rd15, %rd38, %rd39;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB24_16;

	cvta.to.global.u64 	%rd41, %rd16;
	mul.wide.u32 	%rd42, %r1, 8;
	add.s64 	%rd43, %rd41, %rd42;
	st.global.u64 	[%rd43], %rd15;

$L__BB24_16:
	ret;

}
	// .globl	reduce_max_f16
.visible .entry reduce_max_f16(
	.param .u64 reduce_max_f16_param_0,
	.param .u32 reduce_max_f16_param_1,
	.param .u64 reduce_max_f16_param_2
)
{
	.reg .pred 	%p<10>;
	.reg .b16 	%rs<79>;
	.reg .f32 	%f<37>;
	.reg .b32 	%r<66>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_max_f16_param_0];
	ld.param.u32 	%r8, [reduce_max_f16_param_1];
	ld.param.u64 	%rd2, [reduce_max_f16_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r65, %r10, %r1;
	setp.ge.u32 	%p1, %r65, %r8;
	mov.u16 	%rs72, -1024;
	@%p1 bra 	$L__BB25_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r3, %r11, 11;

$L__BB25_2:
	mul.wide.u32 	%rd4, %r65, 2;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u16 	%rs22, [%rd5];
	// begin inline asm
	{  cvt.f32.f16 %f1, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f2, %rs22;}

	// end inline asm
	max.f32 	%f3, %f1, %f2;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f3;}

	// end inline asm
	add.s32 	%r5, %r65, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB25_4;

	mul.wide.u32 	%rd6, %r5, 2;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u16 	%rs25, [%rd7];
	// begin inline asm
	{  cvt.f32.f16 %f4, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f5, %rs25;}

	// end inline asm
	max.f32 	%f6, %f4, %f5;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f6;}

	// end inline asm

$L__BB25_4:
	add.s32 	%r65, %r65, %r3;
	setp.lt.u32 	%p3, %r65, %r8;
	@%p3 bra 	$L__BB25_2;

$L__BB25_5:
	shl.b32 	%r12, %r1, 1;
	mov.u32 	%r13, shared;
	add.s32 	%r7, %r13, %r12;
	st.shared.u16 	[%r7], %rs72;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB25_7;

	ld.shared.u16 	%rs28, [%r7+1024];
	// begin inline asm
	{  cvt.f32.f16 %f7, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f8, %rs28;}

	// end inline asm
	max.f32 	%f9, %f7, %f8;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f9;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB25_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB25_9;

	ld.shared.u16 	%rs31, [%r7+512];
	// begin inline asm
	{  cvt.f32.f16 %f10, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f11, %rs31;}

	// end inline asm
	max.f32 	%f12, %f10, %f11;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f12;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB25_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB25_11;

	ld.shared.u16 	%rs34, [%r7+256];
	// begin inline asm
	{  cvt.f32.f16 %f13, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f14, %rs34;}

	// end inline asm
	max.f32 	%f15, %f13, %f14;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f15;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB25_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB25_13;

	ld.shared.u16 	%rs37, [%r7+128];
	// begin inline asm
	{  cvt.f32.f16 %f16, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f17, %rs37;}

	// end inline asm
	max.f32 	%f18, %f16, %f17;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs72, %f18;}

	// end inline asm
	st.shared.u16 	[%r7], %rs72;

$L__BB25_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB25_16;

	ld.shared.u16 	%rs40, [%r7+64];
	// begin inline asm
	{  cvt.f32.f16 %f19, %rs72;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f20, %rs40;}

	// end inline asm
	max.f32 	%f21, %f19, %f20;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs41, %f21;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r14, {%rs41,%rs41};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r15, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r54, %r15, 8;
	mov.u32 	%r26, 8;
	add.s32 	%r55, %r54, -8192;
	or.b32  	%r19, %r55, 31;
	mov.u32 	%r18, 16;
	mov.u32 	%r52, -1;
	// begin inline asm
	{shfl.sync.down.b32 %r16,%r14,%r18,%r19,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r16;
 mov.b16 %rs44, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f22, %rs41;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f23, %rs44;}

	// end inline asm
	max.f32 	%f24, %f22, %f23;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs47, %f24;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r22, {%rs47,%rs47};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r23, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r56, %r23, 8;
	add.s32 	%r57, %r56, -8192;
	or.b32  	%r27, %r57, 31;
	// begin inline asm
	{shfl.sync.down.b32 %r24,%r22,%r26,%r27,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r24;
 mov.b16 %rs50, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f25, %rs47;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f26, %rs50;}

	// end inline asm
	max.f32 	%f27, %f25, %f26;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs53, %f27;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r30, {%rs53,%rs53};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r31, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r58, %r31, 8;
	add.s32 	%r59, %r58, -8192;
	or.b32  	%r35, %r59, 31;
	mov.u32 	%r34, 4;
	// begin inline asm
	{shfl.sync.down.b32 %r32,%r30,%r34,%r35,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r32;
 mov.b16 %rs56, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f28, %rs53;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f29, %rs56;}

	// end inline asm
	max.f32 	%f30, %f28, %f29;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs59, %f30;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r38, {%rs59,%rs59};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r39, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r60, %r39, 8;
	add.s32 	%r61, %r60, -8192;
	or.b32  	%r43, %r61, 31;
	mov.u32 	%r42, 2;
	// begin inline asm
	{shfl.sync.down.b32 %r40,%r38,%r42,%r43,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r40;
 mov.b16 %rs62, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f31, %rs59;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f32, %rs62;}

	// end inline asm
	max.f32 	%f33, %f31, %f32;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs65, %f33;}

	// end inline asm
	// begin inline asm
	{  mov.b32 %r46, {%rs65,%rs65};}

	// end inline asm
	// begin inline asm
	{mov.u32 %r47, WARP_SZ;
}
	// end inline asm
	shl.b32 	%r62, %r47, 8;
	add.s32 	%r63, %r62, -8192;
	or.b32  	%r51, %r63, 31;
	mov.u32 	%r50, 1;
	// begin inline asm
	{shfl.sync.down.b32 %r48,%r46,%r50,%r51,%r52;
}
	// end inline asm
	// begin inline asm
	{.reg .f16 low,high;
 mov.b32 {low,high}, %r48;
 mov.b16 %rs68, low;}
	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f34, %rs65;}

	// end inline asm
	// begin inline asm
	{  cvt.f32.f16 %f35, %rs68;}

	// end inline asm
	max.f32 	%f36, %f34, %f35;
	// begin inline asm
	{  cvt.rn.f16.f32 %rs71, %f36;}

	// end inline asm
	setp.ne.s32 	%p9, %r1, 0;
	@%p9 bra 	$L__BB25_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r9, 2;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u16 	[%rd10], %rs71;

$L__BB25_16:
	ret;

}
	// .globl	reduce_max_f32
.visible .entry reduce_max_f32(
	.param .u64 reduce_max_f32_param_0,
	.param .u32 reduce_max_f32_param_1,
	.param .u64 reduce_max_f32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .f32 	%f<41>;
	.reg .b32 	%r<33>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_max_f32_param_0];
	ld.param.u32 	%r8, [reduce_max_f32_param_1];
	ld.param.u64 	%rd2, [reduce_max_f32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r9, %ctaid.x;
	shl.b32 	%r10, %r9, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r32, %r10, %r1;
	setp.ge.u32 	%p1, %r32, %r8;
	mov.f32 	%f34, 0fFF800000;
	@%p1 bra 	$L__BB26_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r3, %r11, 11;

$L__BB26_2:
	mul.wide.u32 	%rd4, %r32, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f32 	%f17, [%rd5];
	max.f32 	%f34, %f34, %f17;
	add.s32 	%r5, %r32, 1024;
	setp.ge.u32 	%p2, %r5, %r8;
	@%p2 bra 	$L__BB26_4;

	mul.wide.u32 	%rd6, %r5, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f32 	%f18, [%rd7];
	max.f32 	%f34, %f34, %f18;

$L__BB26_4:
	add.s32 	%r32, %r32, %r3;
	setp.lt.u32 	%p3, %r32, %r8;
	@%p3 bra 	$L__BB26_2;

$L__BB26_5:
	shl.b32 	%r12, %r1, 2;
	mov.u32 	%r13, shared;
	add.s32 	%r7, %r13, %r12;
	st.shared.f32 	[%r7], %f34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB26_7;

	ld.shared.f32 	%f19, [%r7+2048];
	max.f32 	%f34, %f34, %f19;
	st.shared.f32 	[%r7], %f34;

$L__BB26_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB26_9;

	ld.shared.f32 	%f20, [%r7+1024];
	max.f32 	%f34, %f34, %f20;
	st.shared.f32 	[%r7], %f34;

$L__BB26_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB26_11;

	ld.shared.f32 	%f21, [%r7+512];
	max.f32 	%f34, %f34, %f21;
	st.shared.f32 	[%r7], %f34;

$L__BB26_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB26_13;

	ld.shared.f32 	%f22, [%r7+256];
	max.f32 	%f34, %f34, %f22;
	st.shared.f32 	[%r7], %f34;

$L__BB26_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB26_16;

	ld.shared.f32 	%f23, [%r7+128];
	max.f32 	%f24, %f34, %f23;
	mov.b32 	%r14, %f24;
	mov.u32 	%r15, 2;
	mov.u32 	%r16, 31;
	mov.u32 	%r17, 16;
	mov.u32 	%r18, -1;
	shfl.sync.down.b32 	%r19|%p9, %r14, %r17, %r16, %r18;
	mov.b32 	%f25, %r19;
	max.f32 	%f26, %f24, %f25;
	mov.b32 	%r20, %f26;
	mov.u32 	%r21, 8;
	shfl.sync.down.b32 	%r22|%p10, %r20, %r21, %r16, %r18;
	mov.b32 	%f27, %r22;
	max.f32 	%f28, %f26, %f27;
	mov.b32 	%r23, %f28;
	mov.u32 	%r24, 4;
	shfl.sync.down.b32 	%r25|%p11, %r23, %r24, %r16, %r18;
	mov.b32 	%f29, %r25;
	max.f32 	%f30, %f28, %f29;
	mov.b32 	%r26, %f30;
	shfl.sync.down.b32 	%r27|%p12, %r26, %r15, %r16, %r18;
	mov.b32 	%f31, %r27;
	max.f32 	%f32, %f30, %f31;
	mov.b32 	%r28, %f32;
	mov.u32 	%r29, 1;
	shfl.sync.down.b32 	%r30|%p13, %r28, %r29, %r16, %r18;
	mov.b32 	%f33, %r30;
	max.f32 	%f14, %f32, %f33;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB26_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r9, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f32 	[%rd10], %f14;

$L__BB26_16:
	ret;

}
	// .globl	reduce_max_f64
.visible .entry reduce_max_f64(
	.param .u64 reduce_max_f64_param_0,
	.param .u32 reduce_max_f64_param_1,
	.param .u64 reduce_max_f64_param_2
)
{
	.reg .pred 	%p<20>;
	.reg .b32 	%r<42>;
	.reg .f64 	%fd<41>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_max_f64_param_0];
	ld.param.u32 	%r9, [reduce_max_f64_param_1];
	ld.param.u64 	%rd2, [reduce_max_f64_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r10, %r1, 11;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r41, %r10, %r2;
	setp.ge.u32 	%p1, %r41, %r9;
	mov.f64 	%fd34, 0dFFF0000000000000;
	@%p1 bra 	$L__BB27_5;

	mov.u32 	%r11, %nctaid.x;
	shl.b32 	%r4, %r11, 11;

$L__BB27_2:
	mul.wide.u32 	%rd4, %r41, 8;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.f64 	%fd17, [%rd5];
	max.f64 	%fd34, %fd34, %fd17;
	add.s32 	%r6, %r41, 1024;
	setp.ge.u32 	%p2, %r6, %r9;
	@%p2 bra 	$L__BB27_4;

	mul.wide.u32 	%rd6, %r6, 8;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.f64 	%fd18, [%rd7];
	max.f64 	%fd34, %fd34, %fd18;

$L__BB27_4:
	add.s32 	%r41, %r41, %r4;
	setp.lt.u32 	%p3, %r41, %r9;
	@%p3 bra 	$L__BB27_2;

$L__BB27_5:
	shl.b32 	%r12, %r2, 3;
	mov.u32 	%r13, shared_d;
	add.s32 	%r8, %r13, %r12;
	st.shared.f64 	[%r8], %fd34;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r2, 511;
	@%p4 bra 	$L__BB27_7;

	ld.shared.f64 	%fd19, [%r8+4096];
	max.f64 	%fd34, %fd34, %fd19;
	st.shared.f64 	[%r8], %fd34;

$L__BB27_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r2, 255;
	@%p5 bra 	$L__BB27_9;

	ld.shared.f64 	%fd20, [%r8+2048];
	max.f64 	%fd34, %fd34, %fd20;
	st.shared.f64 	[%r8], %fd34;

$L__BB27_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r2, 127;
	@%p6 bra 	$L__BB27_11;

	ld.shared.f64 	%fd21, [%r8+1024];
	max.f64 	%fd34, %fd34, %fd21;
	st.shared.f64 	[%r8], %fd34;

$L__BB27_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r2, 63;
	@%p7 bra 	$L__BB27_13;

	ld.shared.f64 	%fd22, [%r8+512];
	max.f64 	%fd34, %fd34, %fd22;
	st.shared.f64 	[%r8], %fd34;

$L__BB27_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r2, 31;
	@%p8 bra 	$L__BB27_16;

	ld.shared.f64 	%fd33, [%r8+256];
	max.f64 	%fd23, %fd34, %fd33;
	// begin inline asm
	mov.b64 {%r14,%r15}, %fd23;
	// end inline asm
	mov.u32 	%r34, 2;
	mov.u32 	%r35, 31;
	mov.u32 	%r36, 16;
	mov.u32 	%r37, -1;
	shfl.sync.down.b32 	%r17|%p9, %r15, %r36, %r35, %r37;
	shfl.sync.down.b32 	%r16|%p10, %r14, %r36, %r35, %r37;
	// begin inline asm
	mov.b64 %fd24, {%r16,%r17};
	// end inline asm
	max.f64 	%fd25, %fd23, %fd24;
	// begin inline asm
	mov.b64 {%r18,%r19}, %fd25;
	// end inline asm
	mov.u32 	%r38, 8;
	shfl.sync.down.b32 	%r21|%p11, %r19, %r38, %r35, %r37;
	shfl.sync.down.b32 	%r20|%p12, %r18, %r38, %r35, %r37;
	// begin inline asm
	mov.b64 %fd26, {%r20,%r21};
	// end inline asm
	max.f64 	%fd27, %fd25, %fd26;
	// begin inline asm
	mov.b64 {%r22,%r23}, %fd27;
	// end inline asm
	mov.u32 	%r39, 4;
	shfl.sync.down.b32 	%r25|%p13, %r23, %r39, %r35, %r37;
	shfl.sync.down.b32 	%r24|%p14, %r22, %r39, %r35, %r37;
	// begin inline asm
	mov.b64 %fd28, {%r24,%r25};
	// end inline asm
	max.f64 	%fd29, %fd27, %fd28;
	// begin inline asm
	mov.b64 {%r26,%r27}, %fd29;
	// end inline asm
	shfl.sync.down.b32 	%r29|%p15, %r27, %r34, %r35, %r37;
	shfl.sync.down.b32 	%r28|%p16, %r26, %r34, %r35, %r37;
	// begin inline asm
	mov.b64 %fd30, {%r28,%r29};
	// end inline asm
	max.f64 	%fd31, %fd29, %fd30;
	// begin inline asm
	mov.b64 {%r30,%r31}, %fd31;
	// end inline asm
	mov.u32 	%r40, 1;
	shfl.sync.down.b32 	%r33|%p17, %r31, %r40, %r35, %r37;
	shfl.sync.down.b32 	%r32|%p18, %r30, %r40, %r35, %r37;
	// begin inline asm
	mov.b64 %fd32, {%r32,%r33};
	// end inline asm
	max.f64 	%fd14, %fd31, %fd32;
	setp.ne.s32 	%p19, %r2, 0;
	@%p19 bra 	$L__BB27_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r1, 8;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.f64 	[%rd10], %fd14;

$L__BB27_16:
	ret;

}
	// .globl	reduce_or_u32
.visible .entry reduce_or_u32(
	.param .u64 reduce_or_u32_param_0,
	.param .u32 reduce_or_u32_param_1,
	.param .u64 reduce_or_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_or_u32_param_0];
	ld.param.u32 	%r22, [reduce_or_u32_param_1];
	ld.param.u64 	%rd2, [reduce_or_u32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r24, %ctaid.x;
	shl.b32 	%r25, %r24, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r55, %r25, %r1;
	setp.ge.u32 	%p1, %r55, %r22;
	mov.u32 	%r56, 0;
	@%p1 bra 	$L__BB28_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r3, %r27, 11;

$L__BB28_2:
	mul.wide.u32 	%rd4, %r55, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	or.b32  	%r56, %r28, %r56;
	add.s32 	%r7, %r55, 1024;
	setp.ge.u32 	%p2, %r7, %r22;
	@%p2 bra 	$L__BB28_4;

	mul.wide.u32 	%rd6, %r7, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	or.b32  	%r56, %r29, %r56;

$L__BB28_4:
	add.s32 	%r55, %r55, %r3;
	setp.lt.u32 	%p3, %r55, %r22;
	@%p3 bra 	$L__BB28_2;

$L__BB28_5:
	shl.b32 	%r30, %r1, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r12, %r31, %r30;
	st.shared.u32 	[%r12], %r56;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB28_7;

	ld.shared.u32 	%r32, [%r12+2048];
	or.b32  	%r56, %r32, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB28_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB28_9;

	ld.shared.u32 	%r33, [%r12+1024];
	or.b32  	%r56, %r33, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB28_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB28_11;

	ld.shared.u32 	%r34, [%r12+512];
	or.b32  	%r56, %r34, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB28_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB28_13;

	ld.shared.u32 	%r35, [%r12+256];
	or.b32  	%r56, %r35, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB28_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB28_16;

	ld.shared.u32 	%r36, [%r12+128];
	or.b32  	%r37, %r36, %r56;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	or.b32  	%r43, %r42, %r37;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	or.b32  	%r46, %r45, %r43;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	or.b32  	%r49, %r48, %r46;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	or.b32  	%r51, %r50, %r49;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	or.b32  	%r21, %r53, %r51;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB28_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r24, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

$L__BB28_16:
	ret;

}
	// .globl	reduce_and_u32
.visible .entry reduce_and_u32(
	.param .u64 reduce_and_u32_param_0,
	.param .u32 reduce_and_u32_param_1,
	.param .u64 reduce_and_u32_param_2
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<63>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [reduce_and_u32_param_0];
	ld.param.u32 	%r22, [reduce_and_u32_param_1];
	ld.param.u64 	%rd2, [reduce_and_u32_param_2];
	cvta.to.global.u64 	%rd1, %rd3;
	mov.u32 	%r24, %ctaid.x;
	shl.b32 	%r25, %r24, 11;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r55, %r25, %r1;
	setp.ge.u32 	%p1, %r55, %r22;
	mov.u32 	%r56, -1;
	@%p1 bra 	$L__BB29_5;

	mov.u32 	%r27, %nctaid.x;
	shl.b32 	%r3, %r27, 11;

$L__BB29_2:
	mul.wide.u32 	%rd4, %r55, 4;
	add.s64 	%rd5, %rd1, %rd4;
	ld.global.u32 	%r28, [%rd5];
	and.b32  	%r56, %r28, %r56;
	add.s32 	%r7, %r55, 1024;
	setp.ge.u32 	%p2, %r7, %r22;
	@%p2 bra 	$L__BB29_4;

	mul.wide.u32 	%rd6, %r7, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r29, [%rd7];
	and.b32  	%r56, %r29, %r56;

$L__BB29_4:
	add.s32 	%r55, %r55, %r3;
	setp.lt.u32 	%p3, %r55, %r22;
	@%p3 bra 	$L__BB29_2;

$L__BB29_5:
	shl.b32 	%r30, %r1, 2;
	mov.u32 	%r31, shared;
	add.s32 	%r12, %r31, %r30;
	st.shared.u32 	[%r12], %r56;
	bar.sync 	0;
	setp.gt.u32 	%p4, %r1, 511;
	@%p4 bra 	$L__BB29_7;

	ld.shared.u32 	%r32, [%r12+2048];
	and.b32  	%r56, %r32, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB29_7:
	bar.sync 	0;
	setp.gt.u32 	%p5, %r1, 255;
	@%p5 bra 	$L__BB29_9;

	ld.shared.u32 	%r33, [%r12+1024];
	and.b32  	%r56, %r33, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB29_9:
	bar.sync 	0;
	setp.gt.u32 	%p6, %r1, 127;
	@%p6 bra 	$L__BB29_11;

	ld.shared.u32 	%r34, [%r12+512];
	and.b32  	%r56, %r34, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB29_11:
	bar.sync 	0;
	setp.gt.u32 	%p7, %r1, 63;
	@%p7 bra 	$L__BB29_13;

	ld.shared.u32 	%r35, [%r12+256];
	and.b32  	%r56, %r35, %r56;
	st.shared.u32 	[%r12], %r56;

$L__BB29_13:
	bar.sync 	0;
	setp.gt.u32 	%p8, %r1, 31;
	@%p8 bra 	$L__BB29_16;

	ld.shared.u32 	%r36, [%r12+128];
	and.b32  	%r37, %r36, %r56;
	mov.u32 	%r38, 2;
	mov.u32 	%r39, 31;
	mov.u32 	%r40, 16;
	mov.u32 	%r41, -1;
	shfl.sync.down.b32 	%r42|%p9, %r37, %r40, %r39, %r41;
	and.b32  	%r43, %r42, %r37;
	mov.u32 	%r44, 8;
	shfl.sync.down.b32 	%r45|%p10, %r43, %r44, %r39, %r41;
	and.b32  	%r46, %r45, %r43;
	mov.u32 	%r47, 4;
	shfl.sync.down.b32 	%r48|%p11, %r46, %r47, %r39, %r41;
	and.b32  	%r49, %r48, %r46;
	shfl.sync.down.b32 	%r50|%p12, %r49, %r38, %r39, %r41;
	and.b32  	%r51, %r50, %r49;
	mov.u32 	%r52, 1;
	shfl.sync.down.b32 	%r53|%p13, %r51, %r52, %r39, %r41;
	and.b32  	%r21, %r53, %r51;
	setp.ne.s32 	%p14, %r1, 0;
	@%p14 bra 	$L__BB29_16;

	cvta.to.global.u64 	%rd8, %rd2;
	mul.wide.u32 	%rd9, %r24, 4;
	add.s64 	%rd10, %rd8, %rd9;
	st.global.u32 	[%rd10], %r21;

$L__BB29_16:
	ret;

}
	// .globl	prefix_sum_large_init
.visible .entry prefix_sum_large_init(
	.param .u64 prefix_sum_large_init_param_0,
	.param .u32 prefix_sum_large_init_param_1
)
{
	.reg .pred 	%p<4>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [prefix_sum_large_init_param_0];
	ld.param.u32 	%r6, [prefix_sum_large_init_param_1];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r7, %r1, %r8;
	setp.ge.u32 	%p1, %r10, %r6;
	@%p1 bra 	$L__BB30_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r9;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB30_2:
	setp.lt.u32 	%p2, %r10, 32;
	selp.b64 	%rd3, 2, 0, %p2;
	mul.wide.u32 	%rd4, %r10, 8;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.u64 	[%rd5], %rd3;
	add.s32 	%r10, %r10, %r3;
	setp.lt.u32 	%p3, %r10, %r6;
	@%p3 bra 	$L__BB30_2;

$L__BB30_3:
	ret;

}
	// .globl	prefix_sum_exc_small_u32
.visible .entry prefix_sum_exc_small_u32(
	.param .u64 prefix_sum_exc_small_u32_param_0,
	.param .u64 prefix_sum_exc_small_u32_param_1,
	.param .u32 prefix_sum_exc_small_u32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [prefix_sum_exc_small_u32_param_0];
	ld.param.u64 	%rd2, [prefix_sum_exc_small_u32_param_1];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.u32 	{%r15, %r16, %r17, %r18}, [%rd6];
	mov.u32 	%r19, 0;
	add.s32 	%r3, %r16, %r15;
	add.s32 	%r4, %r17, %r3;
	add.s32 	%r5, %r18, %r4;
	shl.b32 	%r23, %r1, 2;
	mov.u32 	%r24, shared;
	add.s32 	%r6, %r24, %r23;
	st.shared.u32 	[%r6], %r19;
	mov.u32 	%r7, %ntid.x;
	setp.lt.u32 	%p1, %r7, 2;
	mov.u32 	%r39, %r5;
	@%p1 bra 	$L__BB31_3;

	shl.b32 	%r26, %r7, 2;
	add.s32 	%r8, %r6, %r26;
	add.s32 	%r9, %r1, %r7;
	mov.u32 	%r37, 1;
	mov.u32 	%r39, %r5;

$L__BB31_2:
	st.shared.u32 	[%r8], %r39;
	bar.sync 	0;
	sub.s32 	%r27, %r9, %r37;
	shl.b32 	%r28, %r27, 2;
	add.s32 	%r30, %r24, %r28;
	ld.shared.u32 	%r31, [%r30];
	ld.shared.u32 	%r32, [%r8];
	add.s32 	%r39, %r31, %r32;
	bar.sync 	0;
	shl.b32 	%r37, %r37, 1;
	setp.lt.u32 	%p2, %r37, %r7;
	@%p2 bra 	$L__BB31_2;

$L__BB31_3:
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	sub.s32 	%r33, %r39, %r5;
	add.s32 	%r34, %r33, %r4;
	add.s32 	%r35, %r33, %r3;
	add.s32 	%r36, %r33, %r15;
	st.global.v4.u32 	[%rd9], {%r33, %r36, %r35, %r34};
	ret;

}
	// .globl	prefix_sum_exc_small_u64
.visible .entry prefix_sum_exc_small_u64(
	.param .u64 prefix_sum_exc_small_u64_param_0,
	.param .u64 prefix_sum_exc_small_u64_param_1,
	.param .u32 prefix_sum_exc_small_u64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd8, [prefix_sum_exc_small_u64_param_0];
	ld.param.u64 	%rd7, [prefix_sum_exc_small_u64_param_1];
	cvta.to.global.u64 	%rd9, %rd8;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd10, %r1, 16;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.v2.u64 	{%rd12, %rd13}, [%rd11];
	mov.u64 	%rd14, 0;
	add.s64 	%rd3, %rd13, %rd12;
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r2, %r9, %r8;
	st.shared.u64 	[%r2], %rd14;
	mov.u32 	%r3, %ntid.x;
	setp.lt.u32 	%p1, %r3, 2;
	mov.u64 	%rd24, %rd3;
	@%p1 bra 	$L__BB32_3;

	shl.b32 	%r11, %r3, 3;
	add.s32 	%r4, %r2, %r11;
	add.s32 	%r5, %r1, %r3;
	mov.u32 	%r16, 1;
	mov.u64 	%rd24, %rd3;

$L__BB32_2:
	st.shared.u64 	[%r4], %rd24;
	bar.sync 	0;
	sub.s32 	%r12, %r5, %r16;
	shl.b32 	%r13, %r12, 3;
	add.s32 	%r15, %r9, %r13;
	ld.shared.u64 	%rd16, [%r15];
	ld.shared.u64 	%rd17, [%r4];
	add.s64 	%rd24, %rd16, %rd17;
	bar.sync 	0;
	shl.b32 	%r16, %r16, 1;
	setp.lt.u32 	%p2, %r16, %r3;
	@%p2 bra 	$L__BB32_2;

$L__BB32_3:
	cvta.to.global.u64 	%rd18, %rd7;
	shl.b64 	%rd19, %rd1, 4;
	add.s64 	%rd20, %rd18, %rd19;
	sub.s64 	%rd21, %rd24, %rd3;
	add.s64 	%rd22, %rd21, %rd12;
	st.global.v2.u64 	[%rd20], {%rd21, %rd22};
	ret;

}
	// .globl	prefix_sum_exc_small_f32
.visible .entry prefix_sum_exc_small_f32(
	.param .u64 prefix_sum_exc_small_f32_param_0,
	.param .u64 prefix_sum_exc_small_f32_param_1,
	.param .u32 prefix_sum_exc_small_f32_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [prefix_sum_exc_small_f32_param_0];
	ld.param.u64 	%rd2, [prefix_sum_exc_small_f32_param_1];
	ld.param.u32 	%r8, [prefix_sum_exc_small_f32_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.f32 	{%f8, %f9, %f10, %f11}, [%rd6];
	shl.b32 	%r9, %r1, 2;
	setp.lt.u32 	%p1, %r9, %r8;
	add.f32 	%f16, %f8, 0f00000000;
	mov.u32 	%r10, 0;
	selp.f32 	%f1, %f16, 0f00000000, %p1;
	or.b32  	%r11, %r9, 1;
	setp.lt.u32 	%p2, %r11, %r8;
	selp.f32 	%f17, %f9, 0f00000000, %p2;
	add.f32 	%f2, %f1, %f17;
	or.b32  	%r12, %r9, 2;
	setp.lt.u32 	%p3, %r12, %r8;
	selp.f32 	%f18, %f10, 0f00000000, %p3;
	add.f32 	%f3, %f2, %f18;
	or.b32  	%r13, %r9, 3;
	setp.lt.u32 	%p4, %r13, %r8;
	selp.f32 	%f19, %f11, 0f00000000, %p4;
	add.f32 	%f4, %f3, %f19;
	mov.u32 	%r14, shared;
	add.s32 	%r2, %r14, %r9;
	st.shared.u32 	[%r2], %r10;
	mov.u32 	%r3, %ntid.x;
	setp.lt.u32 	%p5, %r3, 2;
	mov.f32 	%f28, %f4;
	@%p5 bra 	$L__BB33_3;

	shl.b32 	%r16, %r3, 2;
	add.s32 	%r4, %r2, %r16;
	add.s32 	%r5, %r1, %r3;
	mov.u32 	%r21, 1;
	mov.f32 	%f28, %f4;

$L__BB33_2:
	st.shared.f32 	[%r4], %f28;
	bar.sync 	0;
	sub.s32 	%r17, %r5, %r21;
	shl.b32 	%r18, %r17, 2;
	add.s32 	%r20, %r14, %r18;
	ld.shared.f32 	%f20, [%r20];
	ld.shared.f32 	%f21, [%r4];
	add.f32 	%f28, %f21, %f20;
	bar.sync 	0;
	shl.b32 	%r21, %r21, 1;
	setp.lt.u32 	%p6, %r21, %r3;
	@%p6 bra 	$L__BB33_2;

$L__BB33_3:
	sub.f32 	%f22, %f28, %f4;
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	add.f32 	%f23, %f22, %f3;
	add.f32 	%f24, %f22, %f2;
	add.f32 	%f25, %f22, %f1;
	add.f32 	%f26, %f22, 0f00000000;
	st.global.v4.f32 	[%rd9], {%f26, %f25, %f24, %f23};
	ret;

}
	// .globl	prefix_sum_exc_small_f64
.visible .entry prefix_sum_exc_small_f64(
	.param .u64 prefix_sum_exc_small_f64_param_0,
	.param .u64 prefix_sum_exc_small_f64_param_1,
	.param .u32 prefix_sum_exc_small_f64_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [prefix_sum_exc_small_f64_param_0];
	ld.param.u64 	%rd2, [prefix_sum_exc_small_f64_param_1];
	ld.param.u32 	%r8, [prefix_sum_exc_small_f64_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v2.f64 	{%fd6, %fd7}, [%rd6];
	mov.u64 	%rd7, 0;
	shl.b32 	%r9, %r1, 1;
	setp.lt.u32 	%p1, %r9, %r8;
	add.f64 	%fd10, %fd6, 0d0000000000000000;
	selp.f64 	%fd1, %fd10, 0d0000000000000000, %p1;
	or.b32  	%r10, %r9, 1;
	setp.lt.u32 	%p2, %r10, %r8;
	selp.f64 	%fd11, %fd7, 0d0000000000000000, %p2;
	add.f64 	%fd2, %fd1, %fd11;
	shl.b32 	%r11, %r1, 3;
	mov.u32 	%r12, shared_d;
	add.s32 	%r2, %r12, %r11;
	st.shared.u64 	[%r2], %rd7;
	mov.u32 	%r3, %ntid.x;
	setp.lt.u32 	%p3, %r3, 2;
	mov.f64 	%fd18, %fd2;
	@%p3 bra 	$L__BB34_3;

	shl.b32 	%r14, %r3, 3;
	add.s32 	%r4, %r2, %r14;
	add.s32 	%r5, %r1, %r3;
	mov.u32 	%r19, 1;
	mov.f64 	%fd18, %fd2;

$L__BB34_2:
	st.shared.f64 	[%r4], %fd18;
	bar.sync 	0;
	sub.s32 	%r15, %r5, %r19;
	shl.b32 	%r16, %r15, 3;
	add.s32 	%r18, %r12, %r16;
	ld.shared.f64 	%fd12, [%r18];
	ld.shared.f64 	%fd13, [%r4];
	add.f64 	%fd18, %fd13, %fd12;
	bar.sync 	0;
	shl.b32 	%r19, %r19, 1;
	setp.lt.u32 	%p4, %r19, %r3;
	@%p4 bra 	$L__BB34_2;

$L__BB34_3:
	sub.f64 	%fd14, %fd18, %fd2;
	cvta.to.global.u64 	%rd8, %rd2;
	shl.b64 	%rd9, %rd1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	add.f64 	%fd15, %fd14, %fd1;
	add.f64 	%fd16, %fd14, 0d0000000000000000;
	st.global.v2.f64 	[%rd10], {%fd16, %fd15};
	ret;

}
	// .globl	prefix_sum_inc_small_u32
.visible .entry prefix_sum_inc_small_u32(
	.param .u64 prefix_sum_inc_small_u32_param_0,
	.param .u64 prefix_sum_inc_small_u32_param_1,
	.param .u32 prefix_sum_inc_small_u32_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<40>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [prefix_sum_inc_small_u32_param_0];
	ld.param.u64 	%rd2, [prefix_sum_inc_small_u32_param_1];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.u32 	{%r15, %r16, %r17, %r18}, [%rd6];
	mov.u32 	%r19, 0;
	add.s32 	%r3, %r16, %r15;
	add.s32 	%r4, %r17, %r3;
	add.s32 	%r5, %r18, %r4;
	shl.b32 	%r23, %r1, 2;
	mov.u32 	%r24, shared;
	add.s32 	%r6, %r24, %r23;
	st.shared.u32 	[%r6], %r19;
	mov.u32 	%r7, %ntid.x;
	setp.lt.u32 	%p1, %r7, 2;
	mov.u32 	%r39, %r5;
	@%p1 bra 	$L__BB35_3;

	shl.b32 	%r26, %r7, 2;
	add.s32 	%r8, %r6, %r26;
	add.s32 	%r9, %r1, %r7;
	mov.u32 	%r37, 1;
	mov.u32 	%r39, %r5;

$L__BB35_2:
	st.shared.u32 	[%r8], %r39;
	bar.sync 	0;
	sub.s32 	%r27, %r9, %r37;
	shl.b32 	%r28, %r27, 2;
	add.s32 	%r30, %r24, %r28;
	ld.shared.u32 	%r31, [%r30];
	ld.shared.u32 	%r32, [%r8];
	add.s32 	%r39, %r31, %r32;
	bar.sync 	0;
	shl.b32 	%r37, %r37, 1;
	setp.lt.u32 	%p2, %r37, %r7;
	@%p2 bra 	$L__BB35_2;

$L__BB35_3:
	sub.s32 	%r33, %r39, %r5;
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	add.s32 	%r34, %r33, %r4;
	add.s32 	%r35, %r33, %r3;
	add.s32 	%r36, %r33, %r15;
	st.global.v4.u32 	[%rd9], {%r36, %r35, %r34, %r39};
	ret;

}
	// .globl	prefix_sum_inc_small_u64
.visible .entry prefix_sum_inc_small_u64(
	.param .u64 prefix_sum_inc_small_u64_param_0,
	.param .u64 prefix_sum_inc_small_u64_param_1,
	.param .u32 prefix_sum_inc_small_u64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<17>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd8, [prefix_sum_inc_small_u64_param_0];
	ld.param.u64 	%rd7, [prefix_sum_inc_small_u64_param_1];
	cvta.to.global.u64 	%rd9, %rd8;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd10, %r1, 16;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.v2.u64 	{%rd12, %rd13}, [%rd11];
	mov.u64 	%rd14, 0;
	add.s64 	%rd3, %rd13, %rd12;
	shl.b32 	%r8, %r1, 3;
	mov.u32 	%r9, shared;
	add.s32 	%r2, %r9, %r8;
	st.shared.u64 	[%r2], %rd14;
	mov.u32 	%r3, %ntid.x;
	setp.lt.u32 	%p1, %r3, 2;
	mov.u64 	%rd24, %rd3;
	@%p1 bra 	$L__BB36_3;

	shl.b32 	%r11, %r3, 3;
	add.s32 	%r4, %r2, %r11;
	add.s32 	%r5, %r1, %r3;
	mov.u32 	%r16, 1;
	mov.u64 	%rd24, %rd3;

$L__BB36_2:
	st.shared.u64 	[%r4], %rd24;
	bar.sync 	0;
	sub.s32 	%r12, %r5, %r16;
	shl.b32 	%r13, %r12, 3;
	add.s32 	%r15, %r9, %r13;
	ld.shared.u64 	%rd16, [%r15];
	ld.shared.u64 	%rd17, [%r4];
	add.s64 	%rd24, %rd16, %rd17;
	bar.sync 	0;
	shl.b32 	%r16, %r16, 1;
	setp.lt.u32 	%p2, %r16, %r3;
	@%p2 bra 	$L__BB36_2;

$L__BB36_3:
	sub.s64 	%rd18, %rd24, %rd3;
	cvta.to.global.u64 	%rd19, %rd7;
	shl.b64 	%rd20, %rd1, 4;
	add.s64 	%rd21, %rd19, %rd20;
	add.s64 	%rd22, %rd18, %rd12;
	st.global.v2.u64 	[%rd21], {%rd22, %rd24};
	ret;

}
	// .globl	prefix_sum_inc_small_f32
.visible .entry prefix_sum_inc_small_f32(
	.param .u64 prefix_sum_inc_small_f32_param_0,
	.param .u64 prefix_sum_inc_small_f32_param_1,
	.param .u32 prefix_sum_inc_small_f32_param_2
)
{
	.reg .pred 	%p<7>;
	.reg .f32 	%f<29>;
	.reg .b32 	%r<22>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd3, [prefix_sum_inc_small_f32_param_0];
	ld.param.u64 	%rd2, [prefix_sum_inc_small_f32_param_1];
	ld.param.u32 	%r8, [prefix_sum_inc_small_f32_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v4.f32 	{%f8, %f9, %f10, %f11}, [%rd6];
	shl.b32 	%r9, %r1, 2;
	setp.lt.u32 	%p1, %r9, %r8;
	add.f32 	%f16, %f8, 0f00000000;
	mov.u32 	%r10, 0;
	selp.f32 	%f1, %f16, 0f00000000, %p1;
	or.b32  	%r11, %r9, 1;
	setp.lt.u32 	%p2, %r11, %r8;
	selp.f32 	%f17, %f9, 0f00000000, %p2;
	add.f32 	%f2, %f1, %f17;
	or.b32  	%r12, %r9, 2;
	setp.lt.u32 	%p3, %r12, %r8;
	selp.f32 	%f18, %f10, 0f00000000, %p3;
	add.f32 	%f3, %f2, %f18;
	or.b32  	%r13, %r9, 3;
	setp.lt.u32 	%p4, %r13, %r8;
	selp.f32 	%f19, %f11, 0f00000000, %p4;
	add.f32 	%f4, %f3, %f19;
	mov.u32 	%r14, shared;
	add.s32 	%r2, %r14, %r9;
	st.shared.u32 	[%r2], %r10;
	mov.u32 	%r3, %ntid.x;
	setp.lt.u32 	%p5, %r3, 2;
	mov.f32 	%f28, %f4;
	@%p5 bra 	$L__BB37_3;

	shl.b32 	%r16, %r3, 2;
	add.s32 	%r4, %r2, %r16;
	add.s32 	%r5, %r1, %r3;
	mov.u32 	%r21, 1;
	mov.f32 	%f28, %f4;

$L__BB37_2:
	st.shared.f32 	[%r4], %f28;
	bar.sync 	0;
	sub.s32 	%r17, %r5, %r21;
	shl.b32 	%r18, %r17, 2;
	add.s32 	%r20, %r14, %r18;
	ld.shared.f32 	%f20, [%r20];
	ld.shared.f32 	%f21, [%r4];
	add.f32 	%f28, %f21, %f20;
	bar.sync 	0;
	shl.b32 	%r21, %r21, 1;
	setp.lt.u32 	%p6, %r21, %r3;
	@%p6 bra 	$L__BB37_2;

$L__BB37_3:
	sub.f32 	%f22, %f28, %f4;
	cvta.to.global.u64 	%rd7, %rd2;
	shl.b64 	%rd8, %rd1, 4;
	add.s64 	%rd9, %rd7, %rd8;
	add.f32 	%f23, %f22, %f4;
	add.f32 	%f24, %f22, %f3;
	add.f32 	%f25, %f22, %f2;
	add.f32 	%f26, %f22, %f1;
	st.global.v4.f32 	[%rd9], {%f26, %f25, %f24, %f23};
	ret;

}
	// .globl	prefix_sum_inc_small_f64
.visible .entry prefix_sum_inc_small_f64(
	.param .u64 prefix_sum_inc_small_f64_param_0,
	.param .u64 prefix_sum_inc_small_f64_param_1,
	.param .u32 prefix_sum_inc_small_f64_param_2
)
{
	.reg .pred 	%p<5>;
	.reg .b32 	%r<20>;
	.reg .f64 	%fd<19>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd3, [prefix_sum_inc_small_f64_param_0];
	ld.param.u64 	%rd2, [prefix_sum_inc_small_f64_param_1];
	ld.param.u32 	%r8, [prefix_sum_inc_small_f64_param_2];
	cvta.to.global.u64 	%rd4, %rd3;
	mov.u32 	%r1, %tid.x;
	cvt.u64.u32 	%rd1, %r1;
	mul.wide.u32 	%rd5, %r1, 16;
	add.s64 	%rd6, %rd4, %rd5;
	ld.global.v2.f64 	{%fd6, %fd7}, [%rd6];
	mov.u64 	%rd7, 0;
	shl.b32 	%r9, %r1, 1;
	setp.lt.u32 	%p1, %r9, %r8;
	add.f64 	%fd10, %fd6, 0d0000000000000000;
	selp.f64 	%fd1, %fd10, 0d0000000000000000, %p1;
	or.b32  	%r10, %r9, 1;
	setp.lt.u32 	%p2, %r10, %r8;
	selp.f64 	%fd11, %fd7, 0d0000000000000000, %p2;
	add.f64 	%fd2, %fd1, %fd11;
	shl.b32 	%r11, %r1, 3;
	mov.u32 	%r12, shared_d;
	add.s32 	%r2, %r12, %r11;
	st.shared.u64 	[%r2], %rd7;
	mov.u32 	%r3, %ntid.x;
	setp.lt.u32 	%p3, %r3, 2;
	mov.f64 	%fd18, %fd2;
	@%p3 bra 	$L__BB38_3;

	shl.b32 	%r14, %r3, 3;
	add.s32 	%r4, %r2, %r14;
	add.s32 	%r5, %r1, %r3;
	mov.u32 	%r19, 1;
	mov.f64 	%fd18, %fd2;

$L__BB38_2:
	st.shared.f64 	[%r4], %fd18;
	bar.sync 	0;
	sub.s32 	%r15, %r5, %r19;
	shl.b32 	%r16, %r15, 3;
	add.s32 	%r18, %r12, %r16;
	ld.shared.f64 	%fd12, [%r18];
	ld.shared.f64 	%fd13, [%r4];
	add.f64 	%fd18, %fd13, %fd12;
	bar.sync 	0;
	shl.b32 	%r19, %r19, 1;
	setp.lt.u32 	%p4, %r19, %r3;
	@%p4 bra 	$L__BB38_2;

$L__BB38_3:
	sub.f64 	%fd14, %fd18, %fd2;
	cvta.to.global.u64 	%rd8, %rd2;
	shl.b64 	%rd9, %rd1, 4;
	add.s64 	%rd10, %rd8, %rd9;
	add.f64 	%fd15, %fd14, %fd2;
	add.f64 	%fd16, %fd14, %fd1;
	st.global.v2.f64 	[%rd10], {%fd16, %fd15};
	ret;

}
	// .globl	prefix_sum_exc_large_u32
.visible .entry prefix_sum_exc_large_u32(
	.param .u64 prefix_sum_exc_large_u32_param_0,
	.param .u64 prefix_sum_exc_large_u32_param_1,
	.param .u32 prefix_sum_exc_large_u32_param_2,
	.param .u64 prefix_sum_exc_large_u32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<200>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd5, [prefix_sum_exc_large_u32_param_0];
	ld.param.u64 	%rd4, [prefix_sum_exc_large_u32_param_1];
	ld.param.u64 	%rd6, [prefix_sum_exc_large_u32_param_3];
	cvta.to.global.u64 	%rd7, %rd5;
	mov.u32 	%r31, %ctaid.x;
	shl.b32 	%r32, %r31, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r33, %r32, %r1;
	cvt.u64.u32 	%rd1, %r33;
	mul.wide.u32 	%rd8, %r33, 16;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v4.u32 	{%r34, %r35, %r36, %r37}, [%rd9];
	ld.global.v4.u32 	{%r42, %r43, %r44, %r45}, [%rd9+2048];
	ld.global.v4.u32 	{%r50, %r51, %r52, %r53}, [%rd9+4096];
	ld.global.v4.u32 	{%r58, %r59, %r60, %r61}, [%rd9+6144];
	shl.b32 	%r66, %r1, 4;
	mov.u32 	%r67, shared;
	add.s32 	%r2, %r67, %r66;
	mov.u32 	%r199, 0;
	st.shared.v4.u32 	[%r2], {%r34, %r35, %r36, %r37};
	st.shared.v4.u32 	[%r2+2048], {%r42, %r43, %r44, %r45};
	st.shared.v4.u32 	[%r2+4096], {%r50, %r51, %r52, %r53};
	st.shared.v4.u32 	[%r2+6144], {%r58, %r59, %r60, %r61};
	bar.sync 	0;
	shl.b32 	%r69, %r1, 2;
	shl.b32 	%r70, %r1, 6;
	add.s32 	%r3, %r67, %r70;
	ld.shared.v4.u32 	{%r71, %r72, %r73, %r74}, [%r3];
	ld.shared.v4.u32 	{%r78, %r79, %r80, %r81}, [%r3+16];
	ld.shared.v4.u32 	{%r86, %r87, %r88, %r89}, [%r3+32];
	ld.shared.v4.u32 	{%r94, %r95, %r96, %r97}, [%r3+48];
	add.s32 	%r5, %r72, %r71;
	add.s32 	%r6, %r73, %r5;
	add.s32 	%r7, %r74, %r6;
	add.s32 	%r8, %r78, %r7;
	add.s32 	%r9, %r79, %r8;
	add.s32 	%r10, %r80, %r9;
	add.s32 	%r11, %r81, %r10;
	add.s32 	%r12, %r86, %r11;
	add.s32 	%r13, %r87, %r12;
	add.s32 	%r14, %r88, %r13;
	add.s32 	%r15, %r89, %r14;
	add.s32 	%r16, %r94, %r15;
	add.s32 	%r17, %r95, %r16;
	add.s32 	%r18, %r96, %r17;
	add.s32 	%r19, %r97, %r18;
	bar.sync 	0;
	add.s32 	%r102, %r67, %r69;
	st.shared.u32 	[%r102], %r199;
	st.shared.u32 	[%r102+512], %r19;
	bar.sync 	0;
	ld.shared.u32 	%r103, [%r102+508];
	ld.shared.u32 	%r104, [%r102+512];
	add.s32 	%r105, %r103, %r104;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r105;
	bar.sync 	0;
	ld.shared.u32 	%r106, [%r102+504];
	ld.shared.u32 	%r107, [%r102+512];
	add.s32 	%r108, %r106, %r107;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r108;
	bar.sync 	0;
	ld.shared.u32 	%r109, [%r102+496];
	ld.shared.u32 	%r110, [%r102+512];
	add.s32 	%r111, %r109, %r110;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r111;
	bar.sync 	0;
	ld.shared.u32 	%r112, [%r102+480];
	ld.shared.u32 	%r113, [%r102+512];
	add.s32 	%r114, %r112, %r113;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r114;
	bar.sync 	0;
	ld.shared.u32 	%r115, [%r102+448];
	ld.shared.u32 	%r116, [%r102+512];
	add.s32 	%r117, %r115, %r116;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r117;
	bar.sync 	0;
	ld.shared.u32 	%r118, [%r102+384];
	ld.shared.u32 	%r119, [%r102+512];
	add.s32 	%r120, %r118, %r119;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r120;
	bar.sync 	0;
	ld.shared.u32 	%r121, [%r102+256];
	ld.shared.u32 	%r122, [%r102+512];
	add.s32 	%r20, %r121, %r122;
	bar.sync 	0;
	mul.wide.u32 	%rd10, %r31, 8;
	add.s64 	%rd2, %rd6, %rd10;
	setp.ne.s32 	%p1, %r1, 127;
	@%p1 bra 	$L__BB39_2;

	cvt.u64.u32 	%rd13, %r20;
	shl.b64 	%rd14, %rd13, 32;
	or.b64  	%rd12, %rd14, 1;
	// begin inline asm
	st.cg.u64 [%rd2], %rd12;
	// end inline asm

$L__BB39_2:
	and.b32  	%r21, %r1, 31;
	or.b32  	%r198, %r21, -32;
	cvta.to.global.u64 	%rd3, %rd4;
	bra.uni 	$L__BB39_3;

$L__BB39_8:
	add.s32 	%r199, %r199, %r26;
	add.s32 	%r198, %r198, -32;

$L__BB39_3:
	mul.wide.s32 	%rd17, %r198, 8;
	add.s64 	%rd16, %rd2, %rd17;
	// begin inline asm
	ld.cg.u64 %rd15, [%rd16];
	// end inline asm
	cvt.u32.u64 	%r25, %rd15;
	shr.u64 	%rd18, %rd15, 32;
	cvt.u32.u64 	%r26, %rd18;
	setp.eq.s32 	%p2, %r25, 0;
	mov.u32 	%r124, -1;
	vote.sync.any.pred 	%p3, %p2, %r124;
	@%p3 bra 	$L__BB39_3;

	setp.eq.s32 	%p4, %r25, 2;
	vote.sync.ballot.b32 	%r27, %p4, %r124;
	setp.eq.s32 	%p6, %r27, 0;
	@%p6 bra 	$L__BB39_8;

	clz.b32 	%r127, %r27;
	mov.u32 	%r128, 31;
	sub.s32 	%r129, %r128, %r127;
	setp.lt.u32 	%p8, %r21, %r129;
	selp.b32 	%r130, 0, %r26, %p8;
	mov.u32 	%r131, 0;
	add.s32 	%r132, %r130, %r199;
	mov.u32 	%r133, 2;
	mov.u32 	%r134, 16;
	shfl.sync.down.b32 	%r136|%p9, %r132, %r134, %r128, %r124;
	add.s32 	%r137, %r136, %r132;
	mov.u32 	%r138, 8;
	shfl.sync.down.b32 	%r139|%p10, %r137, %r138, %r128, %r124;
	add.s32 	%r140, %r139, %r137;
	mov.u32 	%r141, 4;
	shfl.sync.down.b32 	%r142|%p11, %r140, %r141, %r128, %r124;
	add.s32 	%r143, %r142, %r140;
	shfl.sync.down.b32 	%r144|%p12, %r143, %r133, %r128, %r124;
	add.s32 	%r145, %r144, %r143;
	mov.u32 	%r146, 1;
	shfl.sync.down.b32 	%r147|%p13, %r145, %r146, %r128, %r124;
	add.s32 	%r148, %r147, %r145;
	shfl.sync.idx.b32 	%r149|%p14, %r148, %r131, %r128, %r124;
	add.s32 	%r28, %r149, %r20;
	@%p1 bra 	$L__BB39_7;

	cvt.u64.u32 	%rd21, %r28;
	shl.b64 	%rd22, %rd21, 32;
	or.b64  	%rd20, %rd22, 2;
	// begin inline asm
	st.cg.u64 [%rd2], %rd20;
	// end inline asm

$L__BB39_7:
	sub.s32 	%r150, %r28, %r19;
	add.s32 	%r151, %r150, %r6;
	add.s32 	%r152, %r150, %r5;
	add.s32 	%r153, %r150, %r71;
	st.shared.v4.u32 	[%r3], {%r150, %r153, %r152, %r151};
	add.s32 	%r154, %r150, %r10;
	add.s32 	%r155, %r150, %r9;
	add.s32 	%r156, %r150, %r8;
	add.s32 	%r157, %r150, %r7;
	st.shared.v4.u32 	[%r3+16], {%r157, %r156, %r155, %r154};
	add.s32 	%r158, %r150, %r14;
	add.s32 	%r159, %r150, %r13;
	add.s32 	%r160, %r150, %r12;
	add.s32 	%r161, %r150, %r11;
	st.shared.v4.u32 	[%r3+32], {%r161, %r160, %r159, %r158};
	add.s32 	%r162, %r150, %r18;
	add.s32 	%r163, %r150, %r17;
	add.s32 	%r164, %r150, %r16;
	add.s32 	%r165, %r150, %r15;
	st.shared.v4.u32 	[%r3+48], {%r165, %r164, %r163, %r162};
	bar.sync 	0;
	shl.b64 	%rd23, %rd1, 4;
	add.s64 	%rd24, %rd3, %rd23;
	ld.shared.v4.u32 	{%r166, %r167, %r168, %r169}, [%r2];
	st.global.v4.u32 	[%rd24], {%r166, %r167, %r168, %r169};
	ld.shared.v4.u32 	{%r174, %r175, %r176, %r177}, [%r2+2048];
	st.global.v4.u32 	[%rd24+2048], {%r174, %r175, %r176, %r177};
	ld.shared.v4.u32 	{%r182, %r183, %r184, %r185}, [%r2+4096];
	st.global.v4.u32 	[%rd24+4096], {%r182, %r183, %r184, %r185};
	ld.shared.v4.u32 	{%r190, %r191, %r192, %r193}, [%r2+6144];
	st.global.v4.u32 	[%rd24+6144], {%r190, %r191, %r192, %r193};
	ret;

}
	// .globl	prefix_sum_exc_large_u64
.visible .entry prefix_sum_exc_large_u64(
	.param .u64 prefix_sum_exc_large_u64_param_0,
	.param .u64 prefix_sum_exc_large_u64_param_1,
	.param .u32 prefix_sum_exc_large_u64_param_2,
	.param .u64 prefix_sum_exc_large_u64_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<101>;
	.reg .b64 	%rd<116>;


	ld.param.u64 	%rd18, [prefix_sum_exc_large_u64_param_0];
	ld.param.u64 	%rd19, [prefix_sum_exc_large_u64_param_3];
	cvta.to.global.u64 	%rd20, %rd18;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r12, %r11, %r1;
	mul.wide.u32 	%rd21, %r12, 16;
	add.s64 	%rd22, %rd20, %rd21;
	ld.global.v2.u64 	{%rd23, %rd24}, [%rd22];
	mov.u64 	%rd115, 0;
	ld.global.v2.u64 	{%rd28, %rd29}, [%rd22+2048];
	ld.global.v2.u64 	{%rd32, %rd33}, [%rd22+4096];
	ld.global.v2.u64 	{%rd36, %rd37}, [%rd22+6144];
	shl.b32 	%r13, %r1, 4;
	mov.u32 	%r14, shared;
	add.s32 	%r2, %r14, %r13;
	st.shared.v2.u64 	[%r2], {%rd23, %rd24};
	st.shared.v2.u64 	[%r2+2048], {%rd28, %rd29};
	st.shared.v2.u64 	[%r2+4096], {%rd32, %rd33};
	st.shared.v2.u64 	[%r2+6144], {%rd36, %rd37};
	bar.sync 	0;
	shl.b32 	%r15, %r1, 6;
	add.s32 	%r3, %r14, %r15;
	ld.shared.v2.u64 	{%rd40, %rd41}, [%r3];
	ld.shared.v2.u64 	{%rd43, %rd44}, [%r3+16];
	ld.shared.v2.u64 	{%rd47, %rd48}, [%r3+32];
	ld.shared.v2.u64 	{%rd51, %rd52}, [%r3+48];
	add.s64 	%rd3, %rd41, %rd40;
	add.s64 	%rd4, %rd43, %rd3;
	add.s64 	%rd5, %rd44, %rd4;
	add.s64 	%rd6, %rd47, %rd5;
	add.s64 	%rd7, %rd48, %rd6;
	add.s64 	%rd8, %rd51, %rd7;
	add.s64 	%rd9, %rd52, %rd8;
	bar.sync 	0;
	shl.b32 	%r16, %r1, 3;
	add.s32 	%r17, %r14, %r16;
	st.shared.u64 	[%r17], %rd115;
	st.shared.u64 	[%r17+1024], %rd9;
	bar.sync 	0;
	ld.shared.u64 	%rd55, [%r17+1016];
	ld.shared.u64 	%rd56, [%r17+1024];
	add.s64 	%rd57, %rd55, %rd56;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd57;
	bar.sync 	0;
	ld.shared.u64 	%rd58, [%r17+1008];
	ld.shared.u64 	%rd59, [%r17+1024];
	add.s64 	%rd60, %rd58, %rd59;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd60;
	bar.sync 	0;
	ld.shared.u64 	%rd61, [%r17+992];
	ld.shared.u64 	%rd62, [%r17+1024];
	add.s64 	%rd63, %rd61, %rd62;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd63;
	bar.sync 	0;
	ld.shared.u64 	%rd64, [%r17+960];
	ld.shared.u64 	%rd65, [%r17+1024];
	add.s64 	%rd66, %rd64, %rd65;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd66;
	bar.sync 	0;
	ld.shared.u64 	%rd67, [%r17+896];
	ld.shared.u64 	%rd68, [%r17+1024];
	add.s64 	%rd69, %rd67, %rd68;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd69;
	bar.sync 	0;
	ld.shared.u64 	%rd70, [%r17+768];
	ld.shared.u64 	%rd71, [%r17+1024];
	add.s64 	%rd72, %rd70, %rd71;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd72;
	bar.sync 	0;
	ld.shared.u64 	%rd73, [%r17+512];
	ld.shared.u64 	%rd74, [%r17+1024];
	add.s64 	%rd10, %rd73, %rd74;
	bar.sync 	0;
	mul.wide.u32 	%rd75, %r10, 8;
	add.s64 	%rd11, %rd19, %rd75;
	setp.ne.s32 	%p1, %r1, 127;
	@%p1 bra 	$L__BB40_2;

	shl.b64 	%rd78, %rd10, 2;
	or.b64  	%rd77, %rd78, 1;
	// begin inline asm
	st.cg.u64 [%rd11], %rd77;
	// end inline asm

$L__BB40_2:
	and.b32  	%r4, %r1, 31;
	or.b32  	%r100, %r4, -32;
	mov.u32 	%r19, -1;
	bra.uni 	$L__BB40_3;

$L__BB40_8:
	add.s64 	%rd115, %rd14, %rd115;
	add.s32 	%r100, %r100, -32;

$L__BB40_3:
	mul.wide.s32 	%rd82, %r100, 8;
	add.s64 	%rd81, %rd11, %rd82;
	// begin inline asm
	ld.cg.u64 %rd80, [%rd81];
	// end inline asm
	cvt.u32.u64 	%r18, %rd80;
	and.b32  	%r7, %r18, 3;
	shr.u64 	%rd14, %rd80, 2;
	setp.eq.s32 	%p2, %r7, 0;
	vote.sync.any.pred 	%p3, %p2, %r19;
	@%p3 bra 	$L__BB40_3;

	setp.eq.s32 	%p4, %r7, 2;
	vote.sync.ballot.b32 	%r8, %p4, %r19;
	setp.eq.s32 	%p6, %r8, 0;
	@%p6 bra 	$L__BB40_8;

	clz.b32 	%r46, %r8;
	mov.u32 	%r47, 31;
	sub.s32 	%r48, %r47, %r46;
	setp.lt.u32 	%p8, %r4, %r48;
	selp.b64 	%rd95, 0, %rd14, %p8;
	add.s64 	%rd83, %rd95, %rd115;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd83;
	// end inline asm
	mov.u32 	%r49, 2;
	mov.u32 	%r50, 16;
	shfl.sync.down.b32 	%r25|%p9, %r23, %r50, %r47, %r19;
	shfl.sync.down.b32 	%r24|%p10, %r22, %r50, %r47, %r19;
	// begin inline asm
	mov.b64 %rd84, {%r24,%r25};
	// end inline asm
	add.s64 	%rd85, %rd84, %rd83;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd85;
	// end inline asm
	mov.u32 	%r52, 8;
	shfl.sync.down.b32 	%r29|%p11, %r27, %r52, %r47, %r19;
	shfl.sync.down.b32 	%r28|%p12, %r26, %r52, %r47, %r19;
	// begin inline asm
	mov.b64 %rd86, {%r28,%r29};
	// end inline asm
	add.s64 	%rd87, %rd86, %rd85;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd87;
	// end inline asm
	mov.u32 	%r53, 4;
	shfl.sync.down.b32 	%r33|%p13, %r31, %r53, %r47, %r19;
	shfl.sync.down.b32 	%r32|%p14, %r30, %r53, %r47, %r19;
	// begin inline asm
	mov.b64 %rd88, {%r32,%r33};
	// end inline asm
	add.s64 	%rd89, %rd88, %rd87;
	// begin inline asm
	mov.b64 {%r34,%r35}, %rd89;
	// end inline asm
	shfl.sync.down.b32 	%r37|%p15, %r35, %r49, %r47, %r19;
	shfl.sync.down.b32 	%r36|%p16, %r34, %r49, %r47, %r19;
	// begin inline asm
	mov.b64 %rd90, {%r36,%r37};
	// end inline asm
	add.s64 	%rd91, %rd90, %rd89;
	// begin inline asm
	mov.b64 {%r38,%r39}, %rd91;
	// end inline asm
	mov.u32 	%r54, 1;
	shfl.sync.down.b32 	%r41|%p17, %r39, %r54, %r47, %r19;
	shfl.sync.down.b32 	%r40|%p18, %r38, %r54, %r47, %r19;
	// begin inline asm
	mov.b64 %rd92, {%r40,%r41};
	// end inline asm
	add.s64 	%rd93, %rd92, %rd91;
	// begin inline asm
	mov.b64 {%r42,%r43}, %rd93;
	// end inline asm
	mov.u32 	%r55, 0;
	shfl.sync.idx.b32 	%r45|%p19, %r43, %r55, %r47, %r19;
	shfl.sync.idx.b32 	%r44|%p20, %r42, %r55, %r47, %r19;
	// begin inline asm
	mov.b64 %rd94, {%r44,%r45};
	// end inline asm
	add.s64 	%rd15, %rd94, %rd10;
	@%p1 bra 	$L__BB40_7;

	mov.u32 	%r95, %ctaid.x;
	mul.wide.u32 	%rd112, %r95, 8;
	ld.param.u64 	%rd111, [prefix_sum_exc_large_u64_param_3];
	add.s64 	%rd110, %rd111, %rd112;
	shl.b64 	%rd98, %rd15, 2;
	or.b64  	%rd97, %rd98, 2;
	// begin inline asm
	st.cg.u64 [%rd110], %rd97;
	// end inline asm

$L__BB40_7:
	mov.u32 	%r99, %tid.x;
	shl.b32 	%r98, %r99, 4;
	mov.u32 	%r97, shared;
	add.s32 	%r96, %r97, %r98;
	ld.param.u64 	%rd114, [prefix_sum_exc_large_u64_param_1];
	cvta.to.global.u64 	%rd113, %rd114;
	mov.u32 	%r94, %tid.x;
	mov.u32 	%r93, %ctaid.x;
	shl.b32 	%r92, %r93, 9;
	add.s32 	%r91, %r92, %r94;
	cvt.u64.u32 	%rd109, %r91;
	shl.b32 	%r90, %r94, 6;
	mov.u32 	%r89, shared;
	add.s32 	%r88, %r89, %r90;
	sub.s64 	%rd99, %rd15, %rd9;
	add.s64 	%rd100, %rd99, %rd40;
	st.shared.v2.u64 	[%r88], {%rd99, %rd100};
	add.s64 	%rd101, %rd99, %rd4;
	add.s64 	%rd102, %rd99, %rd3;
	st.shared.v2.u64 	[%r88+16], {%rd102, %rd101};
	add.s64 	%rd103, %rd99, %rd6;
	add.s64 	%rd104, %rd99, %rd5;
	st.shared.v2.u64 	[%r88+32], {%rd104, %rd103};
	add.s64 	%rd105, %rd99, %rd8;
	add.s64 	%rd106, %rd99, %rd7;
	st.shared.v2.u64 	[%r88+48], {%rd106, %rd105};
	bar.sync 	0;
	shl.b64 	%rd107, %rd109, 4;
	add.s64 	%rd108, %rd113, %rd107;
	ld.shared.v4.u32 	{%r56, %r57, %r58, %r59}, [%r96];
	st.global.v4.u32 	[%rd108], {%r56, %r57, %r58, %r59};
	ld.shared.v4.u32 	{%r64, %r65, %r66, %r67}, [%r96+2048];
	st.global.v4.u32 	[%rd108+2048], {%r64, %r65, %r66, %r67};
	ld.shared.v4.u32 	{%r72, %r73, %r74, %r75}, [%r96+4096];
	st.global.v4.u32 	[%rd108+4096], {%r72, %r73, %r74, %r75};
	ld.shared.v4.u32 	{%r80, %r81, %r82, %r83}, [%r96+6144];
	st.global.v4.u32 	[%rd108+6144], {%r80, %r81, %r82, %r83};
	ret;

}
	// .globl	prefix_sum_exc_large_f32
.visible .entry prefix_sum_exc_large_f32(
	.param .u64 prefix_sum_exc_large_f32_param_0,
	.param .u64 prefix_sum_exc_large_f32_param_1,
	.param .u32 prefix_sum_exc_large_f32_param_2,
	.param .u64 prefix_sum_exc_large_f32_param_3
)
{
	.reg .pred 	%p<31>;
	.reg .f32 	%f<153>;
	.reg .b32 	%r<97>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd6, [prefix_sum_exc_large_f32_param_0];
	ld.param.u64 	%rd5, [prefix_sum_exc_large_f32_param_1];
	ld.param.u32 	%r10, [prefix_sum_exc_large_f32_param_2];
	ld.param.u64 	%rd7, [prefix_sum_exc_large_f32_param_3];
	cvta.to.global.u64 	%rd8, %rd6;
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r12, %r11, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r13, %r12, %r1;
	cvt.u64.u32 	%rd1, %r13;
	mul.wide.u32 	%rd9, %r13, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd10];
	shl.b32 	%r14, %r13, 2;
	setp.lt.u32 	%p1, %r14, %r10;
	mov.u32 	%r15, 0;
	or.b32  	%r16, %r14, 1;
	setp.lt.u32 	%p2, %r16, %r10;
	or.b32  	%r17, %r14, 2;
	setp.lt.u32 	%p3, %r17, %r10;
	or.b32  	%r18, %r14, 3;
	setp.lt.u32 	%p4, %r18, %r10;
	ld.global.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd10+2048];
	add.s32 	%r19, %r14, 512;
	setp.lt.u32 	%p5, %r19, %r10;
	or.b32  	%r20, %r19, 1;
	setp.lt.u32 	%p6, %r20, %r10;
	or.b32  	%r21, %r19, 2;
	setp.lt.u32 	%p7, %r21, %r10;
	or.b32  	%r22, %r19, 3;
	setp.lt.u32 	%p8, %r22, %r10;
	ld.global.v4.f32 	{%f37, %f38, %f39, %f40}, [%rd10+4096];
	add.s32 	%r23, %r14, 1024;
	setp.lt.u32 	%p9, %r23, %r10;
	or.b32  	%r24, %r23, 1;
	setp.lt.u32 	%p10, %r24, %r10;
	or.b32  	%r25, %r23, 2;
	setp.lt.u32 	%p11, %r25, %r10;
	or.b32  	%r26, %r23, 3;
	setp.lt.u32 	%p12, %r26, %r10;
	ld.global.v4.f32 	{%f45, %f46, %f47, %f48}, [%rd10+6144];
	add.s32 	%r27, %r14, 1536;
	setp.lt.u32 	%p13, %r27, %r10;
	or.b32  	%r28, %r27, 1;
	setp.lt.u32 	%p14, %r28, %r10;
	or.b32  	%r29, %r27, 2;
	setp.lt.u32 	%p15, %r29, %r10;
	or.b32  	%r30, %r27, 3;
	setp.lt.u32 	%p16, %r30, %r10;
	shl.b32 	%r31, %r1, 4;
	mov.u32 	%r32, shared;
	add.s32 	%r2, %r32, %r31;
	selp.f32 	%f53, %f24, 0f00000000, %p4;
	selp.f32 	%f54, %f23, 0f00000000, %p3;
	selp.f32 	%f55, %f22, 0f00000000, %p2;
	selp.f32 	%f56, %f21, 0f00000000, %p1;
	st.shared.v4.f32 	[%r2], {%f56, %f55, %f54, %f53};
	selp.f32 	%f57, %f32, 0f00000000, %p8;
	selp.f32 	%f58, %f31, 0f00000000, %p7;
	selp.f32 	%f59, %f30, 0f00000000, %p6;
	selp.f32 	%f60, %f29, 0f00000000, %p5;
	st.shared.v4.f32 	[%r2+2048], {%f60, %f59, %f58, %f57};
	selp.f32 	%f61, %f40, 0f00000000, %p12;
	selp.f32 	%f62, %f39, 0f00000000, %p11;
	selp.f32 	%f63, %f38, 0f00000000, %p10;
	selp.f32 	%f64, %f37, 0f00000000, %p9;
	st.shared.v4.f32 	[%r2+4096], {%f64, %f63, %f62, %f61};
	selp.f32 	%f65, %f48, 0f00000000, %p16;
	selp.f32 	%f66, %f47, 0f00000000, %p15;
	selp.f32 	%f67, %f46, 0f00000000, %p14;
	selp.f32 	%f68, %f45, 0f00000000, %p13;
	st.shared.v4.f32 	[%r2+6144], {%f68, %f67, %f66, %f65};
	bar.sync 	0;
	shl.b32 	%r33, %r1, 2;
	shl.b32 	%r34, %r1, 6;
	add.s32 	%r3, %r32, %r34;
	ld.shared.v4.f32 	{%f69, %f70, %f71, %f72}, [%r3];
	ld.shared.v4.f32 	{%f77, %f78, %f79, %f80}, [%r3+16];
	ld.shared.v4.f32 	{%f85, %f86, %f87, %f88}, [%r3+32];
	ld.shared.v4.f32 	{%f93, %f94, %f95, %f96}, [%r3+48];
	add.f32 	%f1, %f69, 0f00000000;
	add.f32 	%f2, %f1, %f70;
	add.f32 	%f3, %f2, %f71;
	add.f32 	%f4, %f3, %f72;
	add.f32 	%f5, %f4, %f77;
	add.f32 	%f6, %f5, %f78;
	add.f32 	%f7, %f6, %f79;
	add.f32 	%f8, %f7, %f80;
	add.f32 	%f9, %f8, %f85;
	add.f32 	%f10, %f9, %f86;
	add.f32 	%f11, %f10, %f87;
	add.f32 	%f12, %f11, %f88;
	add.f32 	%f13, %f12, %f93;
	add.f32 	%f14, %f13, %f94;
	add.f32 	%f15, %f14, %f95;
	add.f32 	%f16, %f15, %f96;
	bar.sync 	0;
	add.s32 	%r35, %r32, %r33;
	st.shared.u32 	[%r35], %r15;
	st.shared.f32 	[%r35+512], %f16;
	bar.sync 	0;
	ld.shared.f32 	%f101, [%r35+508];
	ld.shared.f32 	%f102, [%r35+512];
	add.f32 	%f103, %f102, %f101;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f103;
	bar.sync 	0;
	ld.shared.f32 	%f104, [%r35+504];
	ld.shared.f32 	%f105, [%r35+512];
	add.f32 	%f106, %f105, %f104;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f106;
	bar.sync 	0;
	ld.shared.f32 	%f107, [%r35+496];
	ld.shared.f32 	%f108, [%r35+512];
	add.f32 	%f109, %f108, %f107;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f109;
	bar.sync 	0;
	ld.shared.f32 	%f110, [%r35+480];
	ld.shared.f32 	%f111, [%r35+512];
	add.f32 	%f112, %f111, %f110;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f112;
	bar.sync 	0;
	ld.shared.f32 	%f113, [%r35+448];
	ld.shared.f32 	%f114, [%r35+512];
	add.f32 	%f115, %f114, %f113;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f115;
	bar.sync 	0;
	ld.shared.f32 	%f116, [%r35+384];
	ld.shared.f32 	%f117, [%r35+512];
	add.f32 	%f118, %f117, %f116;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f118;
	bar.sync 	0;
	ld.shared.f32 	%f119, [%r35+256];
	ld.shared.f32 	%f120, [%r35+512];
	add.f32 	%f17, %f120, %f119;
	bar.sync 	0;
	mul.wide.u32 	%rd11, %r11, 8;
	add.s64 	%rd2, %rd7, %rd11;
	setp.ne.s32 	%p17, %r1, 127;
	@%p17 bra 	$L__BB41_2;

	mov.b32 	%r36, %f17;
	cvt.u64.u32 	%rd14, %r36;
	shl.b64 	%rd15, %rd14, 32;
	or.b64  	%rd13, %rd15, 1;
	// begin inline asm
	st.cg.u64 [%rd2], %rd13;
	// end inline asm

$L__BB41_2:
	and.b32  	%r4, %r1, 31;
	or.b32  	%r96, %r4, -32;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.f32 	%f152, 0f00000000;
	mov.u32 	%r37, -1;
	bra.uni 	$L__BB41_3;

$L__BB41_8:
	add.s32 	%r96, %r96, -32;
	mov.f32 	%f152, %f19;

$L__BB41_3:
	mul.wide.s32 	%rd18, %r96, 8;
	add.s64 	%rd17, %rd2, %rd18;
	// begin inline asm
	ld.cg.u64 %rd16, [%rd17];
	// end inline asm
	cvt.u32.u64 	%r7, %rd16;
	setp.eq.s32 	%p18, %r7, 0;
	vote.sync.any.pred 	%p19, %p18, %r37;
	@%p19 bra 	$L__BB41_3;

	shr.u64 	%rd19, %rd16, 32;
	cvt.u32.u64 	%r39, %rd19;
	mov.b32 	%f122, %r39;
	setp.eq.s32 	%p20, %r7, 2;
	vote.sync.ballot.b32 	%r8, %p20, %r37;
	setp.eq.s32 	%p22, %r8, 0;
	add.f32 	%f19, %f152, %f122;
	@%p22 bra 	$L__BB41_8;

	clz.b32 	%r41, %r8;
	mov.u32 	%r42, 31;
	sub.s32 	%r43, %r42, %r41;
	setp.lt.u32 	%p24, %r4, %r43;
	selp.f32 	%f123, %f152, %f19, %p24;
	mov.b32 	%r44, %f123;
	mov.u32 	%r45, 2;
	mov.u32 	%r46, 16;
	shfl.sync.down.b32 	%r48|%p25, %r44, %r46, %r42, %r37;
	mov.b32 	%f124, %r48;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r49, %f125;
	mov.u32 	%r50, 8;
	shfl.sync.down.b32 	%r51|%p26, %r49, %r50, %r42, %r37;
	mov.b32 	%f126, %r51;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r52, %f127;
	mov.u32 	%r53, 4;
	shfl.sync.down.b32 	%r54|%p27, %r52, %r53, %r42, %r37;
	mov.b32 	%f128, %r54;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r55, %f129;
	shfl.sync.down.b32 	%r56|%p28, %r55, %r45, %r42, %r37;
	mov.b32 	%f130, %r56;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r57, %f131;
	mov.u32 	%r58, 1;
	shfl.sync.down.b32 	%r59|%p29, %r57, %r58, %r42, %r37;
	mov.b32 	%f132, %r59;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r60, %f133;
	shfl.sync.idx.b32 	%r62|%p30, %r60, %r15, %r42, %r37;
	mov.b32 	%f134, %r62;
	add.f32 	%f20, %f17, %f134;
	@%p17 bra 	$L__BB41_7;

	mov.b32 	%r63, %f20;
	cvt.u64.u32 	%rd22, %r63;
	shl.b64 	%rd23, %rd22, 32;
	or.b64  	%rd21, %rd23, 2;
	// begin inline asm
	st.cg.u64 [%rd2], %rd21;
	// end inline asm

$L__BB41_7:
	sub.f32 	%f135, %f20, %f16;
	add.f32 	%f136, %f135, %f3;
	add.f32 	%f137, %f135, %f2;
	add.f32 	%f138, %f135, %f1;
	add.f32 	%f139, %f135, 0f00000000;
	st.shared.v4.f32 	[%r3], {%f139, %f138, %f137, %f136};
	add.f32 	%f140, %f135, %f7;
	add.f32 	%f141, %f135, %f6;
	add.f32 	%f142, %f135, %f5;
	add.f32 	%f143, %f135, %f4;
	st.shared.v4.f32 	[%r3+16], {%f143, %f142, %f141, %f140};
	add.f32 	%f144, %f135, %f11;
	add.f32 	%f145, %f135, %f10;
	add.f32 	%f146, %f135, %f9;
	add.f32 	%f147, %f135, %f8;
	st.shared.v4.f32 	[%r3+32], {%f147, %f146, %f145, %f144};
	add.f32 	%f148, %f135, %f15;
	add.f32 	%f149, %f135, %f14;
	add.f32 	%f150, %f135, %f13;
	add.f32 	%f151, %f135, %f12;
	st.shared.v4.f32 	[%r3+48], {%f151, %f150, %f149, %f148};
	bar.sync 	0;
	shl.b64 	%rd24, %rd1, 4;
	add.s64 	%rd25, %rd3, %rd24;
	ld.shared.v4.u32 	{%r64, %r65, %r66, %r67}, [%r2];
	st.global.v4.u32 	[%rd25], {%r64, %r65, %r66, %r67};
	ld.shared.v4.u32 	{%r72, %r73, %r74, %r75}, [%r2+2048];
	st.global.v4.u32 	[%rd25+2048], {%r72, %r73, %r74, %r75};
	ld.shared.v4.u32 	{%r80, %r81, %r82, %r83}, [%r2+4096];
	st.global.v4.u32 	[%rd25+4096], {%r80, %r81, %r82, %r83};
	ld.shared.v4.u32 	{%r88, %r89, %r90, %r91}, [%r2+6144];
	st.global.v4.u32 	[%rd25+6144], {%r88, %r89, %r90, %r91};
	ret;

}
	// .globl	prefix_sum_exc_large_f64
.visible .entry prefix_sum_exc_large_f64(
	.param .u64 prefix_sum_exc_large_f64_param_0,
	.param .u64 prefix_sum_exc_large_f64_param_1,
	.param .u32 prefix_sum_exc_large_f64_param_2,
	.param .u64 prefix_sum_exc_large_f64_param_3
)
{
	.reg .pred 	%p<29>;
	.reg .b32 	%r<110>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<33>;


	ld.param.u64 	%rd6, [prefix_sum_exc_large_f64_param_0];
	ld.param.u32 	%r10, [prefix_sum_exc_large_f64_param_2];
	ld.param.u64 	%rd7, [prefix_sum_exc_large_f64_param_3];
	cvta.to.global.u64 	%rd8, %rd6;
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r12, %r11, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r13, %r12, %r1;
	mul.wide.u32 	%rd9, %r13, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v2.f64 	{%fd13, %fd14}, [%rd10];
	mov.u64 	%rd11, 0;
	shl.b32 	%r14, %r13, 1;
	setp.lt.u32 	%p1, %r14, %r10;
	or.b32  	%r15, %r14, 1;
	setp.lt.u32 	%p2, %r15, %r10;
	ld.global.v2.f64 	{%fd17, %fd18}, [%rd10+2048];
	add.s32 	%r16, %r14, 256;
	setp.lt.u32 	%p3, %r16, %r10;
	or.b32  	%r17, %r16, 1;
	setp.lt.u32 	%p4, %r17, %r10;
	ld.global.v2.f64 	{%fd21, %fd22}, [%rd10+4096];
	add.s32 	%r18, %r14, 512;
	setp.lt.u32 	%p5, %r18, %r10;
	or.b32  	%r19, %r18, 1;
	setp.lt.u32 	%p6, %r19, %r10;
	ld.global.v2.f64 	{%fd25, %fd26}, [%rd10+6144];
	add.s32 	%r20, %r14, 768;
	setp.lt.u32 	%p7, %r20, %r10;
	or.b32  	%r21, %r20, 1;
	setp.lt.u32 	%p8, %r21, %r10;
	shl.b32 	%r22, %r1, 4;
	mov.u32 	%r23, shared_d;
	add.s32 	%r2, %r23, %r22;
	selp.f64 	%fd29, %fd14, 0d0000000000000000, %p2;
	selp.f64 	%fd30, %fd13, 0d0000000000000000, %p1;
	st.shared.v2.f64 	[%r2], {%fd30, %fd29};
	selp.f64 	%fd31, %fd18, 0d0000000000000000, %p4;
	selp.f64 	%fd32, %fd17, 0d0000000000000000, %p3;
	st.shared.v2.f64 	[%r2+2048], {%fd32, %fd31};
	selp.f64 	%fd33, %fd22, 0d0000000000000000, %p6;
	selp.f64 	%fd34, %fd21, 0d0000000000000000, %p5;
	st.shared.v2.f64 	[%r2+4096], {%fd34, %fd33};
	selp.f64 	%fd35, %fd26, 0d0000000000000000, %p8;
	selp.f64 	%fd36, %fd25, 0d0000000000000000, %p7;
	st.shared.v2.f64 	[%r2+6144], {%fd36, %fd35};
	bar.sync 	0;
	shl.b32 	%r24, %r1, 6;
	add.s32 	%r3, %r23, %r24;
	ld.shared.v2.f64 	{%fd37, %fd38}, [%r3];
	ld.shared.v2.f64 	{%fd41, %fd42}, [%r3+16];
	ld.shared.v2.f64 	{%fd45, %fd46}, [%r3+32];
	ld.shared.v2.f64 	{%fd49, %fd50}, [%r3+48];
	add.f64 	%fd1, %fd37, 0d0000000000000000;
	add.f64 	%fd2, %fd1, %fd38;
	add.f64 	%fd3, %fd2, %fd41;
	add.f64 	%fd4, %fd3, %fd42;
	add.f64 	%fd5, %fd4, %fd45;
	add.f64 	%fd6, %fd5, %fd46;
	add.f64 	%fd7, %fd6, %fd49;
	add.f64 	%fd8, %fd7, %fd50;
	bar.sync 	0;
	shl.b32 	%r25, %r1, 3;
	add.s32 	%r26, %r23, %r25;
	st.shared.u64 	[%r26], %rd11;
	st.shared.f64 	[%r26+1024], %fd8;
	bar.sync 	0;
	ld.shared.f64 	%fd53, [%r26+1016];
	ld.shared.f64 	%fd54, [%r26+1024];
	add.f64 	%fd55, %fd54, %fd53;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd55;
	bar.sync 	0;
	ld.shared.f64 	%fd56, [%r26+1008];
	ld.shared.f64 	%fd57, [%r26+1024];
	add.f64 	%fd58, %fd57, %fd56;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd58;
	bar.sync 	0;
	ld.shared.f64 	%fd59, [%r26+992];
	ld.shared.f64 	%fd60, [%r26+1024];
	add.f64 	%fd61, %fd60, %fd59;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd61;
	bar.sync 	0;
	ld.shared.f64 	%fd62, [%r26+960];
	ld.shared.f64 	%fd63, [%r26+1024];
	add.f64 	%fd64, %fd63, %fd62;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd64;
	bar.sync 	0;
	ld.shared.f64 	%fd65, [%r26+896];
	ld.shared.f64 	%fd66, [%r26+1024];
	add.f64 	%fd67, %fd66, %fd65;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd67;
	bar.sync 	0;
	ld.shared.f64 	%fd68, [%r26+768];
	ld.shared.f64 	%fd69, [%r26+1024];
	add.f64 	%fd70, %fd69, %fd68;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd70;
	bar.sync 	0;
	ld.shared.f64 	%fd71, [%r26+512];
	ld.shared.f64 	%fd72, [%r26+1024];
	add.f64 	%fd9, %fd72, %fd71;
	bar.sync 	0;
	mul.wide.u32 	%rd12, %r11, 8;
	add.s64 	%rd2, %rd7, %rd12;
	setp.ne.s32 	%p9, %r1, 127;
	@%p9 bra 	$L__BB42_2;

	mov.b64 	%rd15, %fd9;
	and.b64  	%rd16, %rd15, -4;
	or.b64  	%rd14, %rd16, 1;
	// begin inline asm
	st.cg.u64 [%rd2], %rd14;
	// end inline asm

$L__BB42_2:
	and.b32  	%r4, %r1, 31;
	or.b32  	%r109, %r4, -32;
	mov.f64 	%fd96, 0d0000000000000000;
	mov.u32 	%r28, -1;
	bra.uni 	$L__BB42_3;

$L__BB42_8:
	add.s32 	%r109, %r109, -32;
	mov.f64 	%fd96, %fd11;

$L__BB42_3:
	mul.wide.s32 	%rd19, %r109, 8;
	add.s64 	%rd18, %rd2, %rd19;
	// begin inline asm
	ld.cg.u64 %rd17, [%rd18];
	// end inline asm
	cvt.u32.u64 	%r27, %rd17;
	and.b32  	%r7, %r27, 3;
	setp.eq.s32 	%p10, %r7, 0;
	vote.sync.any.pred 	%p11, %p10, %r28;
	@%p11 bra 	$L__BB42_3;

	and.b64  	%rd20, %rd17, -4;
	mov.b64 	%fd74, %rd20;
	setp.eq.s32 	%p12, %r7, 2;
	vote.sync.ballot.b32 	%r8, %p12, %r28;
	setp.eq.s32 	%p14, %r8, 0;
	add.f64 	%fd11, %fd96, %fd74;
	@%p14 bra 	$L__BB42_8;

	clz.b32 	%r55, %r8;
	mov.u32 	%r56, 31;
	sub.s32 	%r57, %r56, %r55;
	setp.lt.u32 	%p16, %r4, %r57;
	selp.f64 	%fd75, %fd96, %fd11, %p16;
	// begin inline asm
	mov.b64 {%r31,%r32}, %fd75;
	// end inline asm
	mov.u32 	%r58, 2;
	mov.u32 	%r59, 16;
	shfl.sync.down.b32 	%r34|%p17, %r32, %r59, %r56, %r28;
	shfl.sync.down.b32 	%r33|%p18, %r31, %r59, %r56, %r28;
	// begin inline asm
	mov.b64 %fd76, {%r33,%r34};
	// end inline asm
	add.f64 	%fd77, %fd75, %fd76;
	// begin inline asm
	mov.b64 {%r35,%r36}, %fd77;
	// end inline asm
	mov.u32 	%r61, 8;
	shfl.sync.down.b32 	%r38|%p19, %r36, %r61, %r56, %r28;
	shfl.sync.down.b32 	%r37|%p20, %r35, %r61, %r56, %r28;
	// begin inline asm
	mov.b64 %fd78, {%r37,%r38};
	// end inline asm
	add.f64 	%fd79, %fd77, %fd78;
	// begin inline asm
	mov.b64 {%r39,%r40}, %fd79;
	// end inline asm
	mov.u32 	%r62, 4;
	shfl.sync.down.b32 	%r42|%p21, %r40, %r62, %r56, %r28;
	shfl.sync.down.b32 	%r41|%p22, %r39, %r62, %r56, %r28;
	// begin inline asm
	mov.b64 %fd80, {%r41,%r42};
	// end inline asm
	add.f64 	%fd81, %fd79, %fd80;
	// begin inline asm
	mov.b64 {%r43,%r44}, %fd81;
	// end inline asm
	shfl.sync.down.b32 	%r46|%p23, %r44, %r58, %r56, %r28;
	shfl.sync.down.b32 	%r45|%p24, %r43, %r58, %r56, %r28;
	// begin inline asm
	mov.b64 %fd82, {%r45,%r46};
	// end inline asm
	add.f64 	%fd83, %fd81, %fd82;
	// begin inline asm
	mov.b64 {%r47,%r48}, %fd83;
	// end inline asm
	mov.u32 	%r63, 1;
	shfl.sync.down.b32 	%r50|%p25, %r48, %r63, %r56, %r28;
	shfl.sync.down.b32 	%r49|%p26, %r47, %r63, %r56, %r28;
	// begin inline asm
	mov.b64 %fd84, {%r49,%r50};
	// end inline asm
	add.f64 	%fd85, %fd83, %fd84;
	// begin inline asm
	mov.b64 {%r51,%r52}, %fd85;
	// end inline asm
	mov.u32 	%r64, 0;
	shfl.sync.idx.b32 	%r54|%p27, %r52, %r64, %r56, %r28;
	shfl.sync.idx.b32 	%r53|%p28, %r51, %r64, %r56, %r28;
	// begin inline asm
	mov.b64 %fd86, {%r53,%r54};
	// end inline asm
	add.f64 	%fd12, %fd9, %fd86;
	@%p9 bra 	$L__BB42_7;

	mov.u32 	%r104, %ctaid.x;
	mul.wide.u32 	%rd30, %r104, 8;
	ld.param.u64 	%rd29, [prefix_sum_exc_large_f64_param_3];
	add.s64 	%rd28, %rd29, %rd30;
	mov.b64 	%rd23, %fd12;
	and.b64  	%rd24, %rd23, -4;
	or.b64  	%rd22, %rd24, 2;
	// begin inline asm
	st.cg.u64 [%rd28], %rd22;
	// end inline asm

$L__BB42_7:
	mov.u32 	%r108, %tid.x;
	shl.b32 	%r107, %r108, 4;
	mov.u32 	%r106, shared_d;
	add.s32 	%r105, %r106, %r107;
	ld.param.u64 	%rd32, [prefix_sum_exc_large_f64_param_1];
	cvta.to.global.u64 	%rd31, %rd32;
	mov.u32 	%r103, %tid.x;
	mov.u32 	%r102, %ctaid.x;
	shl.b32 	%r101, %r102, 9;
	add.s32 	%r100, %r101, %r103;
	cvt.u64.u32 	%rd27, %r100;
	shl.b32 	%r99, %r103, 6;
	mov.u32 	%r98, shared_d;
	add.s32 	%r97, %r98, %r99;
	sub.f64 	%fd87, %fd12, %fd8;
	add.f64 	%fd88, %fd87, %fd1;
	add.f64 	%fd89, %fd87, 0d0000000000000000;
	st.shared.v2.f64 	[%r97], {%fd89, %fd88};
	add.f64 	%fd90, %fd87, %fd3;
	add.f64 	%fd91, %fd87, %fd2;
	st.shared.v2.f64 	[%r97+16], {%fd91, %fd90};
	add.f64 	%fd92, %fd87, %fd5;
	add.f64 	%fd93, %fd87, %fd4;
	st.shared.v2.f64 	[%r97+32], {%fd93, %fd92};
	add.f64 	%fd94, %fd87, %fd7;
	add.f64 	%fd95, %fd87, %fd6;
	st.shared.v2.f64 	[%r97+48], {%fd95, %fd94};
	bar.sync 	0;
	shl.b64 	%rd25, %rd27, 4;
	add.s64 	%rd26, %rd31, %rd25;
	ld.shared.v4.u32 	{%r65, %r66, %r67, %r68}, [%r105];
	st.global.v4.u32 	[%rd26], {%r65, %r66, %r67, %r68};
	ld.shared.v4.u32 	{%r73, %r74, %r75, %r76}, [%r105+2048];
	st.global.v4.u32 	[%rd26+2048], {%r73, %r74, %r75, %r76};
	ld.shared.v4.u32 	{%r81, %r82, %r83, %r84}, [%r105+4096];
	st.global.v4.u32 	[%rd26+4096], {%r81, %r82, %r83, %r84};
	ld.shared.v4.u32 	{%r89, %r90, %r91, %r92}, [%r105+6144];
	st.global.v4.u32 	[%rd26+6144], {%r89, %r90, %r91, %r92};
	ret;

}
	// .globl	prefix_sum_inc_large_u32
.visible .entry prefix_sum_inc_large_u32(
	.param .u64 prefix_sum_inc_large_u32_param_0,
	.param .u64 prefix_sum_inc_large_u32_param_1,
	.param .u32 prefix_sum_inc_large_u32_param_2,
	.param .u64 prefix_sum_inc_large_u32_param_3
)
{
	.reg .pred 	%p<15>;
	.reg .b32 	%r<200>;
	.reg .b64 	%rd<25>;


	ld.param.u64 	%rd5, [prefix_sum_inc_large_u32_param_0];
	ld.param.u64 	%rd4, [prefix_sum_inc_large_u32_param_1];
	ld.param.u64 	%rd6, [prefix_sum_inc_large_u32_param_3];
	cvta.to.global.u64 	%rd7, %rd5;
	mov.u32 	%r31, %ctaid.x;
	shl.b32 	%r32, %r31, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r33, %r32, %r1;
	cvt.u64.u32 	%rd1, %r33;
	mul.wide.u32 	%rd8, %r33, 16;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v4.u32 	{%r34, %r35, %r36, %r37}, [%rd9];
	ld.global.v4.u32 	{%r42, %r43, %r44, %r45}, [%rd9+2048];
	ld.global.v4.u32 	{%r50, %r51, %r52, %r53}, [%rd9+4096];
	ld.global.v4.u32 	{%r58, %r59, %r60, %r61}, [%rd9+6144];
	shl.b32 	%r66, %r1, 4;
	mov.u32 	%r67, shared;
	add.s32 	%r2, %r67, %r66;
	mov.u32 	%r199, 0;
	st.shared.v4.u32 	[%r2], {%r34, %r35, %r36, %r37};
	st.shared.v4.u32 	[%r2+2048], {%r42, %r43, %r44, %r45};
	st.shared.v4.u32 	[%r2+4096], {%r50, %r51, %r52, %r53};
	st.shared.v4.u32 	[%r2+6144], {%r58, %r59, %r60, %r61};
	bar.sync 	0;
	shl.b32 	%r69, %r1, 2;
	shl.b32 	%r70, %r1, 6;
	add.s32 	%r3, %r67, %r70;
	ld.shared.v4.u32 	{%r71, %r72, %r73, %r74}, [%r3];
	ld.shared.v4.u32 	{%r78, %r79, %r80, %r81}, [%r3+16];
	ld.shared.v4.u32 	{%r86, %r87, %r88, %r89}, [%r3+32];
	ld.shared.v4.u32 	{%r94, %r95, %r96, %r97}, [%r3+48];
	add.s32 	%r5, %r72, %r71;
	add.s32 	%r6, %r73, %r5;
	add.s32 	%r7, %r74, %r6;
	add.s32 	%r8, %r78, %r7;
	add.s32 	%r9, %r79, %r8;
	add.s32 	%r10, %r80, %r9;
	add.s32 	%r11, %r81, %r10;
	add.s32 	%r12, %r86, %r11;
	add.s32 	%r13, %r87, %r12;
	add.s32 	%r14, %r88, %r13;
	add.s32 	%r15, %r89, %r14;
	add.s32 	%r16, %r94, %r15;
	add.s32 	%r17, %r95, %r16;
	add.s32 	%r18, %r96, %r17;
	add.s32 	%r19, %r97, %r18;
	bar.sync 	0;
	add.s32 	%r102, %r67, %r69;
	st.shared.u32 	[%r102], %r199;
	st.shared.u32 	[%r102+512], %r19;
	bar.sync 	0;
	ld.shared.u32 	%r103, [%r102+508];
	ld.shared.u32 	%r104, [%r102+512];
	add.s32 	%r105, %r103, %r104;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r105;
	bar.sync 	0;
	ld.shared.u32 	%r106, [%r102+504];
	ld.shared.u32 	%r107, [%r102+512];
	add.s32 	%r108, %r106, %r107;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r108;
	bar.sync 	0;
	ld.shared.u32 	%r109, [%r102+496];
	ld.shared.u32 	%r110, [%r102+512];
	add.s32 	%r111, %r109, %r110;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r111;
	bar.sync 	0;
	ld.shared.u32 	%r112, [%r102+480];
	ld.shared.u32 	%r113, [%r102+512];
	add.s32 	%r114, %r112, %r113;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r114;
	bar.sync 	0;
	ld.shared.u32 	%r115, [%r102+448];
	ld.shared.u32 	%r116, [%r102+512];
	add.s32 	%r117, %r115, %r116;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r117;
	bar.sync 	0;
	ld.shared.u32 	%r118, [%r102+384];
	ld.shared.u32 	%r119, [%r102+512];
	add.s32 	%r120, %r118, %r119;
	bar.sync 	0;
	st.shared.u32 	[%r102+512], %r120;
	bar.sync 	0;
	ld.shared.u32 	%r121, [%r102+256];
	ld.shared.u32 	%r122, [%r102+512];
	add.s32 	%r20, %r121, %r122;
	bar.sync 	0;
	mul.wide.u32 	%rd10, %r31, 8;
	add.s64 	%rd2, %rd6, %rd10;
	setp.ne.s32 	%p1, %r1, 127;
	@%p1 bra 	$L__BB43_2;

	cvt.u64.u32 	%rd13, %r20;
	shl.b64 	%rd14, %rd13, 32;
	or.b64  	%rd12, %rd14, 1;
	// begin inline asm
	st.cg.u64 [%rd2], %rd12;
	// end inline asm

$L__BB43_2:
	and.b32  	%r21, %r1, 31;
	or.b32  	%r198, %r21, -32;
	cvta.to.global.u64 	%rd3, %rd4;
	bra.uni 	$L__BB43_3;

$L__BB43_8:
	add.s32 	%r199, %r199, %r26;
	add.s32 	%r198, %r198, -32;

$L__BB43_3:
	mul.wide.s32 	%rd17, %r198, 8;
	add.s64 	%rd16, %rd2, %rd17;
	// begin inline asm
	ld.cg.u64 %rd15, [%rd16];
	// end inline asm
	cvt.u32.u64 	%r25, %rd15;
	shr.u64 	%rd18, %rd15, 32;
	cvt.u32.u64 	%r26, %rd18;
	setp.eq.s32 	%p2, %r25, 0;
	mov.u32 	%r124, -1;
	vote.sync.any.pred 	%p3, %p2, %r124;
	@%p3 bra 	$L__BB43_3;

	setp.eq.s32 	%p4, %r25, 2;
	vote.sync.ballot.b32 	%r27, %p4, %r124;
	setp.eq.s32 	%p6, %r27, 0;
	@%p6 bra 	$L__BB43_8;

	clz.b32 	%r127, %r27;
	mov.u32 	%r128, 31;
	sub.s32 	%r129, %r128, %r127;
	setp.lt.u32 	%p8, %r21, %r129;
	selp.b32 	%r130, 0, %r26, %p8;
	mov.u32 	%r131, 0;
	add.s32 	%r132, %r130, %r199;
	mov.u32 	%r133, 2;
	mov.u32 	%r134, 16;
	shfl.sync.down.b32 	%r136|%p9, %r132, %r134, %r128, %r124;
	add.s32 	%r137, %r136, %r132;
	mov.u32 	%r138, 8;
	shfl.sync.down.b32 	%r139|%p10, %r137, %r138, %r128, %r124;
	add.s32 	%r140, %r139, %r137;
	mov.u32 	%r141, 4;
	shfl.sync.down.b32 	%r142|%p11, %r140, %r141, %r128, %r124;
	add.s32 	%r143, %r142, %r140;
	shfl.sync.down.b32 	%r144|%p12, %r143, %r133, %r128, %r124;
	add.s32 	%r145, %r144, %r143;
	mov.u32 	%r146, 1;
	shfl.sync.down.b32 	%r147|%p13, %r145, %r146, %r128, %r124;
	add.s32 	%r148, %r147, %r145;
	shfl.sync.idx.b32 	%r149|%p14, %r148, %r131, %r128, %r124;
	add.s32 	%r28, %r149, %r20;
	@%p1 bra 	$L__BB43_7;

	cvt.u64.u32 	%rd21, %r28;
	shl.b64 	%rd22, %rd21, 32;
	or.b64  	%rd20, %rd22, 2;
	// begin inline asm
	st.cg.u64 [%rd2], %rd20;
	// end inline asm

$L__BB43_7:
	sub.s32 	%r150, %r28, %r19;
	add.s32 	%r151, %r150, %r7;
	add.s32 	%r152, %r150, %r6;
	add.s32 	%r153, %r150, %r5;
	add.s32 	%r154, %r150, %r71;
	st.shared.v4.u32 	[%r3], {%r154, %r153, %r152, %r151};
	add.s32 	%r155, %r150, %r11;
	add.s32 	%r156, %r150, %r10;
	add.s32 	%r157, %r150, %r9;
	add.s32 	%r158, %r150, %r8;
	st.shared.v4.u32 	[%r3+16], {%r158, %r157, %r156, %r155};
	add.s32 	%r159, %r150, %r15;
	add.s32 	%r160, %r150, %r14;
	add.s32 	%r161, %r150, %r13;
	add.s32 	%r162, %r150, %r12;
	st.shared.v4.u32 	[%r3+32], {%r162, %r161, %r160, %r159};
	add.s32 	%r163, %r150, %r18;
	add.s32 	%r164, %r150, %r17;
	add.s32 	%r165, %r150, %r16;
	st.shared.v4.u32 	[%r3+48], {%r165, %r164, %r163, %r28};
	bar.sync 	0;
	shl.b64 	%rd23, %rd1, 4;
	add.s64 	%rd24, %rd3, %rd23;
	ld.shared.v4.u32 	{%r166, %r167, %r168, %r169}, [%r2];
	st.global.v4.u32 	[%rd24], {%r166, %r167, %r168, %r169};
	ld.shared.v4.u32 	{%r174, %r175, %r176, %r177}, [%r2+2048];
	st.global.v4.u32 	[%rd24+2048], {%r174, %r175, %r176, %r177};
	ld.shared.v4.u32 	{%r182, %r183, %r184, %r185}, [%r2+4096];
	st.global.v4.u32 	[%rd24+4096], {%r182, %r183, %r184, %r185};
	ld.shared.v4.u32 	{%r190, %r191, %r192, %r193}, [%r2+6144];
	st.global.v4.u32 	[%rd24+6144], {%r190, %r191, %r192, %r193};
	ret;

}
	// .globl	prefix_sum_inc_large_u64
.visible .entry prefix_sum_inc_large_u64(
	.param .u64 prefix_sum_inc_large_u64_param_0,
	.param .u64 prefix_sum_inc_large_u64_param_1,
	.param .u32 prefix_sum_inc_large_u64_param_2,
	.param .u64 prefix_sum_inc_large_u64_param_3
)
{
	.reg .pred 	%p<21>;
	.reg .b32 	%r<101>;
	.reg .b64 	%rd<116>;


	ld.param.u64 	%rd18, [prefix_sum_inc_large_u64_param_0];
	ld.param.u64 	%rd19, [prefix_sum_inc_large_u64_param_3];
	cvta.to.global.u64 	%rd20, %rd18;
	mov.u32 	%r10, %ctaid.x;
	shl.b32 	%r11, %r10, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r12, %r11, %r1;
	mul.wide.u32 	%rd21, %r12, 16;
	add.s64 	%rd22, %rd20, %rd21;
	ld.global.v2.u64 	{%rd23, %rd24}, [%rd22];
	mov.u64 	%rd115, 0;
	ld.global.v2.u64 	{%rd28, %rd29}, [%rd22+2048];
	ld.global.v2.u64 	{%rd32, %rd33}, [%rd22+4096];
	ld.global.v2.u64 	{%rd36, %rd37}, [%rd22+6144];
	shl.b32 	%r13, %r1, 4;
	mov.u32 	%r14, shared;
	add.s32 	%r2, %r14, %r13;
	st.shared.v2.u64 	[%r2], {%rd23, %rd24};
	st.shared.v2.u64 	[%r2+2048], {%rd28, %rd29};
	st.shared.v2.u64 	[%r2+4096], {%rd32, %rd33};
	st.shared.v2.u64 	[%r2+6144], {%rd36, %rd37};
	bar.sync 	0;
	shl.b32 	%r15, %r1, 6;
	add.s32 	%r3, %r14, %r15;
	ld.shared.v2.u64 	{%rd40, %rd41}, [%r3];
	ld.shared.v2.u64 	{%rd43, %rd44}, [%r3+16];
	ld.shared.v2.u64 	{%rd47, %rd48}, [%r3+32];
	ld.shared.v2.u64 	{%rd51, %rd52}, [%r3+48];
	add.s64 	%rd3, %rd41, %rd40;
	add.s64 	%rd4, %rd43, %rd3;
	add.s64 	%rd5, %rd44, %rd4;
	add.s64 	%rd6, %rd47, %rd5;
	add.s64 	%rd7, %rd48, %rd6;
	add.s64 	%rd8, %rd51, %rd7;
	add.s64 	%rd9, %rd52, %rd8;
	bar.sync 	0;
	shl.b32 	%r16, %r1, 3;
	add.s32 	%r17, %r14, %r16;
	st.shared.u64 	[%r17], %rd115;
	st.shared.u64 	[%r17+1024], %rd9;
	bar.sync 	0;
	ld.shared.u64 	%rd55, [%r17+1016];
	ld.shared.u64 	%rd56, [%r17+1024];
	add.s64 	%rd57, %rd55, %rd56;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd57;
	bar.sync 	0;
	ld.shared.u64 	%rd58, [%r17+1008];
	ld.shared.u64 	%rd59, [%r17+1024];
	add.s64 	%rd60, %rd58, %rd59;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd60;
	bar.sync 	0;
	ld.shared.u64 	%rd61, [%r17+992];
	ld.shared.u64 	%rd62, [%r17+1024];
	add.s64 	%rd63, %rd61, %rd62;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd63;
	bar.sync 	0;
	ld.shared.u64 	%rd64, [%r17+960];
	ld.shared.u64 	%rd65, [%r17+1024];
	add.s64 	%rd66, %rd64, %rd65;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd66;
	bar.sync 	0;
	ld.shared.u64 	%rd67, [%r17+896];
	ld.shared.u64 	%rd68, [%r17+1024];
	add.s64 	%rd69, %rd67, %rd68;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd69;
	bar.sync 	0;
	ld.shared.u64 	%rd70, [%r17+768];
	ld.shared.u64 	%rd71, [%r17+1024];
	add.s64 	%rd72, %rd70, %rd71;
	bar.sync 	0;
	st.shared.u64 	[%r17+1024], %rd72;
	bar.sync 	0;
	ld.shared.u64 	%rd73, [%r17+512];
	ld.shared.u64 	%rd74, [%r17+1024];
	add.s64 	%rd10, %rd73, %rd74;
	bar.sync 	0;
	mul.wide.u32 	%rd75, %r10, 8;
	add.s64 	%rd11, %rd19, %rd75;
	setp.ne.s32 	%p1, %r1, 127;
	@%p1 bra 	$L__BB44_2;

	shl.b64 	%rd78, %rd10, 2;
	or.b64  	%rd77, %rd78, 1;
	// begin inline asm
	st.cg.u64 [%rd11], %rd77;
	// end inline asm

$L__BB44_2:
	and.b32  	%r4, %r1, 31;
	or.b32  	%r100, %r4, -32;
	mov.u32 	%r19, -1;
	bra.uni 	$L__BB44_3;

$L__BB44_8:
	add.s64 	%rd115, %rd14, %rd115;
	add.s32 	%r100, %r100, -32;

$L__BB44_3:
	mul.wide.s32 	%rd82, %r100, 8;
	add.s64 	%rd81, %rd11, %rd82;
	// begin inline asm
	ld.cg.u64 %rd80, [%rd81];
	// end inline asm
	cvt.u32.u64 	%r18, %rd80;
	and.b32  	%r7, %r18, 3;
	shr.u64 	%rd14, %rd80, 2;
	setp.eq.s32 	%p2, %r7, 0;
	vote.sync.any.pred 	%p3, %p2, %r19;
	@%p3 bra 	$L__BB44_3;

	setp.eq.s32 	%p4, %r7, 2;
	vote.sync.ballot.b32 	%r8, %p4, %r19;
	setp.eq.s32 	%p6, %r8, 0;
	@%p6 bra 	$L__BB44_8;

	clz.b32 	%r46, %r8;
	mov.u32 	%r47, 31;
	sub.s32 	%r48, %r47, %r46;
	setp.lt.u32 	%p8, %r4, %r48;
	selp.b64 	%rd95, 0, %rd14, %p8;
	add.s64 	%rd83, %rd95, %rd115;
	// begin inline asm
	mov.b64 {%r22,%r23}, %rd83;
	// end inline asm
	mov.u32 	%r49, 2;
	mov.u32 	%r50, 16;
	shfl.sync.down.b32 	%r25|%p9, %r23, %r50, %r47, %r19;
	shfl.sync.down.b32 	%r24|%p10, %r22, %r50, %r47, %r19;
	// begin inline asm
	mov.b64 %rd84, {%r24,%r25};
	// end inline asm
	add.s64 	%rd85, %rd84, %rd83;
	// begin inline asm
	mov.b64 {%r26,%r27}, %rd85;
	// end inline asm
	mov.u32 	%r52, 8;
	shfl.sync.down.b32 	%r29|%p11, %r27, %r52, %r47, %r19;
	shfl.sync.down.b32 	%r28|%p12, %r26, %r52, %r47, %r19;
	// begin inline asm
	mov.b64 %rd86, {%r28,%r29};
	// end inline asm
	add.s64 	%rd87, %rd86, %rd85;
	// begin inline asm
	mov.b64 {%r30,%r31}, %rd87;
	// end inline asm
	mov.u32 	%r53, 4;
	shfl.sync.down.b32 	%r33|%p13, %r31, %r53, %r47, %r19;
	shfl.sync.down.b32 	%r32|%p14, %r30, %r53, %r47, %r19;
	// begin inline asm
	mov.b64 %rd88, {%r32,%r33};
	// end inline asm
	add.s64 	%rd89, %rd88, %rd87;
	// begin inline asm
	mov.b64 {%r34,%r35}, %rd89;
	// end inline asm
	shfl.sync.down.b32 	%r37|%p15, %r35, %r49, %r47, %r19;
	shfl.sync.down.b32 	%r36|%p16, %r34, %r49, %r47, %r19;
	// begin inline asm
	mov.b64 %rd90, {%r36,%r37};
	// end inline asm
	add.s64 	%rd91, %rd90, %rd89;
	// begin inline asm
	mov.b64 {%r38,%r39}, %rd91;
	// end inline asm
	mov.u32 	%r54, 1;
	shfl.sync.down.b32 	%r41|%p17, %r39, %r54, %r47, %r19;
	shfl.sync.down.b32 	%r40|%p18, %r38, %r54, %r47, %r19;
	// begin inline asm
	mov.b64 %rd92, {%r40,%r41};
	// end inline asm
	add.s64 	%rd93, %rd92, %rd91;
	// begin inline asm
	mov.b64 {%r42,%r43}, %rd93;
	// end inline asm
	mov.u32 	%r55, 0;
	shfl.sync.idx.b32 	%r45|%p19, %r43, %r55, %r47, %r19;
	shfl.sync.idx.b32 	%r44|%p20, %r42, %r55, %r47, %r19;
	// begin inline asm
	mov.b64 %rd94, {%r44,%r45};
	// end inline asm
	add.s64 	%rd15, %rd94, %rd10;
	@%p1 bra 	$L__BB44_7;

	mov.u32 	%r95, %ctaid.x;
	mul.wide.u32 	%rd112, %r95, 8;
	ld.param.u64 	%rd111, [prefix_sum_inc_large_u64_param_3];
	add.s64 	%rd110, %rd111, %rd112;
	shl.b64 	%rd98, %rd15, 2;
	or.b64  	%rd97, %rd98, 2;
	// begin inline asm
	st.cg.u64 [%rd110], %rd97;
	// end inline asm

$L__BB44_7:
	mov.u32 	%r99, %tid.x;
	shl.b32 	%r98, %r99, 4;
	mov.u32 	%r97, shared;
	add.s32 	%r96, %r97, %r98;
	ld.param.u64 	%rd114, [prefix_sum_inc_large_u64_param_1];
	cvta.to.global.u64 	%rd113, %rd114;
	mov.u32 	%r94, %tid.x;
	mov.u32 	%r93, %ctaid.x;
	shl.b32 	%r92, %r93, 9;
	add.s32 	%r91, %r92, %r94;
	cvt.u64.u32 	%rd109, %r91;
	shl.b32 	%r90, %r94, 6;
	mov.u32 	%r89, shared;
	add.s32 	%r88, %r89, %r90;
	sub.s64 	%rd99, %rd15, %rd9;
	add.s64 	%rd100, %rd99, %rd3;
	add.s64 	%rd101, %rd99, %rd40;
	st.shared.v2.u64 	[%r88], {%rd101, %rd100};
	add.s64 	%rd102, %rd99, %rd5;
	add.s64 	%rd103, %rd99, %rd4;
	st.shared.v2.u64 	[%r88+16], {%rd103, %rd102};
	add.s64 	%rd104, %rd99, %rd7;
	add.s64 	%rd105, %rd99, %rd6;
	st.shared.v2.u64 	[%r88+32], {%rd105, %rd104};
	add.s64 	%rd106, %rd99, %rd8;
	st.shared.v2.u64 	[%r88+48], {%rd106, %rd15};
	bar.sync 	0;
	shl.b64 	%rd107, %rd109, 4;
	add.s64 	%rd108, %rd113, %rd107;
	ld.shared.v4.u32 	{%r56, %r57, %r58, %r59}, [%r96];
	st.global.v4.u32 	[%rd108], {%r56, %r57, %r58, %r59};
	ld.shared.v4.u32 	{%r64, %r65, %r66, %r67}, [%r96+2048];
	st.global.v4.u32 	[%rd108+2048], {%r64, %r65, %r66, %r67};
	ld.shared.v4.u32 	{%r72, %r73, %r74, %r75}, [%r96+4096];
	st.global.v4.u32 	[%rd108+4096], {%r72, %r73, %r74, %r75};
	ld.shared.v4.u32 	{%r80, %r81, %r82, %r83}, [%r96+6144];
	st.global.v4.u32 	[%rd108+6144], {%r80, %r81, %r82, %r83};
	ret;

}
	// .globl	prefix_sum_inc_large_f32
.visible .entry prefix_sum_inc_large_f32(
	.param .u64 prefix_sum_inc_large_f32_param_0,
	.param .u64 prefix_sum_inc_large_f32_param_1,
	.param .u32 prefix_sum_inc_large_f32_param_2,
	.param .u64 prefix_sum_inc_large_f32_param_3
)
{
	.reg .pred 	%p<31>;
	.reg .f32 	%f<153>;
	.reg .b32 	%r<97>;
	.reg .b64 	%rd<26>;


	ld.param.u64 	%rd6, [prefix_sum_inc_large_f32_param_0];
	ld.param.u64 	%rd5, [prefix_sum_inc_large_f32_param_1];
	ld.param.u32 	%r10, [prefix_sum_inc_large_f32_param_2];
	ld.param.u64 	%rd7, [prefix_sum_inc_large_f32_param_3];
	cvta.to.global.u64 	%rd8, %rd6;
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r12, %r11, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r13, %r12, %r1;
	cvt.u64.u32 	%rd1, %r13;
	mul.wide.u32 	%rd9, %r13, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v4.f32 	{%f21, %f22, %f23, %f24}, [%rd10];
	shl.b32 	%r14, %r13, 2;
	setp.lt.u32 	%p1, %r14, %r10;
	mov.u32 	%r15, 0;
	or.b32  	%r16, %r14, 1;
	setp.lt.u32 	%p2, %r16, %r10;
	or.b32  	%r17, %r14, 2;
	setp.lt.u32 	%p3, %r17, %r10;
	or.b32  	%r18, %r14, 3;
	setp.lt.u32 	%p4, %r18, %r10;
	ld.global.v4.f32 	{%f29, %f30, %f31, %f32}, [%rd10+2048];
	add.s32 	%r19, %r14, 512;
	setp.lt.u32 	%p5, %r19, %r10;
	or.b32  	%r20, %r19, 1;
	setp.lt.u32 	%p6, %r20, %r10;
	or.b32  	%r21, %r19, 2;
	setp.lt.u32 	%p7, %r21, %r10;
	or.b32  	%r22, %r19, 3;
	setp.lt.u32 	%p8, %r22, %r10;
	ld.global.v4.f32 	{%f37, %f38, %f39, %f40}, [%rd10+4096];
	add.s32 	%r23, %r14, 1024;
	setp.lt.u32 	%p9, %r23, %r10;
	or.b32  	%r24, %r23, 1;
	setp.lt.u32 	%p10, %r24, %r10;
	or.b32  	%r25, %r23, 2;
	setp.lt.u32 	%p11, %r25, %r10;
	or.b32  	%r26, %r23, 3;
	setp.lt.u32 	%p12, %r26, %r10;
	ld.global.v4.f32 	{%f45, %f46, %f47, %f48}, [%rd10+6144];
	add.s32 	%r27, %r14, 1536;
	setp.lt.u32 	%p13, %r27, %r10;
	or.b32  	%r28, %r27, 1;
	setp.lt.u32 	%p14, %r28, %r10;
	or.b32  	%r29, %r27, 2;
	setp.lt.u32 	%p15, %r29, %r10;
	or.b32  	%r30, %r27, 3;
	setp.lt.u32 	%p16, %r30, %r10;
	shl.b32 	%r31, %r1, 4;
	mov.u32 	%r32, shared;
	add.s32 	%r2, %r32, %r31;
	selp.f32 	%f53, %f24, 0f00000000, %p4;
	selp.f32 	%f54, %f23, 0f00000000, %p3;
	selp.f32 	%f55, %f22, 0f00000000, %p2;
	selp.f32 	%f56, %f21, 0f00000000, %p1;
	st.shared.v4.f32 	[%r2], {%f56, %f55, %f54, %f53};
	selp.f32 	%f57, %f32, 0f00000000, %p8;
	selp.f32 	%f58, %f31, 0f00000000, %p7;
	selp.f32 	%f59, %f30, 0f00000000, %p6;
	selp.f32 	%f60, %f29, 0f00000000, %p5;
	st.shared.v4.f32 	[%r2+2048], {%f60, %f59, %f58, %f57};
	selp.f32 	%f61, %f40, 0f00000000, %p12;
	selp.f32 	%f62, %f39, 0f00000000, %p11;
	selp.f32 	%f63, %f38, 0f00000000, %p10;
	selp.f32 	%f64, %f37, 0f00000000, %p9;
	st.shared.v4.f32 	[%r2+4096], {%f64, %f63, %f62, %f61};
	selp.f32 	%f65, %f48, 0f00000000, %p16;
	selp.f32 	%f66, %f47, 0f00000000, %p15;
	selp.f32 	%f67, %f46, 0f00000000, %p14;
	selp.f32 	%f68, %f45, 0f00000000, %p13;
	st.shared.v4.f32 	[%r2+6144], {%f68, %f67, %f66, %f65};
	bar.sync 	0;
	shl.b32 	%r33, %r1, 2;
	shl.b32 	%r34, %r1, 6;
	add.s32 	%r3, %r32, %r34;
	ld.shared.v4.f32 	{%f69, %f70, %f71, %f72}, [%r3];
	ld.shared.v4.f32 	{%f77, %f78, %f79, %f80}, [%r3+16];
	ld.shared.v4.f32 	{%f85, %f86, %f87, %f88}, [%r3+32];
	ld.shared.v4.f32 	{%f93, %f94, %f95, %f96}, [%r3+48];
	add.f32 	%f1, %f69, 0f00000000;
	add.f32 	%f2, %f1, %f70;
	add.f32 	%f3, %f2, %f71;
	add.f32 	%f4, %f3, %f72;
	add.f32 	%f5, %f4, %f77;
	add.f32 	%f6, %f5, %f78;
	add.f32 	%f7, %f6, %f79;
	add.f32 	%f8, %f7, %f80;
	add.f32 	%f9, %f8, %f85;
	add.f32 	%f10, %f9, %f86;
	add.f32 	%f11, %f10, %f87;
	add.f32 	%f12, %f11, %f88;
	add.f32 	%f13, %f12, %f93;
	add.f32 	%f14, %f13, %f94;
	add.f32 	%f15, %f14, %f95;
	add.f32 	%f16, %f15, %f96;
	bar.sync 	0;
	add.s32 	%r35, %r32, %r33;
	st.shared.u32 	[%r35], %r15;
	st.shared.f32 	[%r35+512], %f16;
	bar.sync 	0;
	ld.shared.f32 	%f101, [%r35+508];
	ld.shared.f32 	%f102, [%r35+512];
	add.f32 	%f103, %f102, %f101;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f103;
	bar.sync 	0;
	ld.shared.f32 	%f104, [%r35+504];
	ld.shared.f32 	%f105, [%r35+512];
	add.f32 	%f106, %f105, %f104;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f106;
	bar.sync 	0;
	ld.shared.f32 	%f107, [%r35+496];
	ld.shared.f32 	%f108, [%r35+512];
	add.f32 	%f109, %f108, %f107;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f109;
	bar.sync 	0;
	ld.shared.f32 	%f110, [%r35+480];
	ld.shared.f32 	%f111, [%r35+512];
	add.f32 	%f112, %f111, %f110;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f112;
	bar.sync 	0;
	ld.shared.f32 	%f113, [%r35+448];
	ld.shared.f32 	%f114, [%r35+512];
	add.f32 	%f115, %f114, %f113;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f115;
	bar.sync 	0;
	ld.shared.f32 	%f116, [%r35+384];
	ld.shared.f32 	%f117, [%r35+512];
	add.f32 	%f118, %f117, %f116;
	bar.sync 	0;
	st.shared.f32 	[%r35+512], %f118;
	bar.sync 	0;
	ld.shared.f32 	%f119, [%r35+256];
	ld.shared.f32 	%f120, [%r35+512];
	add.f32 	%f17, %f120, %f119;
	bar.sync 	0;
	mul.wide.u32 	%rd11, %r11, 8;
	add.s64 	%rd2, %rd7, %rd11;
	setp.ne.s32 	%p17, %r1, 127;
	@%p17 bra 	$L__BB45_2;

	mov.b32 	%r36, %f17;
	cvt.u64.u32 	%rd14, %r36;
	shl.b64 	%rd15, %rd14, 32;
	or.b64  	%rd13, %rd15, 1;
	// begin inline asm
	st.cg.u64 [%rd2], %rd13;
	// end inline asm

$L__BB45_2:
	and.b32  	%r4, %r1, 31;
	or.b32  	%r96, %r4, -32;
	cvta.to.global.u64 	%rd3, %rd5;
	mov.f32 	%f152, 0f00000000;
	mov.u32 	%r37, -1;
	bra.uni 	$L__BB45_3;

$L__BB45_8:
	add.s32 	%r96, %r96, -32;
	mov.f32 	%f152, %f19;

$L__BB45_3:
	mul.wide.s32 	%rd18, %r96, 8;
	add.s64 	%rd17, %rd2, %rd18;
	// begin inline asm
	ld.cg.u64 %rd16, [%rd17];
	// end inline asm
	cvt.u32.u64 	%r7, %rd16;
	setp.eq.s32 	%p18, %r7, 0;
	vote.sync.any.pred 	%p19, %p18, %r37;
	@%p19 bra 	$L__BB45_3;

	shr.u64 	%rd19, %rd16, 32;
	cvt.u32.u64 	%r39, %rd19;
	mov.b32 	%f122, %r39;
	setp.eq.s32 	%p20, %r7, 2;
	vote.sync.ballot.b32 	%r8, %p20, %r37;
	setp.eq.s32 	%p22, %r8, 0;
	add.f32 	%f19, %f152, %f122;
	@%p22 bra 	$L__BB45_8;

	clz.b32 	%r41, %r8;
	mov.u32 	%r42, 31;
	sub.s32 	%r43, %r42, %r41;
	setp.lt.u32 	%p24, %r4, %r43;
	selp.f32 	%f123, %f152, %f19, %p24;
	mov.b32 	%r44, %f123;
	mov.u32 	%r45, 2;
	mov.u32 	%r46, 16;
	shfl.sync.down.b32 	%r48|%p25, %r44, %r46, %r42, %r37;
	mov.b32 	%f124, %r48;
	add.f32 	%f125, %f123, %f124;
	mov.b32 	%r49, %f125;
	mov.u32 	%r50, 8;
	shfl.sync.down.b32 	%r51|%p26, %r49, %r50, %r42, %r37;
	mov.b32 	%f126, %r51;
	add.f32 	%f127, %f125, %f126;
	mov.b32 	%r52, %f127;
	mov.u32 	%r53, 4;
	shfl.sync.down.b32 	%r54|%p27, %r52, %r53, %r42, %r37;
	mov.b32 	%f128, %r54;
	add.f32 	%f129, %f127, %f128;
	mov.b32 	%r55, %f129;
	shfl.sync.down.b32 	%r56|%p28, %r55, %r45, %r42, %r37;
	mov.b32 	%f130, %r56;
	add.f32 	%f131, %f129, %f130;
	mov.b32 	%r57, %f131;
	mov.u32 	%r58, 1;
	shfl.sync.down.b32 	%r59|%p29, %r57, %r58, %r42, %r37;
	mov.b32 	%f132, %r59;
	add.f32 	%f133, %f131, %f132;
	mov.b32 	%r60, %f133;
	shfl.sync.idx.b32 	%r62|%p30, %r60, %r15, %r42, %r37;
	mov.b32 	%f134, %r62;
	add.f32 	%f20, %f17, %f134;
	@%p17 bra 	$L__BB45_7;

	mov.b32 	%r63, %f20;
	cvt.u64.u32 	%rd22, %r63;
	shl.b64 	%rd23, %rd22, 32;
	or.b64  	%rd21, %rd23, 2;
	// begin inline asm
	st.cg.u64 [%rd2], %rd21;
	// end inline asm

$L__BB45_7:
	sub.f32 	%f135, %f20, %f16;
	add.f32 	%f136, %f135, %f4;
	add.f32 	%f137, %f135, %f3;
	add.f32 	%f138, %f135, %f2;
	add.f32 	%f139, %f135, %f1;
	st.shared.v4.f32 	[%r3], {%f139, %f138, %f137, %f136};
	add.f32 	%f140, %f135, %f8;
	add.f32 	%f141, %f135, %f7;
	add.f32 	%f142, %f135, %f6;
	add.f32 	%f143, %f135, %f5;
	st.shared.v4.f32 	[%r3+16], {%f143, %f142, %f141, %f140};
	add.f32 	%f144, %f135, %f12;
	add.f32 	%f145, %f135, %f11;
	add.f32 	%f146, %f135, %f10;
	add.f32 	%f147, %f135, %f9;
	st.shared.v4.f32 	[%r3+32], {%f147, %f146, %f145, %f144};
	add.f32 	%f148, %f135, %f16;
	add.f32 	%f149, %f135, %f15;
	add.f32 	%f150, %f135, %f14;
	add.f32 	%f151, %f135, %f13;
	st.shared.v4.f32 	[%r3+48], {%f151, %f150, %f149, %f148};
	bar.sync 	0;
	shl.b64 	%rd24, %rd1, 4;
	add.s64 	%rd25, %rd3, %rd24;
	ld.shared.v4.u32 	{%r64, %r65, %r66, %r67}, [%r2];
	st.global.v4.u32 	[%rd25], {%r64, %r65, %r66, %r67};
	ld.shared.v4.u32 	{%r72, %r73, %r74, %r75}, [%r2+2048];
	st.global.v4.u32 	[%rd25+2048], {%r72, %r73, %r74, %r75};
	ld.shared.v4.u32 	{%r80, %r81, %r82, %r83}, [%r2+4096];
	st.global.v4.u32 	[%rd25+4096], {%r80, %r81, %r82, %r83};
	ld.shared.v4.u32 	{%r88, %r89, %r90, %r91}, [%r2+6144];
	st.global.v4.u32 	[%rd25+6144], {%r88, %r89, %r90, %r91};
	ret;

}
	// .globl	prefix_sum_inc_large_f64
.visible .entry prefix_sum_inc_large_f64(
	.param .u64 prefix_sum_inc_large_f64_param_0,
	.param .u64 prefix_sum_inc_large_f64_param_1,
	.param .u32 prefix_sum_inc_large_f64_param_2,
	.param .u64 prefix_sum_inc_large_f64_param_3
)
{
	.reg .pred 	%p<29>;
	.reg .b32 	%r<110>;
	.reg .f64 	%fd<97>;
	.reg .b64 	%rd<33>;


	ld.param.u64 	%rd6, [prefix_sum_inc_large_f64_param_0];
	ld.param.u32 	%r10, [prefix_sum_inc_large_f64_param_2];
	ld.param.u64 	%rd7, [prefix_sum_inc_large_f64_param_3];
	cvta.to.global.u64 	%rd8, %rd6;
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r12, %r11, 9;
	mov.u32 	%r1, %tid.x;
	add.s32 	%r13, %r12, %r1;
	mul.wide.u32 	%rd9, %r13, 16;
	add.s64 	%rd10, %rd8, %rd9;
	ld.global.v2.f64 	{%fd13, %fd14}, [%rd10];
	mov.u64 	%rd11, 0;
	shl.b32 	%r14, %r13, 1;
	setp.lt.u32 	%p1, %r14, %r10;
	or.b32  	%r15, %r14, 1;
	setp.lt.u32 	%p2, %r15, %r10;
	ld.global.v2.f64 	{%fd17, %fd18}, [%rd10+2048];
	add.s32 	%r16, %r14, 256;
	setp.lt.u32 	%p3, %r16, %r10;
	or.b32  	%r17, %r16, 1;
	setp.lt.u32 	%p4, %r17, %r10;
	ld.global.v2.f64 	{%fd21, %fd22}, [%rd10+4096];
	add.s32 	%r18, %r14, 512;
	setp.lt.u32 	%p5, %r18, %r10;
	or.b32  	%r19, %r18, 1;
	setp.lt.u32 	%p6, %r19, %r10;
	ld.global.v2.f64 	{%fd25, %fd26}, [%rd10+6144];
	add.s32 	%r20, %r14, 768;
	setp.lt.u32 	%p7, %r20, %r10;
	or.b32  	%r21, %r20, 1;
	setp.lt.u32 	%p8, %r21, %r10;
	shl.b32 	%r22, %r1, 4;
	mov.u32 	%r23, shared_d;
	add.s32 	%r2, %r23, %r22;
	selp.f64 	%fd29, %fd14, 0d0000000000000000, %p2;
	selp.f64 	%fd30, %fd13, 0d0000000000000000, %p1;
	st.shared.v2.f64 	[%r2], {%fd30, %fd29};
	selp.f64 	%fd31, %fd18, 0d0000000000000000, %p4;
	selp.f64 	%fd32, %fd17, 0d0000000000000000, %p3;
	st.shared.v2.f64 	[%r2+2048], {%fd32, %fd31};
	selp.f64 	%fd33, %fd22, 0d0000000000000000, %p6;
	selp.f64 	%fd34, %fd21, 0d0000000000000000, %p5;
	st.shared.v2.f64 	[%r2+4096], {%fd34, %fd33};
	selp.f64 	%fd35, %fd26, 0d0000000000000000, %p8;
	selp.f64 	%fd36, %fd25, 0d0000000000000000, %p7;
	st.shared.v2.f64 	[%r2+6144], {%fd36, %fd35};
	bar.sync 	0;
	shl.b32 	%r24, %r1, 6;
	add.s32 	%r3, %r23, %r24;
	ld.shared.v2.f64 	{%fd37, %fd38}, [%r3];
	ld.shared.v2.f64 	{%fd41, %fd42}, [%r3+16];
	ld.shared.v2.f64 	{%fd45, %fd46}, [%r3+32];
	ld.shared.v2.f64 	{%fd49, %fd50}, [%r3+48];
	add.f64 	%fd1, %fd37, 0d0000000000000000;
	add.f64 	%fd2, %fd1, %fd38;
	add.f64 	%fd3, %fd2, %fd41;
	add.f64 	%fd4, %fd3, %fd42;
	add.f64 	%fd5, %fd4, %fd45;
	add.f64 	%fd6, %fd5, %fd46;
	add.f64 	%fd7, %fd6, %fd49;
	add.f64 	%fd8, %fd7, %fd50;
	bar.sync 	0;
	shl.b32 	%r25, %r1, 3;
	add.s32 	%r26, %r23, %r25;
	st.shared.u64 	[%r26], %rd11;
	st.shared.f64 	[%r26+1024], %fd8;
	bar.sync 	0;
	ld.shared.f64 	%fd53, [%r26+1016];
	ld.shared.f64 	%fd54, [%r26+1024];
	add.f64 	%fd55, %fd54, %fd53;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd55;
	bar.sync 	0;
	ld.shared.f64 	%fd56, [%r26+1008];
	ld.shared.f64 	%fd57, [%r26+1024];
	add.f64 	%fd58, %fd57, %fd56;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd58;
	bar.sync 	0;
	ld.shared.f64 	%fd59, [%r26+992];
	ld.shared.f64 	%fd60, [%r26+1024];
	add.f64 	%fd61, %fd60, %fd59;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd61;
	bar.sync 	0;
	ld.shared.f64 	%fd62, [%r26+960];
	ld.shared.f64 	%fd63, [%r26+1024];
	add.f64 	%fd64, %fd63, %fd62;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd64;
	bar.sync 	0;
	ld.shared.f64 	%fd65, [%r26+896];
	ld.shared.f64 	%fd66, [%r26+1024];
	add.f64 	%fd67, %fd66, %fd65;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd67;
	bar.sync 	0;
	ld.shared.f64 	%fd68, [%r26+768];
	ld.shared.f64 	%fd69, [%r26+1024];
	add.f64 	%fd70, %fd69, %fd68;
	bar.sync 	0;
	st.shared.f64 	[%r26+1024], %fd70;
	bar.sync 	0;
	ld.shared.f64 	%fd71, [%r26+512];
	ld.shared.f64 	%fd72, [%r26+1024];
	add.f64 	%fd9, %fd72, %fd71;
	bar.sync 	0;
	mul.wide.u32 	%rd12, %r11, 8;
	add.s64 	%rd2, %rd7, %rd12;
	setp.ne.s32 	%p9, %r1, 127;
	@%p9 bra 	$L__BB46_2;

	mov.b64 	%rd15, %fd9;
	and.b64  	%rd16, %rd15, -4;
	or.b64  	%rd14, %rd16, 1;
	// begin inline asm
	st.cg.u64 [%rd2], %rd14;
	// end inline asm

$L__BB46_2:
	and.b32  	%r4, %r1, 31;
	or.b32  	%r109, %r4, -32;
	mov.f64 	%fd96, 0d0000000000000000;
	mov.u32 	%r28, -1;
	bra.uni 	$L__BB46_3;

$L__BB46_8:
	add.s32 	%r109, %r109, -32;
	mov.f64 	%fd96, %fd11;

$L__BB46_3:
	mul.wide.s32 	%rd19, %r109, 8;
	add.s64 	%rd18, %rd2, %rd19;
	// begin inline asm
	ld.cg.u64 %rd17, [%rd18];
	// end inline asm
	cvt.u32.u64 	%r27, %rd17;
	and.b32  	%r7, %r27, 3;
	setp.eq.s32 	%p10, %r7, 0;
	vote.sync.any.pred 	%p11, %p10, %r28;
	@%p11 bra 	$L__BB46_3;

	and.b64  	%rd20, %rd17, -4;
	mov.b64 	%fd74, %rd20;
	setp.eq.s32 	%p12, %r7, 2;
	vote.sync.ballot.b32 	%r8, %p12, %r28;
	setp.eq.s32 	%p14, %r8, 0;
	add.f64 	%fd11, %fd96, %fd74;
	@%p14 bra 	$L__BB46_8;

	clz.b32 	%r55, %r8;
	mov.u32 	%r56, 31;
	sub.s32 	%r57, %r56, %r55;
	setp.lt.u32 	%p16, %r4, %r57;
	selp.f64 	%fd75, %fd96, %fd11, %p16;
	// begin inline asm
	mov.b64 {%r31,%r32}, %fd75;
	// end inline asm
	mov.u32 	%r58, 2;
	mov.u32 	%r59, 16;
	shfl.sync.down.b32 	%r34|%p17, %r32, %r59, %r56, %r28;
	shfl.sync.down.b32 	%r33|%p18, %r31, %r59, %r56, %r28;
	// begin inline asm
	mov.b64 %fd76, {%r33,%r34};
	// end inline asm
	add.f64 	%fd77, %fd75, %fd76;
	// begin inline asm
	mov.b64 {%r35,%r36}, %fd77;
	// end inline asm
	mov.u32 	%r61, 8;
	shfl.sync.down.b32 	%r38|%p19, %r36, %r61, %r56, %r28;
	shfl.sync.down.b32 	%r37|%p20, %r35, %r61, %r56, %r28;
	// begin inline asm
	mov.b64 %fd78, {%r37,%r38};
	// end inline asm
	add.f64 	%fd79, %fd77, %fd78;
	// begin inline asm
	mov.b64 {%r39,%r40}, %fd79;
	// end inline asm
	mov.u32 	%r62, 4;
	shfl.sync.down.b32 	%r42|%p21, %r40, %r62, %r56, %r28;
	shfl.sync.down.b32 	%r41|%p22, %r39, %r62, %r56, %r28;
	// begin inline asm
	mov.b64 %fd80, {%r41,%r42};
	// end inline asm
	add.f64 	%fd81, %fd79, %fd80;
	// begin inline asm
	mov.b64 {%r43,%r44}, %fd81;
	// end inline asm
	shfl.sync.down.b32 	%r46|%p23, %r44, %r58, %r56, %r28;
	shfl.sync.down.b32 	%r45|%p24, %r43, %r58, %r56, %r28;
	// begin inline asm
	mov.b64 %fd82, {%r45,%r46};
	// end inline asm
	add.f64 	%fd83, %fd81, %fd82;
	// begin inline asm
	mov.b64 {%r47,%r48}, %fd83;
	// end inline asm
	mov.u32 	%r63, 1;
	shfl.sync.down.b32 	%r50|%p25, %r48, %r63, %r56, %r28;
	shfl.sync.down.b32 	%r49|%p26, %r47, %r63, %r56, %r28;
	// begin inline asm
	mov.b64 %fd84, {%r49,%r50};
	// end inline asm
	add.f64 	%fd85, %fd83, %fd84;
	// begin inline asm
	mov.b64 {%r51,%r52}, %fd85;
	// end inline asm
	mov.u32 	%r64, 0;
	shfl.sync.idx.b32 	%r54|%p27, %r52, %r64, %r56, %r28;
	shfl.sync.idx.b32 	%r53|%p28, %r51, %r64, %r56, %r28;
	// begin inline asm
	mov.b64 %fd86, {%r53,%r54};
	// end inline asm
	add.f64 	%fd12, %fd9, %fd86;
	@%p9 bra 	$L__BB46_7;

	mov.u32 	%r104, %ctaid.x;
	mul.wide.u32 	%rd30, %r104, 8;
	ld.param.u64 	%rd29, [prefix_sum_inc_large_f64_param_3];
	add.s64 	%rd28, %rd29, %rd30;
	mov.b64 	%rd23, %fd12;
	and.b64  	%rd24, %rd23, -4;
	or.b64  	%rd22, %rd24, 2;
	// begin inline asm
	st.cg.u64 [%rd28], %rd22;
	// end inline asm

$L__BB46_7:
	mov.u32 	%r108, %tid.x;
	shl.b32 	%r107, %r108, 4;
	mov.u32 	%r106, shared_d;
	add.s32 	%r105, %r106, %r107;
	ld.param.u64 	%rd32, [prefix_sum_inc_large_f64_param_1];
	cvta.to.global.u64 	%rd31, %rd32;
	mov.u32 	%r103, %tid.x;
	mov.u32 	%r102, %ctaid.x;
	shl.b32 	%r101, %r102, 9;
	add.s32 	%r100, %r101, %r103;
	cvt.u64.u32 	%rd27, %r100;
	shl.b32 	%r99, %r103, 6;
	mov.u32 	%r98, shared_d;
	add.s32 	%r97, %r98, %r99;
	sub.f64 	%fd87, %fd12, %fd8;
	add.f64 	%fd88, %fd87, %fd2;
	add.f64 	%fd89, %fd87, %fd1;
	st.shared.v2.f64 	[%r97], {%fd89, %fd88};
	add.f64 	%fd90, %fd87, %fd4;
	add.f64 	%fd91, %fd87, %fd3;
	st.shared.v2.f64 	[%r97+16], {%fd91, %fd90};
	add.f64 	%fd92, %fd87, %fd6;
	add.f64 	%fd93, %fd87, %fd5;
	st.shared.v2.f64 	[%r97+32], {%fd93, %fd92};
	add.f64 	%fd94, %fd87, %fd8;
	add.f64 	%fd95, %fd87, %fd7;
	st.shared.v2.f64 	[%r97+48], {%fd95, %fd94};
	bar.sync 	0;
	shl.b64 	%rd25, %rd27, 4;
	add.s64 	%rd26, %rd31, %rd25;
	ld.shared.v4.u32 	{%r65, %r66, %r67, %r68}, [%r105];
	st.global.v4.u32 	[%rd26], {%r65, %r66, %r67, %r68};
	ld.shared.v4.u32 	{%r73, %r74, %r75, %r76}, [%r105+2048];
	st.global.v4.u32 	[%rd26+2048], {%r73, %r74, %r75, %r76};
	ld.shared.v4.u32 	{%r81, %r82, %r83, %r84}, [%r105+4096];
	st.global.v4.u32 	[%rd26+4096], {%r81, %r82, %r83, %r84};
	ld.shared.v4.u32 	{%r89, %r90, %r91, %r92}, [%r105+6144];
	st.global.v4.u32 	[%rd26+6144], {%r89, %r90, %r91, %r92};
	ret;

}
	// .globl	compress_small
.visible .entry compress_small(
	.param .u64 compress_small_param_0,
	.param .u64 compress_small_param_1,
	.param .u32 compress_small_param_2,
	.param .u64 compress_small_param_3
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<44>;
	.reg .b64 	%rd<17>;


	ld.param.u64 	%rd3, [compress_small_param_0];
	ld.param.u64 	%rd4, [compress_small_param_1];
	ld.param.u64 	%rd2, [compress_small_param_3];
	cvta.to.global.u64 	%rd1, %rd4;
	cvta.to.global.u64 	%rd5, %rd3;
	mov.u32 	%r1, %tid.x;
	mul.wide.u32 	%rd6, %r1, 4;
	add.s64 	%rd7, %rd5, %rd6;
	ld.global.u32 	%r18, [%rd7];
	and.b32  	%r2, %r18, 255;
	shr.u32 	%r19, %r18, 8;
	and.b32  	%r20, %r19, 255;
	add.s32 	%r3, %r2, %r20;
	shr.u32 	%r21, %r18, 16;
	and.b32  	%r22, %r21, 255;
	add.s32 	%r4, %r3, %r22;
	shr.u32 	%r23, %r18, 24;
	add.s32 	%r5, %r4, %r23;
	shl.b32 	%r24, %r1, 2;
	mov.u32 	%r25, shared;
	add.s32 	%r26, %r25, %r24;
	mov.u32 	%r27, 0;
	st.shared.u32 	[%r26], %r27;
	mov.u32 	%r6, %ntid.x;
	shl.b32 	%r28, %r6, 2;
	add.s32 	%r7, %r26, %r28;
	st.shared.u32 	[%r7], %r5;
	setp.lt.u32 	%p1, %r6, 2;
	mov.u32 	%r43, %r5;
	@%p1 bra 	$L__BB47_3;

	add.s32 	%r8, %r1, %r6;
	mov.u32 	%r42, 1;

$L__BB47_2:
	bar.sync 	0;
	sub.s32 	%r30, %r8, %r42;
	shl.b32 	%r31, %r30, 2;
	add.s32 	%r33, %r25, %r31;
	ld.shared.u32 	%r34, [%r33];
	ld.shared.u32 	%r35, [%r7];
	add.s32 	%r43, %r34, %r35;
	bar.sync 	0;
	st.shared.u32 	[%r7], %r43;
	shl.b32 	%r42, %r42, 1;
	setp.lt.u32 	%p2, %r42, %r6;
	@%p2 bra 	$L__BB47_2;

$L__BB47_3:
	add.s32 	%r37, %r6, -1;
	setp.ne.s32 	%p3, %r1, %r37;
	@%p3 bra 	$L__BB47_5;

	cvta.to.global.u64 	%rd8, %rd2;
	st.global.u32 	[%rd8], %r43;

$L__BB47_5:
	sub.s32 	%r13, %r43, %r5;
	add.s32 	%r14, %r13, %r2;
	add.s32 	%r15, %r13, %r3;
	add.s32 	%r16, %r13, %r4;
	setp.eq.s32 	%p4, %r2, 0;
	@%p4 bra 	$L__BB47_7;

	mul.wide.u32 	%rd9, %r13, 4;
	add.s64 	%rd10, %rd1, %rd9;
	st.global.u32 	[%rd10], %r24;

$L__BB47_7:
	setp.eq.s32 	%p5, %r14, %r15;
	@%p5 bra 	$L__BB47_9;

	add.s32 	%r39, %r24, 1;
	mul.wide.u32 	%rd11, %r14, 4;
	add.s64 	%rd12, %rd1, %rd11;
	st.global.u32 	[%rd12], %r39;

$L__BB47_9:
	setp.eq.s32 	%p6, %r15, %r16;
	@%p6 bra 	$L__BB47_11;

	add.s32 	%r40, %r24, 2;
	mul.wide.u32 	%rd13, %r15, 4;
	add.s64 	%rd14, %rd1, %rd13;
	st.global.u32 	[%rd14], %r40;

$L__BB47_11:
	setp.eq.s32 	%p7, %r16, %r43;
	@%p7 bra 	$L__BB47_13;

	add.s32 	%r41, %r24, 3;
	mul.wide.u32 	%rd15, %r16, 4;
	add.s64 	%rd16, %rd1, %rd15;
	st.global.u32 	[%rd16], %r41;

$L__BB47_13:
	ret;

}
	// .globl	compress_large
.visible .entry compress_large(
	.param .u64 compress_large_param_0,
	.param .u64 compress_large_param_1,
	.param .u64 compress_large_param_2,
	.param .u64 compress_large_param_3
)
{
	.reg .pred 	%p<32>;
	.reg .b32 	%r<152>;
	.reg .b64 	%rd<57>;


	ld.param.u64 	%rd6, [compress_large_param_0];
	ld.param.u64 	%rd7, [compress_large_param_1];
	ld.param.u64 	%rd8, [compress_large_param_2];
	ld.param.u64 	%rd5, [compress_large_param_3];
	cvta.to.global.u64 	%rd1, %rd7;
	cvta.to.global.u64 	%rd9, %rd6;
	mov.u32 	%r1, %ctaid.x;
	shl.b32 	%r49, %r1, 7;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r49, %r2;
	mul.wide.u32 	%rd10, %r3, 16;
	add.s64 	%rd11, %rd9, %rd10;
	ld.global.v4.u32 	{%r50, %r51, %r52, %r53}, [%rd11];
	mov.u32 	%r150, 0;
	and.b32  	%r4, %r50, 255;
	shr.u32 	%r59, %r50, 8;
	and.b32  	%r60, %r59, 255;
	add.s32 	%r5, %r4, %r60;
	shr.u32 	%r61, %r50, 16;
	and.b32  	%r62, %r61, 255;
	add.s32 	%r6, %r5, %r62;
	shr.u32 	%r63, %r50, 24;
	add.s32 	%r7, %r6, %r63;
	and.b32  	%r64, %r51, 255;
	add.s32 	%r8, %r7, %r64;
	shr.u32 	%r65, %r51, 8;
	and.b32  	%r66, %r65, 255;
	add.s32 	%r9, %r8, %r66;
	shr.u32 	%r67, %r51, 16;
	and.b32  	%r68, %r67, 255;
	add.s32 	%r10, %r9, %r68;
	shr.u32 	%r69, %r51, 24;
	add.s32 	%r11, %r10, %r69;
	and.b32  	%r70, %r52, 255;
	add.s32 	%r12, %r11, %r70;
	shr.u32 	%r71, %r52, 8;
	and.b32  	%r72, %r71, 255;
	add.s32 	%r13, %r12, %r72;
	shr.u32 	%r73, %r52, 16;
	and.b32  	%r74, %r73, 255;
	add.s32 	%r14, %r13, %r74;
	shr.u32 	%r75, %r52, 24;
	add.s32 	%r15, %r14, %r75;
	and.b32  	%r76, %r53, 255;
	add.s32 	%r16, %r15, %r76;
	shr.u32 	%r77, %r53, 8;
	and.b32  	%r78, %r77, 255;
	add.s32 	%r17, %r16, %r78;
	shr.u32 	%r79, %r53, 16;
	and.b32  	%r80, %r79, 255;
	add.s32 	%r18, %r17, %r80;
	shr.u32 	%r81, %r53, 24;
	add.s32 	%r19, %r18, %r81;
	shl.b32 	%r82, %r2, 2;
	mov.u32 	%r83, shared;
	add.s32 	%r84, %r83, %r82;
	st.shared.u32 	[%r84], %r150;
	st.shared.u32 	[%r84+512], %r19;
	bar.sync 	0;
	ld.shared.u32 	%r85, [%r84+508];
	ld.shared.u32 	%r86, [%r84+512];
	add.s32 	%r87, %r85, %r86;
	bar.sync 	0;
	st.shared.u32 	[%r84+512], %r87;
	bar.sync 	0;
	ld.shared.u32 	%r88, [%r84+504];
	ld.shared.u32 	%r89, [%r84+512];
	add.s32 	%r90, %r88, %r89;
	bar.sync 	0;
	st.shared.u32 	[%r84+512], %r90;
	bar.sync 	0;
	ld.shared.u32 	%r91, [%r84+496];
	ld.shared.u32 	%r92, [%r84+512];
	add.s32 	%r93, %r91, %r92;
	bar.sync 	0;
	st.shared.u32 	[%r84+512], %r93;
	bar.sync 	0;
	ld.shared.u32 	%r94, [%r84+480];
	ld.shared.u32 	%r95, [%r84+512];
	add.s32 	%r96, %r94, %r95;
	bar.sync 	0;
	st.shared.u32 	[%r84+512], %r96;
	bar.sync 	0;
	ld.shared.u32 	%r97, [%r84+448];
	ld.shared.u32 	%r98, [%r84+512];
	add.s32 	%r99, %r97, %r98;
	bar.sync 	0;
	st.shared.u32 	[%r84+512], %r99;
	bar.sync 	0;
	ld.shared.u32 	%r100, [%r84+384];
	ld.shared.u32 	%r101, [%r84+512];
	add.s32 	%r102, %r100, %r101;
	bar.sync 	0;
	st.shared.u32 	[%r84+512], %r102;
	bar.sync 	0;
	ld.shared.u32 	%r103, [%r84+256];
	ld.shared.u32 	%r104, [%r84+512];
	add.s32 	%r20, %r103, %r104;
	bar.sync 	0;
	st.shared.u32 	[%r84+512], %r20;
	mul.wide.u32 	%rd12, %r1, 8;
	add.s64 	%rd2, %rd8, %rd12;
	setp.ne.s32 	%p1, %r2, 127;
	@%p1 bra 	$L__BB48_2;

	cvt.u64.u32 	%rd15, %r20;
	shl.b64 	%rd16, %rd15, 32;
	or.b64  	%rd14, %rd16, 1;
	// begin inline asm
	st.cg.u64 [%rd2], %rd14;
	// end inline asm

$L__BB48_2:
	mov.u32 	%r21, WARP_SZ;
	add.s32 	%r106, %r21, -1;
	and.b32  	%r22, %r106, %r2;
	sub.s32 	%r151, %r22, %r21;
	cvta.to.global.u64 	%rd3, %rd5;
	bra.uni 	$L__BB48_3;

$L__BB48_41:
	add.s32 	%r150, %r150, %r28;
	sub.s32 	%r151, %r151, %r21;

$L__BB48_3:
	mul.wide.s32 	%rd19, %r151, 8;
	add.s64 	%rd18, %rd2, %rd19;
	// begin inline asm
	ld.cg.u64 %rd17, [%rd18];
	// end inline asm
	cvt.u32.u64 	%r26, %rd17;
	setp.eq.s32 	%p2, %r26, 0;
	mov.u32 	%r107, -1;
	vote.sync.any.pred 	%p3, %p2, %r107;
	@%p3 bra 	$L__BB48_3;

	setp.eq.s32 	%p4, %r26, 2;
	vote.sync.ballot.b32 	%r27, %p4, %r107;
	setp.eq.s32 	%p6, %r27, 0;
	shr.u64 	%rd20, %rd17, 32;
	cvt.u32.u64 	%r28, %rd20;
	@%p6 bra 	$L__BB48_41;

	clz.b32 	%r110, %r27;
	mov.u32 	%r111, 31;
	sub.s32 	%r112, %r111, %r110;
	setp.lt.u32 	%p8, %r22, %r112;
	selp.b32 	%r113, 0, %r28, %p8;
	mov.u32 	%r114, 0;
	add.s32 	%r115, %r113, %r150;
	mov.u32 	%r116, 2;
	mov.u32 	%r117, 16;
	shfl.sync.down.b32 	%r119|%p9, %r115, %r117, %r111, %r107;
	add.s32 	%r120, %r119, %r115;
	mov.u32 	%r121, 8;
	shfl.sync.down.b32 	%r122|%p10, %r120, %r121, %r111, %r107;
	add.s32 	%r123, %r122, %r120;
	mov.u32 	%r124, 4;
	shfl.sync.down.b32 	%r125|%p11, %r123, %r124, %r111, %r107;
	add.s32 	%r126, %r125, %r123;
	shfl.sync.down.b32 	%r127|%p12, %r126, %r116, %r111, %r107;
	add.s32 	%r128, %r127, %r126;
	mov.u32 	%r129, 1;
	shfl.sync.down.b32 	%r130|%p13, %r128, %r129, %r111, %r107;
	add.s32 	%r131, %r130, %r128;
	shfl.sync.idx.b32 	%r132|%p14, %r131, %r114, %r111, %r107;
	add.s32 	%r29, %r132, %r20;
	@%p1 bra 	$L__BB48_8;

	cvt.u64.u32 	%rd23, %r29;
	shl.b64 	%rd24, %rd23, 32;
	or.b64  	%rd22, %rd24, 2;
	// begin inline asm
	st.cg.u64 [%rd2], %rd22;
	// end inline asm
	mov.u32 	%r133, %nctaid.x;
	add.s32 	%r134, %r133, -1;
	setp.ne.s32 	%p15, %r1, %r134;
	@%p15 bra 	$L__BB48_8;

	st.global.u32 	[%rd3], %r29;

$L__BB48_8:
	sub.s32 	%r30, %r29, %r19;
	add.s32 	%r31, %r30, %r4;
	add.s32 	%r32, %r30, %r5;
	add.s32 	%r33, %r30, %r6;
	add.s32 	%r34, %r30, %r7;
	add.s32 	%r35, %r30, %r8;
	add.s32 	%r36, %r30, %r9;
	add.s32 	%r37, %r30, %r10;
	add.s32 	%r38, %r30, %r11;
	add.s32 	%r39, %r30, %r12;
	add.s32 	%r40, %r30, %r13;
	add.s32 	%r41, %r30, %r14;
	add.s32 	%r42, %r30, %r15;
	add.s32 	%r43, %r30, %r16;
	add.s32 	%r44, %r30, %r17;
	add.s32 	%r45, %r30, %r18;
	shl.b32 	%r46, %r3, 4;
	setp.eq.s32 	%p16, %r4, 0;
	@%p16 bra 	$L__BB48_10;

	mul.wide.u32 	%rd25, %r30, 4;
	add.s64 	%rd26, %rd1, %rd25;
	st.global.u32 	[%rd26], %r46;

$L__BB48_10:
	setp.eq.s32 	%p17, %r31, %r32;
	@%p17 bra 	$L__BB48_12;

	add.s32 	%r135, %r46, 1;
	mul.wide.u32 	%rd27, %r31, 4;
	add.s64 	%rd28, %rd1, %rd27;
	st.global.u32 	[%rd28], %r135;

$L__BB48_12:
	setp.eq.s32 	%p18, %r32, %r33;
	@%p18 bra 	$L__BB48_14;

	add.s32 	%r136, %r46, 2;
	mul.wide.u32 	%rd29, %r32, 4;
	add.s64 	%rd30, %rd1, %rd29;
	st.global.u32 	[%rd30], %r136;

$L__BB48_14:
	setp.eq.s32 	%p19, %r33, %r34;
	@%p19 bra 	$L__BB48_16;

	add.s32 	%r137, %r46, 3;
	mul.wide.u32 	%rd31, %r33, 4;
	add.s64 	%rd32, %rd1, %rd31;
	st.global.u32 	[%rd32], %r137;

$L__BB48_16:
	setp.eq.s32 	%p20, %r34, %r35;
	@%p20 bra 	$L__BB48_18;

	add.s32 	%r138, %r46, 4;
	mul.wide.u32 	%rd33, %r34, 4;
	add.s64 	%rd34, %rd1, %rd33;
	st.global.u32 	[%rd34], %r138;

$L__BB48_18:
	setp.eq.s32 	%p21, %r35, %r36;
	@%p21 bra 	$L__BB48_20;

	add.s32 	%r139, %r46, 5;
	mul.wide.u32 	%rd35, %r35, 4;
	add.s64 	%rd36, %rd1, %rd35;
	st.global.u32 	[%rd36], %r139;

$L__BB48_20:
	setp.eq.s32 	%p22, %r36, %r37;
	@%p22 bra 	$L__BB48_22;

	add.s32 	%r140, %r46, 6;
	mul.wide.u32 	%rd37, %r36, 4;
	add.s64 	%rd38, %rd1, %rd37;
	st.global.u32 	[%rd38], %r140;

$L__BB48_22:
	setp.eq.s32 	%p23, %r37, %r38;
	@%p23 bra 	$L__BB48_24;

	add.s32 	%r141, %r46, 7;
	mul.wide.u32 	%rd39, %r37, 4;
	add.s64 	%rd40, %rd1, %rd39;
	st.global.u32 	[%rd40], %r141;

$L__BB48_24:
	setp.eq.s32 	%p24, %r38, %r39;
	@%p24 bra 	$L__BB48_26;

	add.s32 	%r142, %r46, 8;
	mul.wide.u32 	%rd41, %r38, 4;
	add.s64 	%rd42, %rd1, %rd41;
	st.global.u32 	[%rd42], %r142;

$L__BB48_26:
	setp.eq.s32 	%p25, %r39, %r40;
	@%p25 bra 	$L__BB48_28;

	add.s32 	%r143, %r46, 9;
	mul.wide.u32 	%rd43, %r39, 4;
	add.s64 	%rd44, %rd1, %rd43;
	st.global.u32 	[%rd44], %r143;

$L__BB48_28:
	setp.eq.s32 	%p26, %r40, %r41;
	@%p26 bra 	$L__BB48_30;

	add.s32 	%r144, %r46, 10;
	mul.wide.u32 	%rd45, %r40, 4;
	add.s64 	%rd46, %rd1, %rd45;
	st.global.u32 	[%rd46], %r144;

$L__BB48_30:
	setp.eq.s32 	%p27, %r41, %r42;
	@%p27 bra 	$L__BB48_32;

	add.s32 	%r145, %r46, 11;
	mul.wide.u32 	%rd47, %r41, 4;
	add.s64 	%rd48, %rd1, %rd47;
	st.global.u32 	[%rd48], %r145;

$L__BB48_32:
	setp.eq.s32 	%p28, %r42, %r43;
	@%p28 bra 	$L__BB48_34;

	add.s32 	%r146, %r46, 12;
	mul.wide.u32 	%rd49, %r42, 4;
	add.s64 	%rd50, %rd1, %rd49;
	st.global.u32 	[%rd50], %r146;

$L__BB48_34:
	setp.eq.s32 	%p29, %r43, %r44;
	@%p29 bra 	$L__BB48_36;

	add.s32 	%r147, %r46, 13;
	mul.wide.u32 	%rd51, %r43, 4;
	add.s64 	%rd52, %rd1, %rd51;
	st.global.u32 	[%rd52], %r147;

$L__BB48_36:
	setp.eq.s32 	%p30, %r44, %r45;
	@%p30 bra 	$L__BB48_38;

	add.s32 	%r148, %r46, 14;
	mul.wide.u32 	%rd53, %r44, 4;
	add.s64 	%rd54, %rd1, %rd53;
	st.global.u32 	[%rd54], %r148;

$L__BB48_38:
	setp.eq.s32 	%p31, %r45, %r29;
	@%p31 bra 	$L__BB48_40;

	add.s32 	%r149, %r46, 15;
	mul.wide.u32 	%rd55, %r45, 4;
	add.s64 	%rd56, %rd1, %rd55;
	st.global.u32 	[%rd56], %r149;

$L__BB48_40:
	ret;

}
	// .globl	mkperm_phase_1_tiny
.visible .entry mkperm_phase_1_tiny(
	.param .u64 mkperm_phase_1_tiny_param_0,
	.param .u64 mkperm_phase_1_tiny_param_1,
	.param .u32 mkperm_phase_1_tiny_param_2,
	.param .u32 mkperm_phase_1_tiny_param_3,
	.param .u32 mkperm_phase_1_tiny_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<53>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [mkperm_phase_1_tiny_param_0];
	ld.param.u64 	%rd5, [mkperm_phase_1_tiny_param_1];
	ld.param.u32 	%r23, [mkperm_phase_1_tiny_param_2];
	ld.param.u32 	%r25, [mkperm_phase_1_tiny_param_3];
	ld.param.u32 	%r24, [mkperm_phase_1_tiny_param_4];
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r2, %r1, %r25;
	add.s32 	%r3, %r2, %r25;
	mov.u32 	%r4, WARP_SZ;
	mov.u32 	%r5, %ntid.x;
	div.u32 	%r26, %r5, %r4;
	mov.u32 	%r52, %tid.x;
	mul.lo.s32 	%r7, %r26, %r24;
	setp.ge.u32 	%p2, %r52, %r7;
	@%p2 bra 	$L__BB49_3;

	mov.u32 	%r49, %r52;

$L__BB49_2:
	shl.b32 	%r27, %r49, 2;
	mov.u32 	%r28, shared;
	add.s32 	%r29, %r28, %r27;
	mov.u32 	%r30, 0;
	st.shared.u32 	[%r29], %r30;
	add.s32 	%r49, %r49, %r5;
	setp.lt.u32 	%p3, %r49, %r7;
	@%p3 bra 	$L__BB49_2;

$L__BB49_3:
	bar.sync 	0;
	add.s32 	%r50, %r2, %r52;
	setp.ge.u32 	%p4, %r50, %r3;
	@%p4 bra 	$L__BB49_10;

	add.s32 	%r31, %r4, -1;
	and.b32  	%r11, %r31, %r52;
	cvta.to.global.u64 	%rd1, %rd4;
	div.u32 	%r32, %r52, %r4;
	mul.lo.s32 	%r12, %r32, %r24;

$L__BB49_5:
	setp.ge.u32 	%p5, %r50, %r23;
	setp.lt.u32 	%p6, %r50, %r23;
	mov.u32 	%r33, -1;
	vote.sync.ballot.b32 	%r14, %p6, %r33;
	@%p5 bra 	$L__BB49_9;

	mul.wide.u32 	%rd6, %r50, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r15, [%rd7];
	match.any.sync.b32 	%r16, %r15, %r14;
	brev.b32 	%r35, %r16;
	bfind.shiftamt.u32 	%r17, %r35;
	setp.ne.s32 	%p7, %r11, %r17;
	mov.u32 	%r51, 0;
	@%p7 bra 	$L__BB49_8;

	add.s32 	%r36, %r15, %r12;
	shl.b32 	%r37, %r36, 2;
	mov.u32 	%r38, shared;
	add.s32 	%r39, %r38, %r37;
	popc.b32 	%r40, %r16;
	ld.shared.u32 	%r51, [%r39];
	add.s32 	%r41, %r51, %r40;
	st.shared.u32 	[%r39], %r41;

$L__BB49_8:
	mov.u32 	%r42, 31;
	shfl.sync.idx.b32 	%r43|%p8, %r51, %r17, %r42, %r16;

$L__BB49_9:
	bar.warp.sync 	-1;
	add.s32 	%r50, %r50, %r5;
	setp.lt.u32 	%p9, %r50, %r3;
	@%p9 bra 	$L__BB49_5;

$L__BB49_10:
	bar.sync 	0;
	@%p2 bra 	$L__BB49_13;

	cvta.to.global.u64 	%rd2, %rd5;
	mul.lo.s32 	%r44, %r7, %r1;
	cvt.u64.u32 	%rd3, %r44;
	mov.u32 	%r46, shared;

$L__BB49_12:
	shl.b32 	%r45, %r52, 2;
	add.s32 	%r47, %r46, %r45;
	ld.shared.u32 	%r48, [%r47];
	cvt.u64.u32 	%rd8, %r52;
	add.s64 	%rd9, %rd8, %rd3;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	st.global.u32 	[%rd11], %r48;
	add.s32 	%r52, %r52, %r5;
	setp.lt.u32 	%p11, %r52, %r7;
	@%p11 bra 	$L__BB49_12;

$L__BB49_13:
	ret;

}
	// .globl	mkperm_phase_1_small
.visible .entry mkperm_phase_1_small(
	.param .u64 mkperm_phase_1_small_param_0,
	.param .u64 mkperm_phase_1_small_param_1,
	.param .u32 mkperm_phase_1_small_param_2,
	.param .u32 mkperm_phase_1_small_param_3,
	.param .u32 mkperm_phase_1_small_param_4
)
{
	.reg .pred 	%p<12>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [mkperm_phase_1_small_param_0];
	ld.param.u64 	%rd5, [mkperm_phase_1_small_param_1];
	ld.param.u32 	%r20, [mkperm_phase_1_small_param_2];
	ld.param.u32 	%r22, [mkperm_phase_1_small_param_3];
	ld.param.u32 	%r21, [mkperm_phase_1_small_param_4];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r3, %r2, %r22;
	add.s32 	%r4, %r3, %r22;
	mov.u32 	%r46, %tid.x;
	setp.ge.u32 	%p2, %r46, %r21;
	@%p2 bra 	$L__BB50_3;

	mov.u32 	%r43, %r46;

$L__BB50_2:
	shl.b32 	%r23, %r43, 2;
	mov.u32 	%r24, shared;
	add.s32 	%r25, %r24, %r23;
	mov.u32 	%r26, 0;
	st.shared.u32 	[%r25], %r26;
	add.s32 	%r43, %r43, %r1;
	setp.lt.u32 	%p3, %r43, %r21;
	@%p3 bra 	$L__BB50_2;

$L__BB50_3:
	bar.sync 	0;
	add.s32 	%r44, %r3, %r46;
	setp.ge.u32 	%p4, %r44, %r4;
	@%p4 bra 	$L__BB50_10;

	mov.u32 	%r27, WARP_SZ;
	add.s32 	%r28, %r27, -1;
	and.b32  	%r9, %r28, %r46;
	cvta.to.global.u64 	%rd1, %rd4;

$L__BB50_5:
	setp.ge.u32 	%p5, %r44, %r20;
	setp.lt.u32 	%p6, %r44, %r20;
	mov.u32 	%r29, -1;
	vote.sync.ballot.b32 	%r11, %p6, %r29;
	@%p5 bra 	$L__BB50_9;

	mul.wide.u32 	%rd6, %r44, 4;
	add.s64 	%rd7, %rd1, %rd6;
	ld.global.u32 	%r12, [%rd7];
	match.any.sync.b32 	%r13, %r12, %r11;
	brev.b32 	%r31, %r13;
	bfind.shiftamt.u32 	%r14, %r31;
	setp.ne.s32 	%p7, %r9, %r14;
	mov.u32 	%r45, 0;
	@%p7 bra 	$L__BB50_8;

	shl.b32 	%r32, %r12, 2;
	mov.u32 	%r33, shared;
	add.s32 	%r34, %r33, %r32;
	popc.b32 	%r35, %r13;
	atom.shared.add.u32 	%r45, [%r34], %r35;

$L__BB50_8:
	mov.u32 	%r36, 31;
	shfl.sync.idx.b32 	%r37|%p8, %r45, %r14, %r36, %r13;

$L__BB50_9:
	add.s32 	%r44, %r44, %r1;
	setp.lt.u32 	%p9, %r44, %r4;
	@%p9 bra 	$L__BB50_5;

$L__BB50_10:
	bar.sync 	0;
	@%p2 bra 	$L__BB50_13;

	cvta.to.global.u64 	%rd2, %rd5;
	mul.lo.s32 	%r38, %r2, %r21;
	cvt.u64.u32 	%rd3, %r38;
	mov.u32 	%r40, shared;

$L__BB50_12:
	shl.b32 	%r39, %r46, 2;
	add.s32 	%r41, %r40, %r39;
	ld.shared.u32 	%r42, [%r41];
	cvt.u64.u32 	%rd8, %r46;
	add.s64 	%rd9, %rd8, %rd3;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	st.global.u32 	[%rd11], %r42;
	add.s32 	%r46, %r46, %r1;
	setp.lt.u32 	%p11, %r46, %r21;
	@%p11 bra 	$L__BB50_12;

$L__BB50_13:
	ret;

}
	// .globl	mkperm_phase_1_large
.visible .entry mkperm_phase_1_large(
	.param .u64 mkperm_phase_1_large_param_0,
	.param .u64 mkperm_phase_1_large_param_1,
	.param .u32 mkperm_phase_1_large_param_2,
	.param .u32 mkperm_phase_1_large_param_3,
	.param .u32 mkperm_phase_1_large_param_4
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<30>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [mkperm_phase_1_large_param_0];
	ld.param.u64 	%rd5, [mkperm_phase_1_large_param_1];
	ld.param.u32 	%r15, [mkperm_phase_1_large_param_2];
	ld.param.u32 	%r17, [mkperm_phase_1_large_param_3];
	ld.param.u32 	%r16, [mkperm_phase_1_large_param_4];
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r18, %r1, %r17;
	add.s32 	%r2, %r18, %r17;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r28, %r18, %r3;
	setp.ge.u32 	%p2, %r28, %r2;
	@%p2 bra 	$L__BB51_7;

	mov.u32 	%r19, WARP_SZ;
	add.s32 	%r20, %r19, -1;
	and.b32  	%r5, %r20, %r3;
	mul.lo.s32 	%r21, %r1, %r16;
	cvt.u64.u32 	%rd1, %r21;
	mov.u32 	%r6, %ntid.x;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;

$L__BB51_2:
	setp.ge.u32 	%p3, %r28, %r15;
	setp.lt.u32 	%p4, %r28, %r15;
	mov.u32 	%r22, -1;
	vote.sync.ballot.b32 	%r8, %p4, %r22;
	@%p3 bra 	$L__BB51_6;

	mul.wide.u32 	%rd6, %r28, 4;
	add.s64 	%rd7, %rd2, %rd6;
	ld.global.u32 	%r9, [%rd7];
	match.any.sync.b32 	%r10, %r9, %r8;
	brev.b32 	%r24, %r10;
	bfind.shiftamt.u32 	%r11, %r24;
	setp.ne.s32 	%p5, %r5, %r11;
	mov.u32 	%r29, 0;
	@%p5 bra 	$L__BB51_5;

	cvt.u64.u32 	%rd8, %r9;
	add.s64 	%rd9, %rd8, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd3, %rd10;
	popc.b32 	%r25, %r10;
	atom.global.add.u32 	%r29, [%rd11], %r25;

$L__BB51_5:
	mov.u32 	%r26, 31;
	shfl.sync.idx.b32 	%r27|%p6, %r29, %r11, %r26, %r10;

$L__BB51_6:
	add.s32 	%r28, %r28, %r6;
	setp.lt.u32 	%p7, %r28, %r2;
	@%p7 bra 	$L__BB51_2;

$L__BB51_7:
	ret;

}
	// .globl	mkperm_phase_3
.visible .entry mkperm_phase_3(
	.param .u64 mkperm_phase_3_param_0,
	.param .u32 mkperm_phase_3_param_1,
	.param .u32 mkperm_phase_3_param_2,
	.param .u32 mkperm_phase_3_param_3,
	.param .u64 mkperm_phase_3_param_4,
	.param .u64 mkperm_phase_3_param_5
)
{
	.reg .pred 	%p<11>;
	.reg .b32 	%r<47>;
	.reg .b64 	%rd<13>;


	ld.param.u64 	%rd6, [mkperm_phase_3_param_0];
	ld.param.u32 	%r22, [mkperm_phase_3_param_1];
	ld.param.u32 	%r23, [mkperm_phase_3_param_2];
	ld.param.u32 	%r24, [mkperm_phase_3_param_3];
	ld.param.u64 	%rd4, [mkperm_phase_3_param_4];
	ld.param.u64 	%rd5, [mkperm_phase_3_param_5];
	cvta.to.global.u64 	%rd1, %rd6;
	mov.u32 	%r25, WARP_SZ;
	add.s32 	%r26, %r25, -1;
	mov.u32 	%r1, %tid.x;
	and.b32  	%r2, %r26, %r1;
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r27, %ctaid.x;
	mad.lo.s32 	%r43, %r27, %r3, %r1;
	setp.ge.u32 	%p2, %r43, %r23;
	@%p2 bra 	$L__BB52_13;

	shl.b32 	%r28, %r1, 2;
	mov.u32 	%r29, shared;
	add.s32 	%r5, %r29, %r28;
	add.s32 	%r6, %r1, 1;
	mov.u32 	%r30, %nctaid.x;
	mul.lo.s32 	%r7, %r3, %r30;
	mov.u32 	%r31, 32;
	sub.s32 	%r8, %r31, %r2;
	cvta.to.global.u64 	%rd2, %rd4;
	cvta.to.global.u64 	%rd3, %rd5;
	setp.lt.u32 	%p4, %r6, %r3;

$L__BB52_2:
	setp.ge.u32 	%p3, %r43, %r22;
	mov.u32 	%r44, %r24;
	@%p3 bra 	$L__BB52_4;

	mul.wide.u32 	%rd7, %r43, 4;
	add.s64 	%rd8, %rd1, %rd7;
	ld.global.u32 	%r44, [%rd8];

$L__BB52_4:
	st.shared.u32 	[%r5], %r44;
	bar.sync 	0;
	@%p4 bra 	$L__BB52_7;
	bra.uni 	$L__BB52_5;

$L__BB52_7:
	ld.shared.u32 	%r45, [%r5+4];
	bra.uni 	$L__BB52_8;

$L__BB52_5:
	add.s32 	%r12, %r43, 1;
	setp.ge.u32 	%p5, %r12, %r22;
	mov.u32 	%r45, %r24;
	@%p5 bra 	$L__BB52_8;

	mul.wide.u32 	%rd9, %r12, 4;
	add.s64 	%rd10, %rd1, %rd9;
	ld.global.u32 	%r45, [%rd10];

$L__BB52_8:
	setp.eq.s32 	%p6, %r45, %r44;
	setp.ne.s32 	%p7, %r45, %r44;
	mov.u32 	%r32, -1;
	vote.sync.ballot.b32 	%r16, %p7, %r32;
	@%p6 bra 	$L__BB52_12;

	brev.b32 	%r34, %r16;
	bfind.shiftamt.u32 	%r18, %r34;
	setp.ne.s32 	%p8, %r2, %r18;
	mov.u32 	%r33, 0;
	mov.u32 	%r46, %r33;
	@%p8 bra 	$L__BB52_11;

	popc.b32 	%r35, %r16;
	atom.global.add.u32 	%r46, [%rd2], %r35;

$L__BB52_11:
	mov.u32 	%r36, 31;
	shfl.sync.idx.b32 	%r37|%p9, %r46, %r18, %r36, %r16;
	shl.b32 	%r38, %r16, %r8;
	popc.b32 	%r39, %r38;
	add.s32 	%r40, %r37, %r39;
	mul.wide.u32 	%rd11, %r40, 16;
	add.s64 	%rd12, %rd3, %rd11;
	sub.s32 	%r41, %r45, %r44;
	st.global.v4.u32 	[%rd12], {%r43, %r44, %r41, %r33};

$L__BB52_12:
	add.s32 	%r43, %r43, %r7;
	setp.lt.u32 	%p10, %r43, %r23;
	@%p10 bra 	$L__BB52_2;

$L__BB52_13:
	ret;

}
	// .globl	mkperm_phase_4_tiny
.visible .entry mkperm_phase_4_tiny(
	.param .u64 mkperm_phase_4_tiny_param_0,
	.param .u64 mkperm_phase_4_tiny_param_1,
	.param .u64 mkperm_phase_4_tiny_param_2,
	.param .u32 mkperm_phase_4_tiny_param_3,
	.param .u32 mkperm_phase_4_tiny_param_4,
	.param .u32 mkperm_phase_4_tiny_param_5
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<52>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd5, [mkperm_phase_4_tiny_param_0];
	ld.param.u64 	%rd6, [mkperm_phase_4_tiny_param_1];
	ld.param.u64 	%rd7, [mkperm_phase_4_tiny_param_2];
	ld.param.u32 	%r23, [mkperm_phase_4_tiny_param_3];
	ld.param.u32 	%r25, [mkperm_phase_4_tiny_param_4];
	ld.param.u32 	%r24, [mkperm_phase_4_tiny_param_5];
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r2, %r1, %r25;
	add.s32 	%r3, %r2, %r25;
	mov.u32 	%r4, WARP_SZ;
	mov.u32 	%r5, %ntid.x;
	div.u32 	%r6, %r5, %r4;
	mov.u32 	%r7, %tid.x;
	mul.lo.s32 	%r8, %r6, %r24;
	setp.ge.u32 	%p2, %r7, %r8;
	@%p2 bra 	$L__BB53_3;

	mul.lo.s32 	%r26, %r1, %r24;
	mul.lo.s32 	%r27, %r26, %r6;
	cvt.u64.u32 	%rd1, %r27;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r49, %r7;

$L__BB53_2:
	cvt.u64.u32 	%rd8, %r49;
	add.s64 	%rd9, %rd8, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.u32 	%r28, [%rd11];
	shl.b32 	%r29, %r49, 2;
	mov.u32 	%r30, shared;
	add.s32 	%r31, %r30, %r29;
	st.shared.u32 	[%r31], %r28;
	add.s32 	%r49, %r49, %r5;
	setp.lt.u32 	%p3, %r49, %r8;
	@%p3 bra 	$L__BB53_2;

$L__BB53_3:
	bar.sync 	0;
	add.s32 	%r50, %r2, %r7;
	setp.ge.u32 	%p4, %r50, %r3;
	@%p4 bra 	$L__BB53_10;

	add.s32 	%r32, %r4, -1;
	and.b32  	%r12, %r32, %r7;
	mov.u32 	%r33, 32;
	sub.s32 	%r13, %r33, %r12;
	cvta.to.global.u64 	%rd3, %rd7;
	cvta.to.global.u64 	%rd4, %rd5;
	div.u32 	%r34, %r7, %r4;
	mul.lo.s32 	%r14, %r34, %r24;

$L__BB53_5:
	setp.ge.u32 	%p5, %r50, %r23;
	setp.lt.u32 	%p6, %r50, %r23;
	mov.u32 	%r35, -1;
	vote.sync.ballot.b32 	%r16, %p6, %r35;
	@%p5 bra 	$L__BB53_9;

	mul.wide.u32 	%rd12, %r50, 4;
	add.s64 	%rd13, %rd4, %rd12;
	ld.global.u32 	%r17, [%rd13];
	match.any.sync.b32 	%r18, %r17, %r16;
	brev.b32 	%r37, %r18;
	bfind.shiftamt.u32 	%r19, %r37;
	setp.ne.s32 	%p7, %r12, %r19;
	mov.u32 	%r51, 0;
	@%p7 bra 	$L__BB53_8;

	add.s32 	%r38, %r17, %r14;
	shl.b32 	%r39, %r38, 2;
	mov.u32 	%r40, shared;
	add.s32 	%r41, %r40, %r39;
	popc.b32 	%r42, %r18;
	ld.shared.u32 	%r51, [%r41];
	add.s32 	%r43, %r51, %r42;
	st.shared.u32 	[%r41], %r43;

$L__BB53_8:
	mov.u32 	%r44, 31;
	shfl.sync.idx.b32 	%r45|%p8, %r51, %r19, %r44, %r18;
	shl.b32 	%r46, %r18, %r13;
	popc.b32 	%r47, %r46;
	add.s32 	%r48, %r45, %r47;
	mul.wide.u32 	%rd14, %r48, 4;
	add.s64 	%rd15, %rd3, %rd14;
	st.global.u32 	[%rd15], %r50;

$L__BB53_9:
	add.s32 	%r50, %r50, %r5;
	setp.lt.u32 	%p9, %r50, %r3;
	@%p9 bra 	$L__BB53_5;

$L__BB53_10:
	ret;

}
	// .globl	mkperm_phase_4_small
.visible .entry mkperm_phase_4_small(
	.param .u64 mkperm_phase_4_small_param_0,
	.param .u64 mkperm_phase_4_small_param_1,
	.param .u64 mkperm_phase_4_small_param_2,
	.param .u32 mkperm_phase_4_small_param_3,
	.param .u32 mkperm_phase_4_small_param_4,
	.param .u32 mkperm_phase_4_small_param_5
)
{
	.reg .pred 	%p<10>;
	.reg .b32 	%r<45>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd5, [mkperm_phase_4_small_param_0];
	ld.param.u64 	%rd6, [mkperm_phase_4_small_param_1];
	ld.param.u64 	%rd7, [mkperm_phase_4_small_param_2];
	ld.param.u32 	%r19, [mkperm_phase_4_small_param_3];
	ld.param.u32 	%r21, [mkperm_phase_4_small_param_4];
	ld.param.u32 	%r20, [mkperm_phase_4_small_param_5];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r2, %ctaid.x;
	mul.lo.s32 	%r3, %r2, %r21;
	add.s32 	%r4, %r3, %r21;
	mov.u32 	%r5, %tid.x;
	setp.ge.u32 	%p2, %r5, %r20;
	@%p2 bra 	$L__BB54_3;

	mul.lo.s32 	%r22, %r2, %r20;
	cvt.u64.u32 	%rd1, %r22;
	cvta.to.global.u64 	%rd2, %rd6;
	mov.u32 	%r42, %r5;

$L__BB54_2:
	cvt.u64.u32 	%rd8, %r42;
	add.s64 	%rd9, %rd8, %rd1;
	shl.b64 	%rd10, %rd9, 2;
	add.s64 	%rd11, %rd2, %rd10;
	ld.global.u32 	%r23, [%rd11];
	shl.b32 	%r24, %r42, 2;
	mov.u32 	%r25, shared;
	add.s32 	%r26, %r25, %r24;
	st.shared.u32 	[%r26], %r23;
	add.s32 	%r42, %r42, %r1;
	setp.lt.u32 	%p3, %r42, %r20;
	@%p3 bra 	$L__BB54_2;

$L__BB54_3:
	bar.sync 	0;
	add.s32 	%r43, %r3, %r5;
	setp.ge.u32 	%p4, %r43, %r4;
	@%p4 bra 	$L__BB54_10;

	mov.u32 	%r27, WARP_SZ;
	add.s32 	%r28, %r27, -1;
	and.b32  	%r9, %r28, %r5;
	mov.u32 	%r29, 32;
	sub.s32 	%r10, %r29, %r9;
	cvta.to.global.u64 	%rd3, %rd7;
	cvta.to.global.u64 	%rd4, %rd5;

$L__BB54_5:
	setp.ge.u32 	%p5, %r43, %r19;
	setp.lt.u32 	%p6, %r43, %r19;
	mov.u32 	%r30, -1;
	vote.sync.ballot.b32 	%r12, %p6, %r30;
	@%p5 bra 	$L__BB54_9;

	mul.wide.u32 	%rd12, %r43, 4;
	add.s64 	%rd13, %rd4, %rd12;
	ld.global.u32 	%r13, [%rd13];
	match.any.sync.b32 	%r14, %r13, %r12;
	brev.b32 	%r32, %r14;
	bfind.shiftamt.u32 	%r15, %r32;
	setp.ne.s32 	%p7, %r9, %r15;
	mov.u32 	%r44, 0;
	@%p7 bra 	$L__BB54_8;

	shl.b32 	%r33, %r13, 2;
	mov.u32 	%r34, shared;
	add.s32 	%r35, %r34, %r33;
	popc.b32 	%r36, %r14;
	atom.shared.add.u32 	%r44, [%r35], %r36;

$L__BB54_8:
	mov.u32 	%r37, 31;
	shfl.sync.idx.b32 	%r38|%p8, %r44, %r15, %r37, %r14;
	shl.b32 	%r39, %r14, %r10;
	popc.b32 	%r40, %r39;
	add.s32 	%r41, %r38, %r40;
	mul.wide.u32 	%rd14, %r41, 4;
	add.s64 	%rd15, %rd3, %rd14;
	st.global.u32 	[%rd15], %r43;

$L__BB54_9:
	add.s32 	%r43, %r43, %r1;
	setp.lt.u32 	%p9, %r43, %r4;
	@%p9 bra 	$L__BB54_5;

$L__BB54_10:
	ret;

}
	// .globl	mkperm_phase_4_large
.visible .entry mkperm_phase_4_large(
	.param .u64 mkperm_phase_4_large_param_0,
	.param .u64 mkperm_phase_4_large_param_1,
	.param .u64 mkperm_phase_4_large_param_2,
	.param .u32 mkperm_phase_4_large_param_3,
	.param .u32 mkperm_phase_4_large_param_4,
	.param .u32 mkperm_phase_4_large_param_5
)
{
	.reg .pred 	%p<8>;
	.reg .b32 	%r<35>;
	.reg .b64 	%rd<16>;


	ld.param.u64 	%rd5, [mkperm_phase_4_large_param_0];
	ld.param.u64 	%rd6, [mkperm_phase_4_large_param_1];
	ld.param.u64 	%rd7, [mkperm_phase_4_large_param_2];
	ld.param.u32 	%r16, [mkperm_phase_4_large_param_3];
	ld.param.u32 	%r18, [mkperm_phase_4_large_param_4];
	ld.param.u32 	%r17, [mkperm_phase_4_large_param_5];
	mov.u32 	%r1, %ctaid.x;
	mul.lo.s32 	%r19, %r1, %r18;
	add.s32 	%r2, %r19, %r18;
	mov.u32 	%r3, %tid.x;
	add.s32 	%r33, %r19, %r3;
	setp.ge.u32 	%p2, %r33, %r2;
	@%p2 bra 	$L__BB55_7;

	mov.u32 	%r20, WARP_SZ;
	add.s32 	%r21, %r20, -1;
	and.b32  	%r5, %r21, %r3;
	mov.u32 	%r22, 32;
	sub.s32 	%r6, %r22, %r5;
	mul.lo.s32 	%r23, %r1, %r17;
	cvt.u64.u32 	%rd1, %r23;
	mov.u32 	%r7, %ntid.x;
	cvta.to.global.u64 	%rd2, %rd5;
	cvta.to.global.u64 	%rd3, %rd6;
	cvta.to.global.u64 	%rd4, %rd7;

$L__BB55_2:
	setp.ge.u32 	%p3, %r33, %r16;
	setp.lt.u32 	%p4, %r33, %r16;
	mov.u32 	%r24, -1;
	vote.sync.ballot.b32 	%r9, %p4, %r24;
	@%p3 bra 	$L__BB55_6;

	mul.wide.u32 	%rd8, %r33, 4;
	add.s64 	%rd9, %rd2, %rd8;
	ld.global.u32 	%r10, [%rd9];
	match.any.sync.b32 	%r11, %r10, %r9;
	brev.b32 	%r26, %r11;
	bfind.shiftamt.u32 	%r12, %r26;
	setp.ne.s32 	%p5, %r5, %r12;
	mov.u32 	%r34, 0;
	@%p5 bra 	$L__BB55_5;

	cvt.u64.u32 	%rd10, %r10;
	add.s64 	%rd11, %rd10, %rd1;
	shl.b64 	%rd12, %rd11, 2;
	add.s64 	%rd13, %rd3, %rd12;
	popc.b32 	%r27, %r11;
	atom.global.add.u32 	%r34, [%rd13], %r27;

$L__BB55_5:
	mov.u32 	%r28, 31;
	shfl.sync.idx.b32 	%r29|%p6, %r34, %r12, %r28, %r11;
	shl.b32 	%r30, %r11, %r6;
	popc.b32 	%r31, %r30;
	add.s32 	%r32, %r29, %r31;
	mul.wide.u32 	%rd14, %r32, 4;
	add.s64 	%rd15, %rd4, %rd14;
	st.global.u32 	[%rd15], %r33;

$L__BB55_6:
	add.s32 	%r33, %r33, %r7;
	setp.lt.u32 	%p7, %r33, %r2;
	@%p7 bra 	$L__BB55_2;

$L__BB55_7:
	ret;

}
	// .globl	transpose
.visible .entry transpose(
	.param .u64 transpose_param_0,
	.param .u64 transpose_param_1,
	.param .u32 transpose_param_2,
	.param .u32 transpose_param_3
)
{
	.reg .pred 	%p<7>;
	.reg .b32 	%r<25>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [transpose_param_0];
	ld.param.u64 	%rd2, [transpose_param_1];
	ld.param.u32 	%r9, [transpose_param_2];
	ld.param.u32 	%r10, [transpose_param_3];
	mov.u32 	%r11, %ctaid.x;
	shl.b32 	%r1, %r11, 4;
	mov.u32 	%r2, %tid.x;
	add.s32 	%r3, %r1, %r2;
	mov.u32 	%r12, %ctaid.y;
	shl.b32 	%r4, %r12, 4;
	mov.u32 	%r5, %tid.y;
	add.s32 	%r6, %r4, %r5;
	setp.ge.u32 	%p1, %r6, %r9;
	setp.ge.u32 	%p2, %r3, %r10;
	or.pred  	%p3, %p1, %p2;
	@%p3 bra 	$L__BB56_2;

	cvta.to.global.u64 	%rd3, %rd1;
	mad.lo.s32 	%r13, %r6, %r10, %r3;
	mul.wide.u32 	%rd4, %r13, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r14, [%rd5];
	mad.lo.s32 	%r15, %r5, 17, %r2;
	shl.b32 	%r16, %r15, 2;
	mov.u32 	%r17, shared;
	add.s32 	%r18, %r17, %r16;
	st.shared.u32 	[%r18], %r14;

$L__BB56_2:
	bar.sync 	0;
	add.s32 	%r7, %r1, %r5;
	setp.ge.u32 	%p4, %r7, %r10;
	add.s32 	%r8, %r4, %r2;
	setp.ge.u32 	%p5, %r8, %r9;
	or.pred  	%p6, %p5, %p4;
	@%p6 bra 	$L__BB56_4;

	mad.lo.s32 	%r19, %r2, 17, %r5;
	shl.b32 	%r20, %r19, 2;
	mov.u32 	%r21, shared;
	add.s32 	%r22, %r21, %r20;
	ld.shared.u32 	%r23, [%r22];
	mad.lo.s32 	%r24, %r7, %r9, %r8;
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r24, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r23;

$L__BB56_4:
	ret;

}
	// .globl	poke_u8
.visible .entry poke_u8(
	.param .u64 poke_u8_param_0,
	.param .u8 poke_u8_param_1
)
{
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<3>;


	ld.param.u8 	%rs1, [poke_u8_param_1];
	ld.param.u64 	%rd1, [poke_u8_param_0];
	cvta.to.global.u64 	%rd2, %rd1;
	st.global.u8 	[%rd2], %rs1;
	ret;

}
	// .globl	poke_u16
.visible .entry poke_u16(
	.param .u64 poke_u16_param_0,
	.param .u16 poke_u16_param_1
)
{
	.reg .b16 	%rs<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u16_param_0];
	ld.param.u16 	%rs1, [poke_u16_param_1];
	cvta.to.global.u64 	%rd2, %rd1;
	st.global.u16 	[%rd2], %rs1;
	ret;

}
	// .globl	poke_u32
.visible .entry poke_u32(
	.param .u64 poke_u32_param_0,
	.param .u32 poke_u32_param_1
)
{
	.reg .b32 	%r<2>;
	.reg .b64 	%rd<3>;


	ld.param.u64 	%rd1, [poke_u32_param_0];
	ld.param.u32 	%r1, [poke_u32_param_1];
	cvta.to.global.u64 	%rd2, %rd1;
	st.global.u32 	[%rd2], %r1;
	ret;

}
	// .globl	poke_u64
.visible .entry poke_u64(
	.param .u64 poke_u64_param_0,
	.param .u64 poke_u64_param_1
)
{
	.reg .b64 	%rd<4>;


	ld.param.u64 	%rd1, [poke_u64_param_0];
	ld.param.u64 	%rd2, [poke_u64_param_1];
	cvta.to.global.u64 	%rd3, %rd1;
	st.global.u64 	[%rd3], %rd2;
	ret;

}
	// .globl	fill_64
.visible .entry fill_64(
	.param .u64 fill_64_param_0,
	.param .u32 fill_64_param_1,
	.param .u64 fill_64_param_2
)
{
	.reg .pred 	%p<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<6>;


	ld.param.u64 	%rd2, [fill_64_param_0];
	ld.param.u32 	%r6, [fill_64_param_1];
	ld.param.u64 	%rd3, [fill_64_param_2];
	mov.u32 	%r1, %ntid.x;
	mov.u32 	%r7, %ctaid.x;
	mov.u32 	%r8, %tid.x;
	mad.lo.s32 	%r10, %r7, %r1, %r8;
	setp.ge.u32 	%p1, %r10, %r6;
	@%p1 bra 	$L__BB61_3;

	mov.u32 	%r9, %nctaid.x;
	mul.lo.s32 	%r3, %r1, %r9;
	cvta.to.global.u64 	%rd1, %rd2;

$L__BB61_2:
	mul.wide.u32 	%rd4, %r10, 8;
	add.s64 	%rd5, %rd1, %rd4;
	st.global.u64 	[%rd5], %rd3;
	add.s32 	%r10, %r10, %r3;
	setp.lt.u32 	%p2, %r10, %r6;
	@%p2 bra 	$L__BB61_2;

$L__BB61_3:
	ret;

}
	// .globl	aggregate
.visible .entry aggregate(
	.param .u64 aggregate_param_0,
	.param .u64 aggregate_param_1,
	.param .u32 aggregate_param_2
)
{
	.reg .pred 	%p<13>;
	.reg .b16 	%rs<3>;
	.reg .b32 	%r<11>;
	.reg .b64 	%rd<12>;


	ld.param.u64 	%rd4, [aggregate_param_0];
	ld.param.u64 	%rd5, [aggregate_param_1];
	ld.param.u32 	%r2, [aggregate_param_2];
	mov.u32 	%r3, %ntid.x;
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %tid.x;
	mad.lo.s32 	%r1, %r4, %r3, %r5;
	setp.ge.u32 	%p1, %r1, %r2;
	@%p1 bra 	$L__BB62_20;

	cvta.to.global.u64 	%rd6, %rd4;
	cvta.to.global.u64 	%rd7, %rd5;
	mul.wide.u32 	%rd8, %r1, 16;
	add.s64 	%rd9, %rd7, %rd8;
	ld.global.v2.u32 	{%r7, %r8}, [%rd9];
	cvt.u64.u32 	%rd10, %r8;
	add.s64 	%rd1, %rd6, %rd10;
	ld.global.u64 	%rd2, [%rd9+8];
	setp.gt.s32 	%p2, %r7, 0;
	@%p2 bra 	$L__BB62_7;

	setp.gt.s32 	%p8, %r7, -3;
	@%p8 bra 	$L__BB62_5;

	setp.eq.s32 	%p11, %r7, -8;
	@%p11 bra 	$L__BB62_19;

	setp.eq.s32 	%p12, %r7, -4;
	@%p12 bra 	$L__BB62_18;
	bra.uni 	$L__BB62_20;

$L__BB62_18:
	ld.u32 	%r10, [%rd2];
	st.global.u32 	[%rd1], %r10;
	bra.uni 	$L__BB62_20;

$L__BB62_7:
	setp.gt.s32 	%p3, %r7, 3;
	@%p3 bra 	$L__BB62_10;

	setp.eq.s32 	%p6, %r7, 1;
	@%p6 bra 	$L__BB62_15;

	setp.eq.s32 	%p7, %r7, 2;
	@%p7 bra 	$L__BB62_14;
	bra.uni 	$L__BB62_20;

$L__BB62_14:
	st.global.u16 	[%rd1], %rd2;
	bra.uni 	$L__BB62_20;

$L__BB62_5:
	setp.eq.s32 	%p9, %r7, -2;
	@%p9 bra 	$L__BB62_17;

	setp.eq.s32 	%p10, %r7, -1;
	@%p10 bra 	$L__BB62_16;
	bra.uni 	$L__BB62_20;

$L__BB62_16:
	ld.u8 	%rs1, [%rd2];
	st.global.u8 	[%rd1], %rs1;
	bra.uni 	$L__BB62_20;

$L__BB62_10:
	setp.eq.s32 	%p4, %r7, 4;
	@%p4 bra 	$L__BB62_13;

	setp.ne.s32 	%p5, %r7, 8;
	@%p5 bra 	$L__BB62_20;

	st.global.u64 	[%rd1], %rd2;
	bra.uni 	$L__BB62_20;

$L__BB62_19:
	ld.u64 	%rd11, [%rd2];
	st.global.u64 	[%rd1], %rd11;
	bra.uni 	$L__BB62_20;

$L__BB62_15:
	st.global.u8 	[%rd1], %rd2;
	bra.uni 	$L__BB62_20;

$L__BB62_17:
	ld.u16 	%rs2, [%rd2];
	st.global.u16 	[%rd1], %rs2;
	bra.uni 	$L__BB62_20;

$L__BB62_13:
	st.global.u32 	[%rd1], %rd2;

$L__BB62_20:
	ret;

}
	// .globl	block_copy_u32
.visible .entry block_copy_u32(
	.param .u64 block_copy_u32_param_0,
	.param .u64 block_copy_u32_param_1,
	.param .u32 block_copy_u32_param_2,
	.param .u32 block_copy_u32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<9>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_copy_u32_param_0];
	ld.param.u64 	%rd2, [block_copy_u32_param_1];
	ld.param.u32 	%r3, [block_copy_u32_param_2];
	ld.param.u32 	%r2, [block_copy_u32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB63_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u32 	%r8, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.u32 	[%rd8], %r8;

$L__BB63_2:
	ret;

}
	// .globl	block_sum_u32
.visible .entry block_sum_u32(
	.param .u64 block_sum_u32_param_0,
	.param .u64 block_sum_u32_param_1,
	.param .u32 block_sum_u32_param_2,
	.param .u32 block_sum_u32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<10>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_sum_u32_param_0];
	ld.param.u64 	%rd2, [block_sum_u32_param_1];
	ld.param.u32 	%r3, [block_sum_u32_param_2];
	ld.param.u32 	%r2, [block_sum_u32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB64_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 4;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u32 	%r8, [%rd8];
	atom.global.add.u32 	%r9, [%rd6], %r8;

$L__BB64_2:
	ret;

}
	// .globl	block_copy_u64
.visible .entry block_copy_u64(
	.param .u64 block_copy_u64_param_0,
	.param .u64 block_copy_u64_param_1,
	.param .u32 block_copy_u64_param_2,
	.param .u32 block_copy_u64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<10>;


	ld.param.u64 	%rd1, [block_copy_u64_param_0];
	ld.param.u64 	%rd2, [block_copy_u64_param_1];
	ld.param.u32 	%r3, [block_copy_u64_param_2];
	ld.param.u32 	%r2, [block_copy_u64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB65_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.u64 	%rd6, [%rd5];
	cvta.to.global.u64 	%rd7, %rd2;
	mul.wide.u32 	%rd8, %r1, 8;
	add.s64 	%rd9, %rd7, %rd8;
	st.global.u64 	[%rd9], %rd6;

$L__BB65_2:
	ret;

}
	// .globl	block_sum_u64
.visible .entry block_sum_u64(
	.param .u64 block_sum_u64_param_0,
	.param .u64 block_sum_u64_param_1,
	.param .u32 block_sum_u64_param_2,
	.param .u32 block_sum_u64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<11>;


	ld.param.u64 	%rd1, [block_sum_u64_param_0];
	ld.param.u64 	%rd2, [block_sum_u64_param_1];
	ld.param.u32 	%r3, [block_sum_u64_param_2];
	ld.param.u32 	%r2, [block_sum_u64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB66_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 8;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.u64 	%rd9, [%rd8];
	atom.global.add.u64 	%rd10, [%rd6], %rd9;

$L__BB66_2:
	ret;

}
	// .globl	block_copy_f32
.visible .entry block_copy_f32(
	.param .u64 block_copy_f32_param_0,
	.param .u64 block_copy_f32_param_1,
	.param .u32 block_copy_f32_param_2,
	.param .u32 block_copy_f32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<2>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_copy_f32_param_0];
	ld.param.u64 	%rd2, [block_copy_f32_param_1];
	ld.param.u32 	%r3, [block_copy_f32_param_2];
	ld.param.u32 	%r2, [block_copy_f32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB67_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 4;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f32 	%f1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f32 	[%rd8], %f1;

$L__BB67_2:
	ret;

}
	// .globl	block_sum_f32
.visible .entry block_sum_f32(
	.param .u64 block_sum_f32_param_0,
	.param .u64 block_sum_f32_param_1,
	.param .u32 block_sum_f32_param_2,
	.param .u32 block_sum_f32_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .f32 	%f<3>;
	.reg .b32 	%r<8>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_sum_f32_param_0];
	ld.param.u64 	%rd2, [block_sum_f32_param_1];
	ld.param.u32 	%r3, [block_sum_f32_param_2];
	ld.param.u32 	%r2, [block_sum_f32_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB68_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 4;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 4;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f32 	%f1, [%rd8];
	atom.global.add.f32 	%f2, [%rd6], %f1;

$L__BB68_2:
	ret;

}
	// .globl	block_copy_f64
.visible .entry block_copy_f64(
	.param .u64 block_copy_f64_param_0,
	.param .u64 block_copy_f64_param_1,
	.param .u32 block_copy_f64_param_2,
	.param .u32 block_copy_f64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<2>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_copy_f64_param_0];
	ld.param.u64 	%rd2, [block_copy_f64_param_1];
	ld.param.u32 	%r3, [block_copy_f64_param_2];
	ld.param.u32 	%r2, [block_copy_f64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB69_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	mul.wide.u32 	%rd4, %r7, 8;
	add.s64 	%rd5, %rd3, %rd4;
	ld.global.f64 	%fd1, [%rd5];
	cvta.to.global.u64 	%rd6, %rd2;
	mul.wide.u32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd6, %rd7;
	st.global.f64 	[%rd8], %fd1;

$L__BB69_2:
	ret;

}
	// .globl	block_sum_f64
.visible .entry block_sum_f64(
	.param .u64 block_sum_f64_param_0,
	.param .u64 block_sum_f64_param_1,
	.param .u32 block_sum_f64_param_2,
	.param .u32 block_sum_f64_param_3
)
{
	.reg .pred 	%p<2>;
	.reg .b32 	%r<8>;
	.reg .f64 	%fd<3>;
	.reg .b64 	%rd<9>;


	ld.param.u64 	%rd1, [block_sum_f64_param_0];
	ld.param.u64 	%rd2, [block_sum_f64_param_1];
	ld.param.u32 	%r3, [block_sum_f64_param_2];
	ld.param.u32 	%r2, [block_sum_f64_param_3];
	mov.u32 	%r4, %ctaid.x;
	mov.u32 	%r5, %ntid.x;
	mov.u32 	%r6, %tid.x;
	mad.lo.s32 	%r1, %r4, %r5, %r6;
	setp.ge.u32 	%p1, %r1, %r3;
	@%p1 bra 	$L__BB70_2;

	cvta.to.global.u64 	%rd3, %rd1;
	div.u32 	%r7, %r1, %r2;
	cvta.to.global.u64 	%rd4, %rd2;
	mul.wide.u32 	%rd5, %r7, 8;
	add.s64 	%rd6, %rd4, %rd5;
	mul.wide.u32 	%rd7, %r1, 8;
	add.s64 	%rd8, %rd3, %rd7;
	ld.global.f64 	%fd1, [%rd8];
	atom.global.add.f64 	%fd2, [%rd6], %fd1;

$L__BB70_2:
	ret;

}

